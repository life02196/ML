{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b034a25f-e8d7-4e26-a0b4-0e4662541276",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load library\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a vector as a row\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tensor_row \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Load library\n",
    "import torch\n",
    "\n",
    "# Create a vector as a row\n",
    "tensor_row = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Create a vector as a column\n",
    "tensor_column = torch.tensor(\n",
    "    [\n",
    "        [1],\n",
    "        [2],\n",
    "        [3]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "805ff03c-bf9a-49ce-aa77-650237ea7281",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nvidia' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnvidia\u001b[49m\u001b[38;5;241m-\u001b[39msmi\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nvidia' is not defined"
     ]
    }
   ],
   "source": [
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d4c165c-35fc-4a3b-b80a-d6a3d28aa6c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1061395986.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7422030-26fd-4fde-8265-ab900a746f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.3.0-cp312-cp312-win_amd64.whl.metadata (26 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.10.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch)\n",
      "  Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Downloading tbb-2021.12.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.3.0-cp312-cp312-win_amd64.whl (159.7 MB)\n",
      "   ---------------------------------------- 0.0/159.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/159.7 MB 1.9 MB/s eta 0:01:24\n",
      "   ---------------------------------------- 0.1/159.7 MB 1.4 MB/s eta 0:01:52\n",
      "   ---------------------------------------- 0.3/159.7 MB 2.1 MB/s eta 0:01:15\n",
      "   ---------------------------------------- 0.5/159.7 MB 2.7 MB/s eta 0:01:00\n",
      "   ---------------------------------------- 0.9/159.7 MB 4.1 MB/s eta 0:00:39\n",
      "   ---------------------------------------- 1.7/159.7 MB 6.4 MB/s eta 0:00:25\n",
      "    --------------------------------------- 3.0/159.7 MB 9.6 MB/s eta 0:00:17\n",
      "    --------------------------------------- 4.0/159.7 MB 12.0 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 4.4/159.7 MB 10.7 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 5.8/159.7 MB 13.6 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 5.8/159.7 MB 13.6 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 8.4/159.7 MB 15.7 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 8.4/159.7 MB 15.7 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 10.9/159.7 MB 23.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 12.0/159.7 MB 27.3 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 12.7/159.7 MB 26.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 15.9/159.7 MB 29.7 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 17.0/159.7 MB 32.7 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 18.3/159.7 MB 31.2 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 19.5/159.7 MB 32.7 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 20.6/159.7 MB 31.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 21.9/159.7 MB 29.7 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 23.2/159.7 MB 32.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 24.4/159.7 MB 31.1 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 25.6/159.7 MB 27.3 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 26.9/159.7 MB 27.3 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 28.1/159.7 MB 27.3 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 29.3/159.7 MB 27.3 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 30.7/159.7 MB 27.3 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 32.0/159.7 MB 27.3 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 33.2/159.7 MB 27.3 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 34.5/159.7 MB 27.3 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 35.7/159.7 MB 27.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 37.0/159.7 MB 27.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 38.3/159.7 MB 27.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 39.6/159.7 MB 27.3 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 41.0/159.7 MB 28.5 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 42.2/159.7 MB 27.3 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 43.5/159.7 MB 27.3 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 44.8/159.7 MB 27.3 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 46.0/159.7 MB 28.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 47.3/159.7 MB 28.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 48.8/159.7 MB 28.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 50.1/159.7 MB 28.5 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 51.4/159.7 MB 28.5 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 52.7/159.7 MB 28.5 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 54.0/159.7 MB 28.5 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 55.3/159.7 MB 28.5 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 56.6/159.7 MB 28.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 58.0/159.7 MB 28.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 59.3/159.7 MB 28.4 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 60.7/159.7 MB 28.5 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 62.0/159.7 MB 28.5 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 63.2/159.7 MB 29.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 64.6/159.7 MB 29.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 66.0/159.7 MB 29.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 67.5/159.7 MB 29.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 68.8/159.7 MB 29.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 70.1/159.7 MB 29.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 71.4/159.7 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 72.7/159.7 MB 28.4 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 74.1/159.7 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 75.6/159.7 MB 29.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 77.0/159.7 MB 29.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 78.3/159.7 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 79.6/159.7 MB 29.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 80.9/159.7 MB 29.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 82.3/159.7 MB 29.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 83.7/159.7 MB 29.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 85.2/159.7 MB 29.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 86.6/159.7 MB 29.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 87.9/159.7 MB 31.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 89.2/159.7 MB 29.7 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 90.5/159.7 MB 29.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 92.0/159.7 MB 29.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 93.4/159.7 MB 29.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 94.9/159.7 MB 31.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 96.3/159.7 MB 31.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 97.7/159.7 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 99.0/159.7 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 100.4/159.7 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 101.8/159.7 MB 29.8 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 103.2/159.7 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 104.7/159.7 MB 31.1 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 106.1/159.7 MB 31.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 107.5/159.7 MB 31.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 108.0/159.7 MB 31.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 108.0/159.7 MB 31.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 108.7/159.7 MB 24.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 110.0/159.7 MB 23.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 111.3/159.7 MB 23.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 113.0/159.7 MB 23.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 114.4/159.7 MB 23.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 115.6/159.7 MB 23.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 116.9/159.7 MB 23.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 118.3/159.7 MB 31.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 119.7/159.7 MB 29.8 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 121.3/159.7 MB 31.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 122.7/159.7 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 124.0/159.7 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 125.4/159.7 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 126.8/159.7 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 128.2/159.7 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 129.8/159.7 MB 31.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 131.3/159.7 MB 31.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 132.7/159.7 MB 31.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 134.0/159.7 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 135.4/159.7 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 136.8/159.7 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 138.3/159.7 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 139.8/159.7 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 141.3/159.7 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 142.6/159.7 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 144.0/159.7 MB 31.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 144.7/159.7 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 145.4/159.7 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 147.9/159.7 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 149.0/159.7 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 150.0/159.7 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 151.0/159.7 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 151.9/159.7 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 152.9/159.7 MB 24.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 154.0/159.7 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 155.1/159.7 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  156.1/159.7 MB 25.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  157.2/159.7 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  158.2/159.7 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.2/159.7 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 159.7/159.7 MB 17.2 MB/s eta 0:00:00\n",
      "Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "   ---------------------------------------- 0.0/228.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/228.5 MB 24.9 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 2.1/228.5 MB 22.9 MB/s eta 0:00:10\n",
      "    --------------------------------------- 3.1/228.5 MB 22.2 MB/s eta 0:00:11\n",
      "    --------------------------------------- 4.2/228.5 MB 22.3 MB/s eta 0:00:11\n",
      "    --------------------------------------- 5.2/228.5 MB 23.6 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 6.2/228.5 MB 23.5 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 7.4/228.5 MB 23.7 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 8.4/228.5 MB 23.3 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 9.3/228.5 MB 23.0 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 10.4/228.5 MB 22.6 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 11.4/228.5 MB 22.5 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 12.6/228.5 MB 22.6 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 13.8/228.5 MB 23.4 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 14.8/228.5 MB 23.4 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 15.8/228.5 MB 22.6 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 16.8/228.5 MB 22.6 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 17.8/228.5 MB 23.4 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 19.0/228.5 MB 24.2 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 20.2/228.5 MB 22.5 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 21.2/228.5 MB 23.4 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 22.1/228.5 MB 23.4 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 23.1/228.5 MB 23.4 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 24.3/228.5 MB 23.4 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 25.5/228.5 MB 24.2 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 26.5/228.5 MB 23.4 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 27.5/228.5 MB 23.4 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 28.6/228.5 MB 23.4 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 29.6/228.5 MB 23.4 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 30.9/228.5 MB 23.4 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 32.1/228.5 MB 24.2 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 33.2/228.5 MB 23.4 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 34.3/228.5 MB 23.4 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 35.4/228.5 MB 24.2 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 36.5/228.5 MB 24.2 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 37.7/228.5 MB 25.2 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 39.0/228.5 MB 25.2 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 40.1/228.5 MB 25.2 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 41.2/228.5 MB 25.2 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 42.3/228.5 MB 25.2 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 43.4/228.5 MB 25.1 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 44.6/228.5 MB 25.1 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 45.8/228.5 MB 25.2 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 47.1/228.5 MB 25.2 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 48.2/228.5 MB 25.2 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 49.3/228.5 MB 25.2 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 50.5/228.5 MB 25.2 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 51.6/228.5 MB 25.2 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 52.8/228.5 MB 25.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 54.0/228.5 MB 25.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 55.3/228.5 MB 25.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 56.5/228.5 MB 25.2 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 57.6/228.5 MB 25.2 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 57.6/228.5 MB 25.2 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 59.7/228.5 MB 25.2 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 60.5/228.5 MB 25.2 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 61.4/228.5 MB 23.4 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 62.4/228.5 MB 23.4 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 63.2/228.5 MB 22.6 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 64.0/228.5 MB 21.8 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 64.8/228.5 MB 21.1 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 65.6/228.5 MB 20.5 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 66.5/228.5 MB 19.9 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 67.4/228.5 MB 19.9 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 68.3/228.5 MB 21.1 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 69.1/228.5 MB 19.3 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 70.0/228.5 MB 18.7 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 70.8/228.5 MB 18.7 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 71.6/228.5 MB 18.2 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 72.5/228.5 MB 18.2 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 73.5/228.5 MB 18.7 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 74.4/228.5 MB 18.7 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 75.2/228.5 MB 19.3 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 76.1/228.5 MB 19.3 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 76.9/228.5 MB 19.3 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 77.8/228.5 MB 19.3 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 78.7/228.5 MB 19.3 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 79.7/228.5 MB 19.3 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 80.6/228.5 MB 19.2 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 81.5/228.5 MB 19.2 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 82.3/228.5 MB 19.3 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 83.2/228.5 MB 19.3 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 84.1/228.5 MB 19.3 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 85.0/228.5 MB 19.3 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 86.0/228.5 MB 19.8 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 86.9/228.5 MB 19.9 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 87.2/228.5 MB 19.3 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 87.3/228.5 MB 17.7 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 87.8/228.5 MB 17.2 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 88.8/228.5 MB 16.8 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 89.7/228.5 MB 16.8 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 90.6/228.5 MB 16.8 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 91.6/228.5 MB 17.3 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 92.4/228.5 MB 17.2 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 93.3/228.5 MB 17.2 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 94.3/228.5 MB 17.2 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 95.2/228.5 MB 17.7 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 96.1/228.5 MB 17.2 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 97.1/228.5 MB 17.7 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 98.0/228.5 MB 20.5 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 98.9/228.5 MB 20.5 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 99.9/228.5 MB 20.5 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 100.8/228.5 MB 20.5 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 101.8/228.5 MB 20.5 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 102.7/228.5 MB 20.5 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 103.7/228.5 MB 20.5 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 104.7/228.5 MB 20.5 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 105.6/228.5 MB 20.5 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 106.5/228.5 MB 20.5 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 107.5/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 108.4/228.5 MB 20.5 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 109.4/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 110.4/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 111.3/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 112.3/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 113.2/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 114.2/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 115.1/228.5 MB 20.5 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 116.1/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 117.1/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 118.0/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 119.0/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 120.0/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 121.0/228.5 MB 21.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 122.0/228.5 MB 21.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 123.0/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 123.9/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 124.8/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 125.8/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 126.8/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 127.8/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 128.8/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 129.7/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 130.7/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 131.7/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 132.6/228.5 MB 21.9 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 133.7/228.5 MB 21.9 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 134.7/228.5 MB 21.8 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 135.6/228.5 MB 21.8 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 135.8/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 137.4/228.5 MB 21.1 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 138.1/228.5 MB 20.5 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 138.7/228.5 MB 19.8 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 139.5/228.5 MB 19.3 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 140.2/228.5 MB 18.7 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 140.9/228.5 MB 18.2 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 141.5/228.5 MB 17.7 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 142.3/228.5 MB 18.2 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 143.0/228.5 MB 17.7 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 143.7/228.5 MB 17.2 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 144.4/228.5 MB 16.8 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 145.2/228.5 MB 16.4 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 145.9/228.5 MB 16.0 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 146.6/228.5 MB 16.8 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 147.3/228.5 MB 15.6 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 148.0/228.5 MB 15.6 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 148.8/228.5 MB 15.6 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 149.5/228.5 MB 15.6 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 150.3/228.5 MB 15.6 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 150.9/228.5 MB 16.0 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 151.7/228.5 MB 16.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 152.4/228.5 MB 16.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 153.2/228.5 MB 16.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 154.0/228.5 MB 16.4 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 154.7/228.5 MB 16.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 155.5/228.5 MB 16.4 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 156.2/228.5 MB 16.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 156.9/228.5 MB 16.4 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 157.7/228.5 MB 16.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 158.5/228.5 MB 16.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 159.3/228.5 MB 16.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 160.1/228.5 MB 16.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 160.8/228.5 MB 16.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 161.6/228.5 MB 16.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 162.4/228.5 MB 16.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 163.1/228.5 MB 16.4 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 163.9/228.5 MB 16.8 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 164.7/228.5 MB 16.8 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 165.5/228.5 MB 16.8 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 166.2/228.5 MB 16.8 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 167.0/228.5 MB 16.8 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 167.8/228.5 MB 16.8 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 167.8/228.5 MB 16.8 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 169.2/228.5 MB 16.8 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 169.8/228.5 MB 16.4 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 170.3/228.5 MB 16.0 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 170.9/228.5 MB 15.6 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 171.4/228.5 MB 15.6 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 172.0/228.5 MB 15.2 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 172.5/228.5 MB 14.9 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 173.1/228.5 MB 14.9 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 173.7/228.5 MB 14.6 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 174.2/228.5 MB 14.2 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 174.8/228.5 MB 13.9 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 175.4/228.5 MB 13.6 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 176.0/228.5 MB 13.6 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 176.6/228.5 MB 13.4 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 177.2/228.5 MB 13.1 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 177.8/228.5 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 178.3/228.5 MB 13.6 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 178.9/228.5 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 179.5/228.5 MB 12.4 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 180.1/228.5 MB 12.3 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 180.7/228.5 MB 12.4 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 181.3/228.5 MB 12.6 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 181.9/228.5 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 182.5/228.5 MB 12.9 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 183.1/228.5 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 183.7/228.5 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 184.3/228.5 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 184.9/228.5 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 185.5/228.5 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 186.1/228.5 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 186.7/228.5 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 187.3/228.5 MB 12.9 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 187.9/228.5 MB 13.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 188.6/228.5 MB 13.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 189.2/228.5 MB 13.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 189.8/228.5 MB 13.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 190.4/228.5 MB 13.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 191.0/228.5 MB 13.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 191.6/228.5 MB 13.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 192.3/228.5 MB 13.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 192.9/228.5 MB 13.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 193.4/228.5 MB 13.6 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 193.4/228.5 MB 13.6 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 194.7/228.5 MB 13.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 195.1/228.5 MB 13.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 195.5/228.5 MB 12.8 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 196.0/228.5 MB 12.6 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 196.4/228.5 MB 12.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 196.9/228.5 MB 12.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 197.3/228.5 MB 12.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 197.8/228.5 MB 12.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 198.2/228.5 MB 11.9 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 198.7/228.5 MB 11.7 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 199.1/228.5 MB 11.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 199.6/228.5 MB 11.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 200.1/228.5 MB 11.3 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 200.6/228.5 MB 11.1 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 201.0/228.5 MB 10.9 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 201.5/228.5 MB 10.7 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 201.9/228.5 MB 10.7 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 202.4/228.5 MB 10.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 202.9/228.5 MB 10.4 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 203.4/228.5 MB 10.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 203.8/228.5 MB 10.7 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 204.3/228.5 MB 10.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 204.8/228.5 MB 10.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 205.3/228.5 MB 10.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 205.8/228.5 MB 10.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 206.3/228.5 MB 10.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 206.7/228.5 MB 10.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 207.2/228.5 MB 10.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 207.7/228.5 MB 10.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 208.2/228.5 MB 10.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 208.7/228.5 MB 10.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 209.2/228.5 MB 10.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 209.7/228.5 MB 10.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 210.2/228.5 MB 10.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 210.7/228.5 MB 10.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 211.2/228.5 MB 10.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 211.7/228.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 212.2/228.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 212.7/228.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 213.2/228.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 213.7/228.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 214.2/228.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 214.7/228.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 215.2/228.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 215.7/228.5 MB 10.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 216.0/228.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 216.0/228.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 217.1/228.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 217.4/228.5 MB 10.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 217.8/228.5 MB 10.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 218.1/228.5 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 218.5/228.5 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 218.9/228.5 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 219.2/228.5 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 219.6/228.5 MB 9.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 220.0/228.5 MB 9.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 220.3/228.5 MB 9.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 220.7/228.5 MB 9.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 221.1/228.5 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 221.5/228.5 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 221.8/228.5 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 222.2/228.5 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 222.6/228.5 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  223.0/228.5 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  223.4/228.5 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  223.8/228.5 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  224.2/228.5 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  224.5/228.5 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  224.9/228.5 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  225.3/228.5 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  225.7/228.5 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  226.1/228.5 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  226.5/228.5 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  226.9/228.5 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  227.3/228.5 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  227.7/228.5 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  228.1/228.5 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  228.5/228.5 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  228.5/228.5 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  228.5/228.5 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  228.5/228.5 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  228.5/228.5 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  228.5/228.5 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 228.5/228.5 MB 7.0 MB/s eta 0:00:00\n",
      "Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.8/3.5 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.1/3.5 MB 7.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.3/3.5 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.6/3.5 MB 7.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.9/3.5 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.1/3.5 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.5/3.5 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.8/3.5 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.0/3.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.3/3.5 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 6.4 MB/s eta 0:00:00\n",
      "Downloading tbb-2021.12.0-py3-none-win_amd64.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 286.4/286.4 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.7 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.7 MB 7.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.9/1.7 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.1/1.7 MB 6.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.4/1.7 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "   ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/5.7 MB 7.0 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.6/5.7 MB 6.6 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.9/5.7 MB 6.4 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.2/5.7 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.5/5.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.8/5.7 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.1/5.7 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.4/5.7 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.7/5.7 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.0/5.7 MB 6.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.3/5.7 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.6/5.7 MB 6.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.0/5.7 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.3/5.7 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.6/5.7 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.9/5.7 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.2/5.7 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.3/5.7 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.4/5.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.7/5.7 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.7/5.7 MB 6.2 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 348.2/536.2 kB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 536.2/536.2 kB 6.7 MB/s eta 0:00:00\n",
      "Installing collected packages: tbb, mpmath, intel-openmp, sympy, networkx, mkl, filelock, torch\n",
      "Successfully installed filelock-3.14.0 intel-openmp-2021.4.0 mkl-2021.4.0 mpmath-1.3.0 networkx-3.3 sympy-1.12 tbb-2021.12.0 torch-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed03c4e-b11b-42c0-bc2e-f6d1ff7a3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import torch\n",
    "\n",
    "# Create a vector as a row\n",
    "tensor_row = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Create a vector as a column\n",
    "tensor_column = torch.tensor(\n",
    "    [\n",
    "        [1],\n",
    "        [2],\n",
    "        [3]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df538d53-415c-487c-a00d-576895bd50ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9497d5ef-73ce-4a40-81b9-16581c094a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.18.0-cp312-cp312-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.3.0-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torchvision-0.18.0-cp312-cp312-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.2 MB 1.3 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.1/1.2 MB 825.8 kB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.2/1.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.4/1.2 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.7/1.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.2/1.2 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 4.4 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.3.0-cp312-cp312-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 1.2/2.4 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 30.2 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "Successfully installed torchaudio-2.3.0 torchvision-0.18.0\n"
     ]
    }
   ],
   "source": [
    " pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f91d4d8a-1b25-4923-8244-2a7ea304c026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- ---------------\n",
      "aiobotocore               2.12.1\n",
      "aiohttp                   3.9.3\n",
      "aioitertools              0.11.0\n",
      "aiosignal                 1.3.1\n",
      "anyio                     4.3.0\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n",
      "attrs                     23.2.0\n",
      "Babel                     2.14.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.1.0\n",
      "botocore                  1.34.51\n",
      "certifi                   2024.2.2\n",
      "cffi                      1.16.0\n",
      "charset-normalizer        3.3.2\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.2\n",
      "contourpy                 1.2.0\n",
      "cramjam                   2.8.2\n",
      "cycler                    0.12.1\n",
      "debugpy                   1.8.1\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "docker                    7.0.0\n",
      "et-xmlfile                1.1.0\n",
      "executing                 2.0.1\n",
      "faiss-cpu                 1.8.0\n",
      "fastavro                  1.9.4\n",
      "fastjsonschema            2.19.1\n",
      "fastparquet               2024.2.0\n",
      "filelock                  3.14.0\n",
      "fonttools                 4.50.0\n",
      "fqdn                      1.5.1\n",
      "frozenlist                1.4.1\n",
      "fsspec                    2024.2.0\n",
      "graphviz                  0.20.3\n",
      "greenlet                  3.0.3\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.4\n",
      "httpx                     0.27.0\n",
      "idna                      3.6\n",
      "intel-openmp              2021.4.0\n",
      "ipykernel                 6.29.3\n",
      "ipython                   8.22.2\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.3\n",
      "jmespath                  1.0.1\n",
      "joblib                    1.3.2\n",
      "json5                     0.9.22\n",
      "jsonpointer               2.4\n",
      "jsonschema                4.21.1\n",
      "jsonschema-specifications 2023.12.1\n",
      "jupyter_client            8.6.1\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.9.1\n",
      "jupyter-lsp               2.2.4\n",
      "jupyter_server            2.13.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.1.4\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.25.4\n",
      "kiwisolver                1.4.5\n",
      "lightgbm                  4.3.0\n",
      "MarkupSafe                2.1.5\n",
      "matplotlib                3.8.3\n",
      "matplotlib-inline         0.1.6\n",
      "mistune                   3.0.2\n",
      "mkl                       2021.4.0\n",
      "mpmath                    1.3.0\n",
      "multidict                 6.0.5\n",
      "nbclient                  0.10.0\n",
      "nbconvert                 7.16.2\n",
      "nbformat                  5.10.2\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.3\n",
      "notebook                  7.1.1\n",
      "notebook_shim             0.2.4\n",
      "numpy                     1.26.4\n",
      "openpyxl                  3.1.2\n",
      "overrides                 7.7.0\n",
      "packaging                 24.0\n",
      "pandas                    2.2.1\n",
      "pandavro                  1.8.0\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.3\n",
      "pillow                    10.2.0\n",
      "pip                       24.0\n",
      "platformdirs              4.2.0\n",
      "prometheus_client         0.20.0\n",
      "prompt-toolkit            3.0.43\n",
      "psutil                    5.9.8\n",
      "pure-eval                 0.2.2\n",
      "pyarrow                   15.0.1\n",
      "pycparser                 2.21\n",
      "pydotplus                 2.0.2\n",
      "Pygments                  2.17.2\n",
      "PyMySQL                   1.1.0\n",
      "pyparsing                 3.1.2\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        2.0.7\n",
      "pytz                      2024.1\n",
      "pywin32                   306\n",
      "pywinpty                  2.0.13\n",
      "PyYAML                    6.0.1\n",
      "pyzmq                     25.1.2\n",
      "referencing               0.33.0\n",
      "requests                  2.31.0\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.18.0\n",
      "s3fs                      2024.2.0\n",
      "scikit-learn              1.4.1.post1\n",
      "scipy                     1.12.0\n",
      "seaborn                   0.13.2\n",
      "Send2Trash                1.8.2\n",
      "six                       1.16.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.5\n",
      "SQLAlchemy                2.0.28\n",
      "stack-data                0.6.3\n",
      "sympy                     1.12\n",
      "tbb                       2021.12.0\n",
      "terminado                 0.18.1\n",
      "threadpoolctl             3.3.0\n",
      "tinycss2                  1.2.1\n",
      "torch                     2.3.0\n",
      "torchaudio                2.3.0\n",
      "torchvision               0.18.0\n",
      "tornado                   6.4\n",
      "tqdm                      4.66.4\n",
      "traitlets                 5.14.2\n",
      "types-python-dateutil     2.8.19.20240311\n",
      "typing_extensions         4.10.0\n",
      "tzdata                    2024.1\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.0.7\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 1.13\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.7.0\n",
      "wrapt                     1.16.0\n",
      "xgboost                   2.0.3\n",
      "yarl                      1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4584c8d-8472-4371-9ad6-4bf4446facd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import torch\n",
    "\n",
    "# Create a vector as a row\n",
    "tensor_row = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Create a vector as a column\n",
    "tensor_column = torch.tensor(\n",
    "    [\n",
    "        [1],\n",
    "        [2],\n",
    "        [3]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0596b84d-c9b9-4710-b116-34ff740bb8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdbae680-62ea-41a2-8abc-fd6d54fc1f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ae7c12-1345-4533-832b-917237fd481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Create a NumPy array\n",
    "vector_row = np.array([1, 2, 3])\n",
    "\n",
    "# Create a tensor from a NumPy array\n",
    "tensor_row = torch.from_numpy(vector_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0296227-948e-4292-bb6b-5966aa88e285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8135b1e0-6cbf-455d-ba2c-aa4791ec4a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], dtype=torch.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "220bae03-46c5-456a-80b4-5d6a7c269526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "tensor = torch.tensor(\n",
    "[\n",
    "[0, 0],\n",
    "[0, 1],\n",
    "[3, 0]\n",
    "]\n",
    ")\n",
    "\n",
    "# Create a sparse tensor from a regular tensor\n",
    "sparse_tensor = tensor.to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba8c29a1-a535-4927-a8ae-f99d53c51db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[1, 2],\n",
       "                       [1, 0]]),\n",
       "       values=tensor([1, 3]),\n",
       "       size=(3, 2), nnz=2, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "755f645b-387a-4e61-9720-f88748305c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tensor))\n",
    "print(type(sparse_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78250d84-3abe-440c-9d46-c50f2645c9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "import torch\n",
    "\n",
    "# Create vector tensor\n",
    "vector = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create matrix tensor\n",
    "matrix = torch.tensor(\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [7, 8, 9]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Select third element of vector\n",
    "vector[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4e79df3-3f81-4e65-b895-170fef17e8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c3b55bd-61fa-4e88-a89d-f0c3fd23e6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select second row, second column\n",
    "matrix[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a78f9013-b4bf-4cc7-86eb-5130430afc6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c7e1deb-70cf-470a-a2ad-347963bdc731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select all elements of a vector\n",
    "vector[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75f902fb-263a-4a24-81a5-b9400900e618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select everything up to and including the third element\n",
    "vector[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9780d639-02cf-44e6-baf8-326c36800f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select everything after the third element\n",
    "vector[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "480af453-7488-45f6-883e-3418545bb833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the last element\n",
    "vector[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf0e0989-1c8c-4672-9d63-81fe983accfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the first two rows and all columns of a matrix\n",
    "matrix[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d17c1078-f38b-4a00-afd3-4516a55a8fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [5],\n",
       "        [8]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select all rows and the second column\n",
    "matrix[:,1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b674806-9b9d-487e-8b59-6f6a3fee7d3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "step must be greater than zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Reverse the vector\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mvector\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: step must be greater than zero"
     ]
    }
   ],
   "source": [
    "# Reverse the vector\n",
    "vector[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "249c3fe2-92a8-49df-bb33-f3193e6a11f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 5, 4, 3, 2, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.flip(dims=(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6eaec004-f841-4bea-ba58-059e8aa658ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "tensor = torch.tensor([[1,2,3], [1,2,3]])\n",
    "\n",
    "# Get the shape of the tensor\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19a09be1-541b-4d62-8619-e6ad45fec314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data type of items in the tensor\n",
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5d9f1f9-747b-4edd-a695-a3262b49e7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.strided"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the layout of the tensor\n",
    "tensor.layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92dda0c8-109b-4859-bd82-b02199a3bcb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the device being used by the tensor\n",
    "tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24b68de0-5ce2-4484-aeac-a8bf03767b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100, 200, 300])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Broadcast an arithmetic operation to all elements in a tensor\n",
    "tensor * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10bf0832-7f63-45c8-926d-b96fbd6d3b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "torch.tensor([1,2,3])\n",
    "\n",
    "# Find the largest value\n",
    "tensor.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eae9cd9a-88d0-4c87-8d2b-dbf748fbc265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the smallest value\n",
    "tensor.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b9a507e-4069-48ae-84f4-44d8571edfa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a multidimensional tensor\n",
    "tensor = torch.tensor([[1,2,3],[1,2,5]])\n",
    "\n",
    "# Find the largest value\n",
    "tensor.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dae13146-af4d-4048-b359-d1008c485e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6],\n",
       "        [ 7,  8,  9, 10, 11, 12]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "import torch\n",
    "\n",
    "# Create 4x3 tensor\n",
    "tensor = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9],\n",
    "                       [10, 11, 12]])\n",
    "\n",
    "# Reshape tensor into 2x6 tensor\n",
    "tensor.reshape(2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69854d55-a63a-4539-a6bb-da5b52696490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2],\n",
       "         [3]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "import torch\n",
    "\n",
    "# Create a two-dimensional tensor\n",
    "tensor = torch.tensor([[[1,2,3]]])\n",
    "\n",
    "# Transpose it\n",
    "tensor.mT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dfe0be4d-86ea-4e66-b80d-d7184039aec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1]],\n",
       "\n",
       "        [[2]],\n",
       "\n",
       "        [[3]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.permute(*torch.arange(tensor.ndim - 1, -1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5baf304e-0133-44e1-899c-db39a43b3a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "import torch\n",
    "\n",
    "# Create tensor\n",
    "tensor = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]])\n",
    "\n",
    "# Flatten tensor\n",
    "tensor.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca317094-376d-4f21-acd7-2b7cf9c44716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "import torch\n",
    "\n",
    "# Create one tensor\n",
    "tensor_1 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Create another tensor\n",
    "tensor_2 = torch.tensor([4, 5, 6])\n",
    "\n",
    "# Calculate the dot product of the two tensors\n",
    "tensor_1.dot(tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c630d52-1ec9-4b13-af3a-c64a3c34265d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 10, 18])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "import torch\n",
    "\n",
    "# Create one tensor\n",
    "tensor_1 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Create another tensor\n",
    "tensor_2 = torch.tensor([4, 5, 6])\n",
    "\n",
    "# Multiply the two tensors\n",
    "tensor_1 * tensor_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18582e02-857b-46a9-ad73-cc912cd26a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 7, 9])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_1+tensor_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5eccd8be-09d8-4eef-838e-f4a75bb42404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3, -3, -3])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_1-tensor_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "daf768d1-9f54-43b8-bc5d-079013f8e7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2500, 0.4000, 0.5000])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_1/tensor_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b6874c0-7c42-404f-8343-1fd3a742f28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8ff852d-5f6c-4289-9df1-33280f242b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.cudnn.enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f90a102-9f97-4bfc-8424-52029b6b0767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- ---------------\n",
      "aiobotocore               2.12.1\n",
      "aiohttp                   3.9.3\n",
      "aioitertools              0.11.0\n",
      "aiosignal                 1.3.1\n",
      "anyio                     4.3.0\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n",
      "attrs                     23.2.0\n",
      "Babel                     2.14.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.1.0\n",
      "botocore                  1.34.51\n",
      "certifi                   2024.2.2\n",
      "cffi                      1.16.0\n",
      "charset-normalizer        3.3.2\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.2\n",
      "contourpy                 1.2.0\n",
      "cramjam                   2.8.2\n",
      "cycler                    0.12.1\n",
      "debugpy                   1.8.1\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "docker                    7.0.0\n",
      "et-xmlfile                1.1.0\n",
      "executing                 2.0.1\n",
      "faiss-cpu                 1.8.0\n",
      "fastavro                  1.9.4\n",
      "fastjsonschema            2.19.1\n",
      "fastparquet               2024.2.0\n",
      "filelock                  3.14.0\n",
      "fonttools                 4.50.0\n",
      "fqdn                      1.5.1\n",
      "frozenlist                1.4.1\n",
      "fsspec                    2024.2.0\n",
      "graphviz                  0.20.3\n",
      "greenlet                  3.0.3\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.4\n",
      "httpx                     0.27.0\n",
      "idna                      3.6\n",
      "intel-openmp              2021.4.0\n",
      "ipykernel                 6.29.3\n",
      "ipython                   8.22.2\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.3\n",
      "jmespath                  1.0.1\n",
      "joblib                    1.3.2\n",
      "json5                     0.9.22\n",
      "jsonpointer               2.4\n",
      "jsonschema                4.21.1\n",
      "jsonschema-specifications 2023.12.1\n",
      "jupyter_client            8.6.1\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.9.1\n",
      "jupyter-lsp               2.2.4\n",
      "jupyter_server            2.13.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.1.4\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.25.4\n",
      "kiwisolver                1.4.5\n",
      "lightgbm                  4.3.0\n",
      "MarkupSafe                2.1.5\n",
      "matplotlib                3.8.3\n",
      "matplotlib-inline         0.1.6\n",
      "mistune                   3.0.2\n",
      "mkl                       2021.4.0\n",
      "mpmath                    1.3.0\n",
      "multidict                 6.0.5\n",
      "nbclient                  0.10.0\n",
      "nbconvert                 7.16.2\n",
      "nbformat                  5.10.2\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.3\n",
      "notebook                  7.1.1\n",
      "notebook_shim             0.2.4\n",
      "numpy                     1.26.4\n",
      "openpyxl                  3.1.2\n",
      "overrides                 7.7.0\n",
      "packaging                 24.0\n",
      "pandas                    2.2.1\n",
      "pandavro                  1.8.0\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.3\n",
      "pillow                    10.2.0\n",
      "pip                       24.0\n",
      "platformdirs              4.2.0\n",
      "prometheus_client         0.20.0\n",
      "prompt-toolkit            3.0.43\n",
      "psutil                    5.9.8\n",
      "pure-eval                 0.2.2\n",
      "pyarrow                   15.0.1\n",
      "pycparser                 2.21\n",
      "pydotplus                 2.0.2\n",
      "Pygments                  2.17.2\n",
      "PyMySQL                   1.1.0\n",
      "pyparsing                 3.1.2\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        2.0.7\n",
      "pytz                      2024.1\n",
      "pywin32                   306\n",
      "pywinpty                  2.0.13\n",
      "PyYAML                    6.0.1\n",
      "pyzmq                     25.1.2\n",
      "referencing               0.33.0\n",
      "requests                  2.31.0\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.18.0\n",
      "s3fs                      2024.2.0\n",
      "scikit-learn              1.4.1.post1\n",
      "scipy                     1.12.0\n",
      "seaborn                   0.13.2\n",
      "Send2Trash                1.8.2\n",
      "six                       1.16.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.5\n",
      "SQLAlchemy                2.0.28\n",
      "stack-data                0.6.3\n",
      "sympy                     1.12\n",
      "tbb                       2021.12.0\n",
      "terminado                 0.18.1\n",
      "threadpoolctl             3.3.0\n",
      "tinycss2                  1.2.1\n",
      "torch                     2.2.2+cu121\n",
      "torchaudio                2.2.2+cu121\n",
      "torchvision               0.17.2+cu121\n",
      "tornado                   6.4\n",
      "tqdm                      4.66.4\n",
      "traitlets                 5.14.2\n",
      "types-python-dateutil     2.8.19.20240311\n",
      "typing_extensions         4.10.0\n",
      "tzdata                    2024.1\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.0.7\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 1.13\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.7.0\n",
      "wrapt                     1.16.0\n",
      "xgboost                   2.0.3\n",
      "yarl                      1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "917716d5-4bea-4139-ae04-e7bf6550e430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~orch'.\n",
      "  You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch==2.2.2\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.2.2%2Bcu121-cp312-cp312-win_amd64.whl (2454.8 MB)\n",
      "     ---------------------------------------- 0.0/2.5 GB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.5 GB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.5 GB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.5 GB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.5 GB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.5 GB 201.8 kB/s eta 3:22:46\n",
      "     ---------------------------------------- 0.0/2.5 GB 201.8 kB/s eta 3:22:46\n",
      "     ---------------------------------------- 0.0/2.5 GB 201.8 kB/s eta 3:22:46\n",
      "     ---------------------------------------- 0.0/2.5 GB 201.8 kB/s eta 3:22:46\n",
      "     ---------------------------------------- 0.0/2.5 GB 201.8 kB/s eta 3:22:46\n",
      "     ---------------------------------------- 0.0/2.5 GB 291.2 kB/s eta 2:20:30\n",
      "     ---------------------------------------- 0.0/2.5 GB 291.2 kB/s eta 2:20:30\n",
      "     ---------------------------------------- 0.0/2.5 GB 291.2 kB/s eta 2:20:30\n",
      "     ---------------------------------------- 0.0/2.5 GB 291.2 kB/s eta 2:20:30\n",
      "     ---------------------------------------- 0.0/2.5 GB 454.0 kB/s eta 1:30:07\n",
      "     ---------------------------------------- 0.0/2.5 GB 464.0 kB/s eta 1:28:10\n",
      "     ---------------------------------------- 0.0/2.5 GB 464.0 kB/s eta 1:28:10\n",
      "     ---------------------------------------- 0.0/2.5 GB 464.0 kB/s eta 1:28:10\n",
      "     ---------------------------------------- 0.0/2.5 GB 758.0 kB/s eta 0:53:58\n",
      "     ---------------------------------------- 0.0/2.5 GB 740.1 kB/s eta 0:55:16\n",
      "     ---------------------------------------- 0.0/2.5 GB 713.3 kB/s eta 0:57:21\n",
      "     ---------------------------------------- 0.0/2.5 GB 713.3 kB/s eta 0:57:21\n",
      "     ---------------------------------------- 0.0/2.5 GB 747.5 kB/s eta 0:54:43\n",
      "     ---------------------------------------- 0.0/2.5 GB 1.3 MB/s eta 0:32:41\n",
      "     ---------------------------------------- 0.0/2.5 GB 1.2 MB/s eta 0:33:38\n",
      "     ---------------------------------------- 0.0/2.5 GB 1.2 MB/s eta 0:33:38\n",
      "     ---------------------------------------- 0.0/2.5 GB 1.2 MB/s eta 0:33:38\n",
      "     ---------------------------------------- 0.0/2.5 GB 2.1 MB/s eta 0:19:24\n",
      "     ---------------------------------------- 0.0/2.5 GB 2.1 MB/s eta 0:19:10\n",
      "     ---------------------------------------- 0.0/2.5 GB 2.1 MB/s eta 0:19:45\n",
      "     ---------------------------------------- 0.0/2.5 GB 2.1 MB/s eta 0:19:45\n",
      "     ---------------------------------------- 0.0/2.5 GB 2.6 MB/s eta 0:15:51\n",
      "     ---------------------------------------- 0.0/2.5 GB 3.5 MB/s eta 0:11:44\n",
      "     ---------------------------------------- 0.0/2.5 GB 3.4 MB/s eta 0:12:04\n",
      "     ---------------------------------------- 0.0/2.5 GB 3.4 MB/s eta 0:12:04\n",
      "     ---------------------------------------- 0.0/2.5 GB 3.4 MB/s eta 0:12:04\n",
      "     ---------------------------------------- 0.0/2.5 GB 4.6 MB/s eta 0:08:56\n",
      "     ---------------------------------------- 0.0/2.5 GB 4.5 MB/s eta 0:09:09\n",
      "     ---------------------------------------- 0.0/2.5 GB 4.5 MB/s eta 0:09:09\n",
      "     ---------------------------------------- 0.0/2.5 GB 4.5 MB/s eta 0:09:09\n",
      "     ---------------------------------------- 0.0/2.5 GB 5.2 MB/s eta 0:07:48\n",
      "     ---------------------------------------- 0.0/2.5 GB 5.5 MB/s eta 0:07:22\n",
      "     ---------------------------------------- 0.0/2.5 GB 5.4 MB/s eta 0:07:32\n",
      "     ---------------------------------------- 0.0/2.5 GB 5.4 MB/s eta 0:07:32\n",
      "     ---------------------------------------- 0.0/2.5 GB 8.5 MB/s eta 0:04:48\n",
      "     ---------------------------------------- 0.0/2.5 GB 13.6 MB/s eta 0:03:00\n",
      "     ---------------------------------------- 0.0/2.5 GB 14.9 MB/s eta 0:02:45\n",
      "     ---------------------------------------- 0.0/2.5 GB 14.9 MB/s eta 0:02:45\n",
      "     ---------------------------------------- 0.0/2.5 GB 14.9 MB/s eta 0:02:45\n",
      "     ---------------------------------------- 0.0/2.5 GB 16.4 MB/s eta 0:02:30\n",
      "     ---------------------------------------- 0.0/2.5 GB 15.2 MB/s eta 0:02:41\n",
      "     ---------------------------------------- 0.0/2.5 GB 15.2 MB/s eta 0:02:41\n",
      "     ---------------------------------------- 0.0/2.5 GB 15.2 MB/s eta 0:02:41\n",
      "     ---------------------------------------- 0.0/2.5 GB 12.8 MB/s eta 0:03:10\n",
      "     ---------------------------------------- 0.0/2.5 GB 16.4 MB/s eta 0:02:29\n",
      "     ---------------------------------------- 0.0/2.5 GB 15.2 MB/s eta 0:02:41\n",
      "     ---------------------------------------- 0.0/2.5 GB 15.2 MB/s eta 0:02:41\n",
      "     ---------------------------------------- 0.0/2.5 GB 13.4 MB/s eta 0:03:03\n",
      "     ---------------------------------------- 0.0/2.5 GB 16.4 MB/s eta 0:02:29\n",
      "     ---------------------------------------- 0.0/2.5 GB 16.0 MB/s eta 0:02:33\n",
      "     ---------------------------------------- 0.0/2.5 GB 16.0 MB/s eta 0:02:33\n",
      "     ---------------------------------------- 0.0/2.5 GB 16.0 MB/s eta 0:02:33\n",
      "     ---------------------------------------- 0.0/2.5 GB 16.8 MB/s eta 0:02:25\n",
      "     ---------------------------------------- 0.0/2.5 GB 16.0 MB/s eta 0:02:32\n",
      "     ---------------------------------------- 0.0/2.5 GB 15.6 MB/s eta 0:02:36\n",
      "     ---------------------------------------- 0.0/2.5 GB 15.6 MB/s eta 0:02:36\n",
      "     ---------------------------------------- 0.0/2.5 GB 16.8 MB/s eta 0:02:25\n",
      "     ---------------------------------------- 0.0/2.5 GB 16.8 MB/s eta 0:02:25\n",
      "     ---------------------------------------- 0.0/2.5 GB 16.0 MB/s eta 0:02:33\n",
      "     ---------------------------------------- 0.0/2.5 GB 16.0 MB/s eta 0:02:33\n",
      "     ---------------------------------------- 0.0/2.5 GB 17.3 MB/s eta 0:02:21\n",
      "      --------------------------------------- 0.0/2.5 GB 17.2 MB/s eta 0:02:21\n",
      "      --------------------------------------- 0.0/2.5 GB 21.1 MB/s eta 0:01:55\n",
      "      --------------------------------------- 0.0/2.5 GB 21.1 MB/s eta 0:01:55\n",
      "      --------------------------------------- 0.0/2.5 GB 21.1 MB/s eta 0:01:55\n",
      "      --------------------------------------- 0.0/2.5 GB 17.2 MB/s eta 0:02:21\n",
      "      --------------------------------------- 0.0/2.5 GB 22.6 MB/s eta 0:01:48\n",
      "      --------------------------------------- 0.0/2.5 GB 21.8 MB/s eta 0:01:51\n",
      "      --------------------------------------- 0.0/2.5 GB 21.8 MB/s eta 0:01:51\n",
      "      --------------------------------------- 0.0/2.5 GB 17.2 MB/s eta 0:02:21\n",
      "      --------------------------------------- 0.0/2.5 GB 22.6 MB/s eta 0:01:47\n",
      "      --------------------------------------- 0.0/2.5 GB 22.6 MB/s eta 0:01:47\n",
      "      --------------------------------------- 0.0/2.5 GB 22.6 MB/s eta 0:01:47\n",
      "      --------------------------------------- 0.0/2.5 GB 22.6 MB/s eta 0:01:47\n",
      "      --------------------------------------- 0.0/2.5 GB 17.2 MB/s eta 0:02:21\n",
      "      --------------------------------------- 0.0/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "      --------------------------------------- 0.0/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "      --------------------------------------- 0.0/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "      --------------------------------------- 0.0/2.5 GB 17.3 MB/s eta 0:02:20\n",
      "      --------------------------------------- 0.0/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "      --------------------------------------- 0.0/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "      --------------------------------------- 0.0/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "      --------------------------------------- 0.0/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "      --------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "      --------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "      --------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "      --------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "      --------------------------------------- 0.1/2.5 GB 17.3 MB/s eta 0:02:20\n",
      "      --------------------------------------- 0.1/2.5 GB 24.2 MB/s eta 0:01:39\n",
      "      --------------------------------------- 0.1/2.5 GB 24.2 MB/s eta 0:01:39\n",
      "      --------------------------------------- 0.1/2.5 GB 24.2 MB/s eta 0:01:39\n",
      "      --------------------------------------- 0.1/2.5 GB 18.2 MB/s eta 0:02:12\n",
      "      --------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "      --------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "      --------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "      --------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "     - -------------------------------------- 0.1/2.5 GB 17.7 MB/s eta 0:02:16\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "     - -------------------------------------- 0.1/2.5 GB 17.7 MB/s eta 0:02:16\n",
      "     - -------------------------------------- 0.1/2.5 GB 22.6 MB/s eta 0:01:46\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:43\n",
      "     - -------------------------------------- 0.1/2.5 GB 17.7 MB/s eta 0:02:15\n",
      "     - -------------------------------------- 0.1/2.5 GB 24.3 MB/s eta 0:01:39\n",
      "     - -------------------------------------- 0.1/2.5 GB 24.3 MB/s eta 0:01:39\n",
      "     - -------------------------------------- 0.1/2.5 GB 24.3 MB/s eta 0:01:39\n",
      "     - -------------------------------------- 0.1/2.5 GB 17.3 MB/s eta 0:02:19\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:42\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:42\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:42\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:42\n",
      "     - -------------------------------------- 0.1/2.5 GB 22.6 MB/s eta 0:01:46\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:42\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:42\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:42\n",
      "     - -------------------------------------- 0.1/2.5 GB 17.7 MB/s eta 0:02:15\n",
      "     - -------------------------------------- 0.1/2.5 GB 24.2 MB/s eta 0:01:38\n",
      "     - -------------------------------------- 0.1/2.5 GB 24.2 MB/s eta 0:01:38\n",
      "     - -------------------------------------- 0.1/2.5 GB 24.2 MB/s eta 0:01:38\n",
      "     - -------------------------------------- 0.1/2.5 GB 17.7 MB/s eta 0:02:14\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:42\n",
      "     - -------------------------------------- 0.1/2.5 GB 24.2 MB/s eta 0:01:38\n",
      "     - -------------------------------------- 0.1/2.5 GB 24.2 MB/s eta 0:01:38\n",
      "     - -------------------------------------- 0.1/2.5 GB 24.2 MB/s eta 0:01:38\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 18.2 MB/s eta 0:02:10\n",
      "     - -------------------------------------- 0.1/2.5 GB 24.2 MB/s eta 0:01:38\n",
      "     - -------------------------------------- 0.1/2.5 GB 24.2 MB/s eta 0:01:38\n",
      "     - -------------------------------------- 0.1/2.5 GB 24.2 MB/s eta 0:01:38\n",
      "     - -------------------------------------- 0.1/2.5 GB 24.2 MB/s eta 0:01:38\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 17.7 MB/s eta 0:02:13\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 17.2 MB/s eta 0:02:17\n",
      "     - -------------------------------------- 0.1/2.5 GB 22.5 MB/s eta 0:01:45\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 17.7 MB/s eta 0:02:13\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 17.2 MB/s eta 0:02:17\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:41\n",
      "     - -------------------------------------- 0.1/2.5 GB 17.7 MB/s eta 0:02:13\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     - -------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     - -------------------------------------- 0.1/2.5 GB 17.7 MB/s eta 0:02:12\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 17.2 MB/s eta 0:02:16\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 17.7 MB/s eta 0:02:12\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 17.3 MB/s eta 0:02:15\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:40\n",
      "     -- ------------------------------------- 0.1/2.5 GB 17.2 MB/s eta 0:02:15\n",
      "     -- ------------------------------------- 0.1/2.5 GB 24.2 MB/s eta 0:01:36\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:39\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:39\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:39\n",
      "     -- ------------------------------------- 0.1/2.5 GB 17.7 MB/s eta 0:02:11\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:39\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:39\n",
      "     -- ------------------------------------- 0.1/2.5 GB 23.4 MB/s eta 0:01:39\n",
      "     -- ------------------------------------- 0.1/2.5 GB 17.7 MB/s eta 0:02:11\n",
      "     -- ------------------------------------- 0.2/2.5 GB 23.4 MB/s eta 0:01:39\n",
      "     -- ------------------------------------- 0.2/2.5 GB 23.4 MB/s eta 0:01:39\n",
      "     -- ------------------------------------- 0.2/2.5 GB 23.4 MB/s eta 0:01:39\n",
      "     -- ------------------------------------- 0.2/2.5 GB 17.7 MB/s eta 0:02:11\n",
      "     -- ------------------------------------- 0.2/2.5 GB 23.4 MB/s eta 0:01:39\n",
      "     -- ------------------------------------- 0.2/2.5 GB 24.2 MB/s eta 0:01:35\n",
      "     -- ------------------------------------- 0.2/2.5 GB 24.2 MB/s eta 0:01:35\n",
      "     -- ------------------------------------- 0.2/2.5 GB 24.2 MB/s eta 0:01:35\n",
      "     -- ------------------------------------- 0.2/2.5 GB 23.4 MB/s eta 0:01:39\n",
      "     -- ------------------------------------- 0.2/2.5 GB 24.2 MB/s eta 0:01:35\n",
      "     -- ------------------------------------- 0.2/2.5 GB 24.2 MB/s eta 0:01:35\n",
      "     -- ------------------------------------- 0.2/2.5 GB 24.2 MB/s eta 0:01:35\n",
      "     -- ------------------------------------- 0.2/2.5 GB 18.2 MB/s eta 0:02:07\n",
      "     -- ------------------------------------- 0.2/2.5 GB 24.2 MB/s eta 0:01:35\n",
      "     -- ------------------------------------- 0.2/2.5 GB 24.2 MB/s eta 0:01:35\n",
      "     -- ------------------------------------- 0.2/2.5 GB 24.2 MB/s eta 0:01:35\n",
      "     -- ------------------------------------- 0.2/2.5 GB 24.2 MB/s eta 0:01:35\n",
      "     -- ------------------------------------- 0.2/2.5 GB 23.4 MB/s eta 0:01:38\n",
      "     -- ------------------------------------- 0.2/2.5 GB 22.6 MB/s eta 0:01:42\n",
      "     -- ------------------------------------- 0.2/2.5 GB 22.6 MB/s eta 0:01:42\n",
      "     -- ------------------------------------- 0.2/2.5 GB 22.6 MB/s eta 0:01:42\n",
      "     -- ------------------------------------- 0.2/2.5 GB 17.7 MB/s eta 0:02:10\n",
      "     -- ------------------------------------- 0.2/2.5 GB 24.2 MB/s eta 0:01:35\n",
      "     -- ------------------------------------- 0.2/2.5 GB 24.2 MB/s eta 0:01:35\n",
      "     -- ------------------------------------- 0.2/2.5 GB 24.2 MB/s eta 0:01:35\n",
      "     -- ------------------------------------- 0.2/2.5 GB 17.7 MB/s eta 0:02:09\n",
      "     -- ------------------------------------- 0.2/2.5 GB 22.6 MB/s eta 0:01:41\n",
      "     -- ------------------------------------- 0.2/2.5 GB 24.2 MB/s eta 0:01:34\n",
      "     -- ------------------------------------- 0.2/2.5 GB 24.2 MB/s eta 0:01:34\n",
      "     -- ------------------------------------- 0.2/2.5 GB 24.2 MB/s eta 0:01:34\n",
      "     -- ------------------------------------- 0.2/2.5 GB 17.7 MB/s eta 0:02:09\n",
      "     -- ------------------------------------- 0.2/2.5 GB 23.4 MB/s eta 0:01:38\n",
      "     -- ------------------------------------- 0.2/2.5 GB 23.4 MB/s eta 0:01:38\n",
      "     -- ------------------------------------- 0.2/2.5 GB 23.4 MB/s eta 0:01:38\n",
      "     -- ------------------------------------- 0.2/2.5 GB 17.3 MB/s eta 0:02:12\n",
      "     -- ------------------------------------- 0.2/2.5 GB 23.4 MB/s eta 0:01:38\n",
      "     -- ------------------------------------- 0.2/2.5 GB 23.4 MB/s eta 0:01:38\n",
      "     -- ------------------------------------- 0.2/2.5 GB 23.4 MB/s eta 0:01:38\n",
      "     -- ------------------------------------- 0.2/2.5 GB 23.4 MB/s eta 0:01:38\n",
      "     --- ------------------------------------ 0.2/2.5 GB 17.7 MB/s eta 0:02:09\n",
      "     --- ------------------------------------ 0.2/2.5 GB 22.6 MB/s eta 0:01:41\n",
      "     --- ------------------------------------ 0.2/2.5 GB 22.6 MB/s eta 0:01:41\n",
      "     --- ------------------------------------ 0.2/2.5 GB 22.6 MB/s eta 0:01:41\n",
      "     --- ------------------------------------ 0.2/2.5 GB 17.7 MB/s eta 0:02:09\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:37\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:37\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:37\n",
      "     --- ------------------------------------ 0.2/2.5 GB 17.7 MB/s eta 0:02:08\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:37\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:37\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:37\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:37\n",
      "     --- ------------------------------------ 0.2/2.5 GB 17.7 MB/s eta 0:02:08\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:37\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:37\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:37\n",
      "     --- ------------------------------------ 0.2/2.5 GB 17.7 MB/s eta 0:02:08\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:37\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:37\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:37\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:37\n",
      "     --- ------------------------------------ 0.2/2.5 GB 17.2 MB/s eta 0:02:11\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:36\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:36\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:36\n",
      "     --- ------------------------------------ 0.2/2.5 GB 17.7 MB/s eta 0:02:07\n",
      "     --- ------------------------------------ 0.2/2.5 GB 24.2 MB/s eta 0:01:33\n",
      "     --- ------------------------------------ 0.2/2.5 GB 24.2 MB/s eta 0:01:33\n",
      "     --- ------------------------------------ 0.2/2.5 GB 24.2 MB/s eta 0:01:33\n",
      "     --- ------------------------------------ 0.2/2.5 GB 24.2 MB/s eta 0:01:33\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:36\n",
      "     --- ------------------------------------ 0.2/2.5 GB 24.2 MB/s eta 0:01:33\n",
      "     --- ------------------------------------ 0.2/2.5 GB 24.2 MB/s eta 0:01:33\n",
      "     --- ------------------------------------ 0.2/2.5 GB 24.2 MB/s eta 0:01:33\n",
      "     --- ------------------------------------ 0.2/2.5 GB 17.7 MB/s eta 0:02:07\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:36\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:36\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:36\n",
      "     --- ------------------------------------ 0.2/2.5 GB 17.7 MB/s eta 0:02:07\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:36\n",
      "     --- ------------------------------------ 0.2/2.5 GB 24.2 MB/s eta 0:01:33\n",
      "     --- ------------------------------------ 0.2/2.5 GB 24.2 MB/s eta 0:01:33\n",
      "     --- ------------------------------------ 0.2/2.5 GB 24.2 MB/s eta 0:01:33\n",
      "     --- ------------------------------------ 0.2/2.5 GB 17.7 MB/s eta 0:02:06\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:36\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:36\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:36\n",
      "     --- ------------------------------------ 0.2/2.5 GB 17.7 MB/s eta 0:02:06\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     --- ------------------------------------ 0.2/2.5 GB 24.2 MB/s eta 0:01:32\n",
      "     --- ------------------------------------ 0.2/2.5 GB 24.2 MB/s eta 0:01:32\n",
      "     --- ------------------------------------ 0.2/2.5 GB 24.2 MB/s eta 0:01:32\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:36\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     --- ------------------------------------ 0.2/2.5 GB 18.2 MB/s eta 0:02:03\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     --- ------------------------------------ 0.2/2.5 GB 17.7 MB/s eta 0:02:06\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     --- ------------------------------------ 0.2/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     ---- ----------------------------------- 0.2/2.5 GB 17.7 MB/s eta 0:02:05\n",
      "     ---- ----------------------------------- 0.2/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     ---- ----------------------------------- 0.2/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     ---- ----------------------------------- 0.2/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     ---- ----------------------------------- 0.2/2.5 GB 17.7 MB/s eta 0:02:05\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 24.2 MB/s eta 0:01:31\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:35\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 17.7 MB/s eta 0:02:05\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 24.2 MB/s eta 0:01:31\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 24.2 MB/s eta 0:01:31\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 24.2 MB/s eta 0:01:31\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 18.2 MB/s eta 0:02:01\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:34\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:37\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:37\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:37\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 17.7 MB/s eta 0:02:04\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:34\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:34\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:34\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 18.2 MB/s eta 0:02:01\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:34\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:34\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:34\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:34\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 17.7 MB/s eta 0:02:04\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:34\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:34\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:34\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 17.7 MB/s eta 0:02:03\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:37\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 21.8 MB/s eta 0:01:40\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 21.8 MB/s eta 0:01:40\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 17.7 MB/s eta 0:02:03\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 17.7 MB/s eta 0:02:03\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:37\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:37\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:37\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 17.7 MB/s eta 0:02:03\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:36\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:36\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:36\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 17.7 MB/s eta 0:02:03\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 24.2 MB/s eta 0:01:30\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 24.2 MB/s eta 0:01:30\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 24.2 MB/s eta 0:01:30\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 24.2 MB/s eta 0:01:30\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 17.3 MB/s eta 0:02:06\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 22.5 MB/s eta 0:01:36\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 22.5 MB/s eta 0:01:36\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 22.5 MB/s eta 0:01:36\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 17.2 MB/s eta 0:02:06\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:36\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:36\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:36\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 16.8 MB/s eta 0:02:09\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 17.2 MB/s eta 0:02:05\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 17.7 MB/s eta 0:02:02\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 17.7 MB/s eta 0:02:01\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 17.7 MB/s eta 0:02:01\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:32\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 24.2 MB/s eta 0:01:28\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 24.2 MB/s eta 0:01:28\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 24.2 MB/s eta 0:01:28\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 17.7 MB/s eta 0:02:01\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:31\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:31\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:31\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 17.7 MB/s eta 0:02:01\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:31\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:35\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:35\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:35\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:31\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:31\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:31\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:31\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 17.7 MB/s eta 0:02:00\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:34\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:34\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 22.6 MB/s eta 0:01:34\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 17.7 MB/s eta 0:02:00\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 24.2 MB/s eta 0:01:28\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 24.2 MB/s eta 0:01:28\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 24.2 MB/s eta 0:01:28\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 24.2 MB/s eta 0:01:28\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 17.7 MB/s eta 0:02:00\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:31\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:31\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:31\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 17.7 MB/s eta 0:02:00\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:31\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 17.7 MB/s eta 0:01:59\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 17.7 MB/s eta 0:01:59\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 17.7 MB/s eta 0:01:58\n",
      "     ------ --------------------------------- 0.4/2.5 GB 24.2 MB/s eta 0:01:27\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ------ --------------------------------- 0.4/2.5 GB 17.7 MB/s eta 0:01:58\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:30\n",
      "     ------ --------------------------------- 0.4/2.5 GB 17.7 MB/s eta 0:01:58\n",
      "     ------ --------------------------------- 0.4/2.5 GB 24.2 MB/s eta 0:01:26\n",
      "     ------ --------------------------------- 0.4/2.5 GB 24.2 MB/s eta 0:01:26\n",
      "     ------ --------------------------------- 0.4/2.5 GB 24.2 MB/s eta 0:01:26\n",
      "     ------ --------------------------------- 0.4/2.5 GB 24.2 MB/s eta 0:01:26\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 17.7 MB/s eta 0:01:58\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 17.7 MB/s eta 0:01:57\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 17.7 MB/s eta 0:01:57\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:29\n",
      "     ------ --------------------------------- 0.4/2.5 GB 17.3 MB/s eta 0:02:00\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 17.3 MB/s eta 0:02:00\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 17.7 MB/s eta 0:01:56\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 17.2 MB/s eta 0:01:59\n",
      "     ------ --------------------------------- 0.4/2.5 GB 22.5 MB/s eta 0:01:31\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 17.7 MB/s eta 0:01:56\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:28\n",
      "     ------ --------------------------------- 0.4/2.5 GB 17.7 MB/s eta 0:01:55\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:27\n",
      "     ------ --------------------------------- 0.4/2.5 GB 24.2 MB/s eta 0:01:24\n",
      "     ------ --------------------------------- 0.4/2.5 GB 24.2 MB/s eta 0:01:24\n",
      "     ------ --------------------------------- 0.4/2.5 GB 24.2 MB/s eta 0:01:24\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:27\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:27\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:27\n",
      "     ------ --------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:27\n",
      "     ------- -------------------------------- 0.4/2.5 GB 17.7 MB/s eta 0:01:55\n",
      "     ------- -------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:27\n",
      "     ------- -------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:27\n",
      "     ------- -------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:27\n",
      "     ------- -------------------------------- 0.4/2.5 GB 17.2 MB/s eta 0:01:58\n",
      "     ------- -------------------------------- 0.4/2.5 GB 17.2 MB/s eta 0:01:58\n",
      "     ------- -------------------------------- 0.4/2.5 GB 22.6 MB/s eta 0:01:30\n",
      "     ------- -------------------------------- 0.4/2.5 GB 22.6 MB/s eta 0:01:30\n",
      "     ------- -------------------------------- 0.4/2.5 GB 22.6 MB/s eta 0:01:30\n",
      "     ------- -------------------------------- 0.4/2.5 GB 17.2 MB/s eta 0:01:58\n",
      "     ------- -------------------------------- 0.4/2.5 GB 22.6 MB/s eta 0:01:30\n",
      "     ------- -------------------------------- 0.4/2.5 GB 22.6 MB/s eta 0:01:30\n",
      "     ------- -------------------------------- 0.4/2.5 GB 22.6 MB/s eta 0:01:30\n",
      "     ------- -------------------------------- 0.4/2.5 GB 16.8 MB/s eta 0:02:01\n",
      "     ------- -------------------------------- 0.4/2.5 GB 24.2 MB/s eta 0:01:23\n",
      "     ------- -------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:26\n",
      "     ------- -------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:26\n",
      "     ------- -------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:26\n",
      "     ------- -------------------------------- 0.4/2.5 GB 17.7 MB/s eta 0:01:54\n",
      "     ------- -------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:26\n",
      "     ------- -------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:26\n",
      "     ------- -------------------------------- 0.4/2.5 GB 23.4 MB/s eta 0:01:26\n",
      "     ------- -------------------------------- 0.4/2.5 GB 17.7 MB/s eta 0:01:54\n",
      "     ------- -------------------------------- 0.5/2.5 GB 24.2 MB/s eta 0:01:23\n",
      "     ------- -------------------------------- 0.5/2.5 GB 24.2 MB/s eta 0:01:23\n",
      "     ------- -------------------------------- 0.5/2.5 GB 24.2 MB/s eta 0:01:23\n",
      "     ------- -------------------------------- 0.5/2.5 GB 24.2 MB/s eta 0:01:23\n",
      "     ------- -------------------------------- 0.5/2.5 GB 24.2 MB/s eta 0:01:23\n",
      "     ------- -------------------------------- 0.5/2.5 GB 22.6 MB/s eta 0:01:29\n",
      "     ------- -------------------------------- 0.5/2.5 GB 22.6 MB/s eta 0:01:29\n",
      "     ------- -------------------------------- 0.5/2.5 GB 22.6 MB/s eta 0:01:29\n",
      "     ------- -------------------------------- 0.5/2.5 GB 17.7 MB/s eta 0:01:53\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:26\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:26\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:26\n",
      "     ------- -------------------------------- 0.5/2.5 GB 17.2 MB/s eta 0:01:56\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:26\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:26\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:26\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:26\n",
      "     ------- -------------------------------- 0.5/2.5 GB 17.7 MB/s eta 0:01:53\n",
      "     ------- -------------------------------- 0.5/2.5 GB 22.5 MB/s eta 0:01:29\n",
      "     ------- -------------------------------- 0.5/2.5 GB 22.5 MB/s eta 0:01:29\n",
      "     ------- -------------------------------- 0.5/2.5 GB 22.5 MB/s eta 0:01:29\n",
      "     ------- -------------------------------- 0.5/2.5 GB 17.7 MB/s eta 0:01:53\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 17.7 MB/s eta 0:01:52\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 17.7 MB/s eta 0:01:52\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 17.2 MB/s eta 0:01:55\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:25\n",
      "     ------- -------------------------------- 0.5/2.5 GB 17.7 MB/s eta 0:01:52\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:24\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:24\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:24\n",
      "     ------- -------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:24\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:24\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:24\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:24\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:24\n",
      "     -------- ------------------------------- 0.5/2.5 GB 17.7 MB/s eta 0:01:51\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:24\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:24\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:24\n",
      "     -------- ------------------------------- 0.5/2.5 GB 17.7 MB/s eta 0:01:51\n",
      "     -------- ------------------------------- 0.5/2.5 GB 24.3 MB/s eta 0:01:21\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:24\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:24\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:24\n",
      "     -------- ------------------------------- 0.5/2.5 GB 17.7 MB/s eta 0:01:51\n",
      "     -------- ------------------------------- 0.5/2.5 GB 24.2 MB/s eta 0:01:21\n",
      "     -------- ------------------------------- 0.5/2.5 GB 24.2 MB/s eta 0:01:21\n",
      "     -------- ------------------------------- 0.5/2.5 GB 24.2 MB/s eta 0:01:21\n",
      "     -------- ------------------------------- 0.5/2.5 GB 17.7 MB/s eta 0:01:51\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:24\n",
      "     -------- ------------------------------- 0.5/2.5 GB 22.6 MB/s eta 0:01:27\n",
      "     -------- ------------------------------- 0.5/2.5 GB 22.6 MB/s eta 0:01:27\n",
      "     -------- ------------------------------- 0.5/2.5 GB 22.6 MB/s eta 0:01:27\n",
      "     -------- ------------------------------- 0.5/2.5 GB 17.2 MB/s eta 0:01:53\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:23\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:23\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:23\n",
      "     -------- ------------------------------- 0.5/2.5 GB 17.7 MB/s eta 0:01:50\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:23\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:23\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:23\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:23\n",
      "     -------- ------------------------------- 0.5/2.5 GB 17.7 MB/s eta 0:01:50\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:23\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:23\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:23\n",
      "     -------- ------------------------------- 0.5/2.5 GB 17.2 MB/s eta 0:01:53\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:23\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:23\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:23\n",
      "     -------- ------------------------------- 0.5/2.5 GB 17.2 MB/s eta 0:01:52\n",
      "     -------- ------------------------------- 0.5/2.5 GB 21.8 MB/s eta 0:01:29\n",
      "     -------- ------------------------------- 0.5/2.5 GB 22.5 MB/s eta 0:01:26\n",
      "     -------- ------------------------------- 0.5/2.5 GB 22.5 MB/s eta 0:01:26\n",
      "     -------- ------------------------------- 0.5/2.5 GB 22.5 MB/s eta 0:01:26\n",
      "     -------- ------------------------------- 0.5/2.5 GB 17.2 MB/s eta 0:01:52\n",
      "     -------- ------------------------------- 0.5/2.5 GB 22.6 MB/s eta 0:01:26\n",
      "     -------- ------------------------------- 0.5/2.5 GB 22.6 MB/s eta 0:01:26\n",
      "     -------- ------------------------------- 0.5/2.5 GB 22.6 MB/s eta 0:01:26\n",
      "     -------- ------------------------------- 0.5/2.5 GB 16.8 MB/s eta 0:01:55\n",
      "     -------- ------------------------------- 0.5/2.5 GB 16.8 MB/s eta 0:01:55\n",
      "     -------- ------------------------------- 0.5/2.5 GB 22.6 MB/s eta 0:01:25\n",
      "     -------- ------------------------------- 0.5/2.5 GB 22.6 MB/s eta 0:01:25\n",
      "     -------- ------------------------------- 0.5/2.5 GB 22.6 MB/s eta 0:01:25\n",
      "     -------- ------------------------------- 0.5/2.5 GB 17.2 MB/s eta 0:01:52\n",
      "     -------- ------------------------------- 0.5/2.5 GB 22.6 MB/s eta 0:01:25\n",
      "     -------- ------------------------------- 0.5/2.5 GB 22.6 MB/s eta 0:01:25\n",
      "     -------- ------------------------------- 0.5/2.5 GB 22.6 MB/s eta 0:01:25\n",
      "     -------- ------------------------------- 0.5/2.5 GB 17.2 MB/s eta 0:01:52\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:22\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:22\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:22\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:22\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:22\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:22\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:22\n",
      "     -------- ------------------------------- 0.5/2.5 GB 23.4 MB/s eta 0:01:22\n",
      "     -------- ------------------------------- 0.6/2.5 GB 17.7 MB/s eta 0:01:48\n",
      "     --------- ------------------------------ 0.6/2.5 GB 24.2 MB/s eta 0:01:19\n",
      "     --------- ------------------------------ 0.6/2.5 GB 24.2 MB/s eta 0:01:19\n",
      "     --------- ------------------------------ 0.6/2.5 GB 24.2 MB/s eta 0:01:19\n",
      "     --------- ------------------------------ 0.6/2.5 GB 17.7 MB/s eta 0:01:48\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:22\n",
      "     --------- ------------------------------ 0.6/2.5 GB 24.2 MB/s eta 0:01:19\n",
      "     --------- ------------------------------ 0.6/2.5 GB 24.2 MB/s eta 0:01:19\n",
      "     --------- ------------------------------ 0.6/2.5 GB 24.2 MB/s eta 0:01:19\n",
      "     --------- ------------------------------ 0.6/2.5 GB 17.7 MB/s eta 0:01:48\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:22\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:22\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:22\n",
      "     --------- ------------------------------ 0.6/2.5 GB 17.7 MB/s eta 0:01:47\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:21\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:21\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:21\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:21\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:21\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:21\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:21\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:21\n",
      "     --------- ------------------------------ 0.6/2.5 GB 17.2 MB/s eta 0:01:50\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:21\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:21\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:21\n",
      "     --------- ------------------------------ 0.6/2.5 GB 17.2 MB/s eta 0:01:50\n",
      "     --------- ------------------------------ 0.6/2.5 GB 17.2 MB/s eta 0:01:50\n",
      "     --------- ------------------------------ 0.6/2.5 GB 22.6 MB/s eta 0:01:24\n",
      "     --------- ------------------------------ 0.6/2.5 GB 22.6 MB/s eta 0:01:24\n",
      "     --------- ------------------------------ 0.6/2.5 GB 22.6 MB/s eta 0:01:24\n",
      "     --------- ------------------------------ 0.6/2.5 GB 17.3 MB/s eta 0:01:49\n",
      "     --------- ------------------------------ 0.6/2.5 GB 22.5 MB/s eta 0:01:24\n",
      "     --------- ------------------------------ 0.6/2.5 GB 22.5 MB/s eta 0:01:24\n",
      "     --------- ------------------------------ 0.6/2.5 GB 22.5 MB/s eta 0:01:24\n",
      "     --------- ------------------------------ 0.6/2.5 GB 17.2 MB/s eta 0:01:49\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:21\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:21\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:21\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:21\n",
      "     --------- ------------------------------ 0.6/2.5 GB 22.6 MB/s eta 0:01:23\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:20\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:20\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:20\n",
      "     --------- ------------------------------ 0.6/2.5 GB 17.7 MB/s eta 0:01:46\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:20\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:20\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:20\n",
      "     --------- ------------------------------ 0.6/2.5 GB 17.7 MB/s eta 0:01:46\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:20\n",
      "     --------- ------------------------------ 0.6/2.5 GB 24.3 MB/s eta 0:01:17\n",
      "     --------- ------------------------------ 0.6/2.5 GB 24.3 MB/s eta 0:01:17\n",
      "     --------- ------------------------------ 0.6/2.5 GB 24.3 MB/s eta 0:01:17\n",
      "     --------- ------------------------------ 0.6/2.5 GB 17.2 MB/s eta 0:01:48\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:20\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:20\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:20\n",
      "     --------- ------------------------------ 0.6/2.5 GB 17.7 MB/s eta 0:01:45\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:20\n",
      "     --------- ------------------------------ 0.6/2.5 GB 24.2 MB/s eta 0:01:17\n",
      "     --------- ------------------------------ 0.6/2.5 GB 24.2 MB/s eta 0:01:17\n",
      "     --------- ------------------------------ 0.6/2.5 GB 24.2 MB/s eta 0:01:17\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:19\n",
      "     --------- ------------------------------ 0.6/2.5 GB 24.2 MB/s eta 0:01:17\n",
      "     --------- ------------------------------ 0.6/2.5 GB 24.2 MB/s eta 0:01:17\n",
      "     --------- ------------------------------ 0.6/2.5 GB 24.2 MB/s eta 0:01:17\n",
      "     --------- ------------------------------ 0.6/2.5 GB 18.2 MB/s eta 0:01:42\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:19\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:19\n",
      "     --------- ------------------------------ 0.6/2.5 GB 23.4 MB/s eta 0:01:19\n",
      "     --------- ------------------------------ 0.6/2.5 GB 17.2 MB/s eta 0:01:48\n",
      "     --------- ------------------------------ 0.6/2.5 GB 17.3 MB/s eta 0:01:47\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 22.6 MB/s eta 0:01:22\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 22.6 MB/s eta 0:01:22\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 22.6 MB/s eta 0:01:22\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 17.2 MB/s eta 0:01:47\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 22.6 MB/s eta 0:01:22\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 22.6 MB/s eta 0:01:22\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 22.6 MB/s eta 0:01:22\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 17.3 MB/s eta 0:01:47\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 22.6 MB/s eta 0:01:22\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 23.4 MB/s eta 0:01:19\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 23.4 MB/s eta 0:01:19\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 23.4 MB/s eta 0:01:19\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 24.2 MB/s eta 0:01:16\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 24.2 MB/s eta 0:01:16\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 24.2 MB/s eta 0:01:16\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 24.2 MB/s eta 0:01:16\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 17.7 MB/s eta 0:01:44\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 23.4 MB/s eta 0:01:19\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 23.4 MB/s eta 0:01:19\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 23.4 MB/s eta 0:01:19\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 17.7 MB/s eta 0:01:44\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 23.4 MB/s eta 0:01:18\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 23.4 MB/s eta 0:01:18\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 23.4 MB/s eta 0:01:18\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 23.4 MB/s eta 0:01:18\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 17.2 MB/s eta 0:01:46\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 24.2 MB/s eta 0:01:15\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 24.2 MB/s eta 0:01:15\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 24.2 MB/s eta 0:01:15\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 17.7 MB/s eta 0:01:43\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 17.2 MB/s eta 0:01:46\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 17.2 MB/s eta 0:01:46\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 17.2 MB/s eta 0:01:46\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 17.2 MB/s eta 0:01:46\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 16.8 MB/s eta 0:01:49\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 16.8 MB/s eta 0:01:48\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 16.8 MB/s eta 0:01:48\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 16.8 MB/s eta 0:01:48\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 16.8 MB/s eta 0:01:48\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 22.5 MB/s eta 0:01:21\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 22.5 MB/s eta 0:01:21\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 22.5 MB/s eta 0:01:21\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 17.2 MB/s eta 0:01:45\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 23.4 MB/s eta 0:01:18\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 23.4 MB/s eta 0:01:18\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 23.4 MB/s eta 0:01:18\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 23.4 MB/s eta 0:01:18\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 17.2 MB/s eta 0:01:45\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 22.6 MB/s eta 0:01:20\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 22.6 MB/s eta 0:01:20\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 22.6 MB/s eta 0:01:20\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:48\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 17.3 MB/s eta 0:01:45\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 17.3 MB/s eta 0:01:45\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 17.3 MB/s eta 0:01:45\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 17.3 MB/s eta 0:01:45\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 17.3 MB/s eta 0:01:45\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 17.3 MB/s eta 0:01:45\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 17.3 MB/s eta 0:01:45\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 17.3 MB/s eta 0:01:45\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 12.6 MB/s eta 0:02:23\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 15.6 MB/s eta 0:01:55\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 14.9 MB/s eta 0:02:01\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 14.9 MB/s eta 0:02:01\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 13.1 MB/s eta 0:02:17\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 16.4 MB/s eta 0:01:50\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 14.9 MB/s eta 0:02:01\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 14.9 MB/s eta 0:02:01\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 14.9 MB/s eta 0:02:01\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 12.8 MB/s eta 0:02:20\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 23.4 MB/s eta 0:01:17\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 20.5 MB/s eta 0:01:28\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 20.5 MB/s eta 0:01:28\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 17.2 MB/s eta 0:01:44\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 17.3 MB/s eta 0:01:44\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 16.4 MB/s eta 0:01:49\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 16.4 MB/s eta 0:01:49\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 17.2 MB/s eta 0:01:44\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:46\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 15.6 MB/s eta 0:01:55\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:46\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:46\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:46\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:46\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 15.6 MB/s eta 0:01:54\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 15.6 MB/s eta 0:01:54\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:46\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:46\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 15.6 MB/s eta 0:01:54\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.4 MB/s eta 0:01:49\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.4 MB/s eta 0:01:49\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.2 MB/s eta 0:01:43\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.7 MB/s eta 0:01:40\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.2 MB/s eta 0:01:43\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.2 MB/s eta 0:01:43\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:46\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 23.4 MB/s eta 0:01:16\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 21.1 MB/s eta 0:01:24\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 21.1 MB/s eta 0:01:24\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.2 MB/s eta 0:01:43\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:45\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 21.1 MB/s eta 0:01:24\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 21.1 MB/s eta 0:01:24\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 21.1 MB/s eta 0:01:24\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.2 MB/s eta 0:01:43\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:45\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.3 MB/s eta 0:01:42\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.3 MB/s eta 0:01:42\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.3 MB/s eta 0:01:42\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.7 MB/s eta 0:01:40\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 21.1 MB/s eta 0:01:24\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 21.1 MB/s eta 0:01:24\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 21.1 MB/s eta 0:01:24\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 18.2 MB/s eta 0:01:37\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 21.8 MB/s eta 0:01:21\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 21.1 MB/s eta 0:01:24\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 21.1 MB/s eta 0:01:24\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.3 MB/s eta 0:01:42\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 23.4 MB/s eta 0:01:15\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 22.6 MB/s eta 0:01:18\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 22.6 MB/s eta 0:01:18\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.2 MB/s eta 0:01:42\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 23.4 MB/s eta 0:01:15\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 21.9 MB/s eta 0:01:20\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 21.9 MB/s eta 0:01:20\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 21.9 MB/s eta 0:01:20\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.7 MB/s eta 0:01:39\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 23.4 MB/s eta 0:01:15\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 23.4 MB/s eta 0:01:15\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 23.4 MB/s eta 0:01:15\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.7 MB/s eta 0:01:39\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.7 MB/s eta 0:01:39\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.7 MB/s eta 0:01:39\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.7 MB/s eta 0:01:39\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 17.7 MB/s eta 0:01:39\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:44\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.4 MB/s eta 0:01:46\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.4 MB/s eta 0:01:46\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.4 MB/s eta 0:01:46\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:44\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.4 MB/s eta 0:01:46\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.4 MB/s eta 0:01:46\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.4 MB/s eta 0:01:46\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 13.1 MB/s eta 0:02:13\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.0 MB/s eta 0:01:49\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.0 MB/s eta 0:01:49\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.0 MB/s eta 0:01:49\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.0 MB/s eta 0:01:49\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:43\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:43\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:43\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:43\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 13.4 MB/s eta 0:02:10\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:43\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:43\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:43\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:43\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.4 MB/s eta 0:01:46\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.4 MB/s eta 0:01:46\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.4 MB/s eta 0:01:46\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 16.4 MB/s eta 0:01:46\n",
      "     ------------ --------------------------- 0.7/2.5 GB 16.4 MB/s eta 0:01:45\n",
      "     ------------ --------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:43\n",
      "     ------------ --------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:43\n",
      "     ------------ --------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:43\n",
      "     ------------ --------------------------- 0.7/2.5 GB 13.4 MB/s eta 0:02:09\n",
      "     ------------ --------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.7/2.5 GB 16.4 MB/s eta 0:01:45\n",
      "     ------------ --------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.7/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.7/2.5 GB 13.4 MB/s eta 0:02:08\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:42\n",
      "     ------------ --------------------------- 0.8/2.5 GB 13.6 MB/s eta 0:02:05\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.4 MB/s eta 0:01:44\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.4 MB/s eta 0:01:44\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.4 MB/s eta 0:01:44\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 13.4 MB/s eta 0:02:07\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:41\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:40\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.4 MB/s eta 0:01:43\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.4 MB/s eta 0:01:43\n",
      "     ------------ --------------------------- 0.8/2.5 GB 13.6 MB/s eta 0:02:04\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:40\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.4 MB/s eta 0:01:43\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.4 MB/s eta 0:01:43\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.4 MB/s eta 0:01:43\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.4 MB/s eta 0:01:43\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.4 MB/s eta 0:01:43\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.4 MB/s eta 0:01:43\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.4 MB/s eta 0:01:43\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:40\n",
      "     ------------ --------------------------- 0.8/2.5 GB 17.2 MB/s eta 0:01:37\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.0 MB/s eta 0:01:45\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.0 MB/s eta 0:01:45\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.0 MB/s eta 0:01:45\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:40\n",
      "     ------------ --------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:37\n",
      "     ------------ --------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:37\n",
      "     ------------ --------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:37\n",
      "     ------------ --------------------------- 0.8/2.5 GB 17.2 MB/s eta 0:01:37\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.0 MB/s eta 0:01:45\n",
      "     ------------ --------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:37\n",
      "     ------------ --------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:37\n",
      "     ------------ --------------------------- 0.8/2.5 GB 14.2 MB/s eta 0:01:57\n",
      "     ------------ --------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:39\n",
      "     ------------ --------------------------- 0.8/2.5 GB 17.2 MB/s eta 0:01:37\n",
      "     ------------ --------------------------- 0.8/2.5 GB 17.2 MB/s eta 0:01:37\n",
      "     ------------ --------------------------- 0.8/2.5 GB 17.2 MB/s eta 0:01:37\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:39\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:39\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:39\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:39\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:39\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:39\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.4 MB/s eta 0:01:41\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.4 MB/s eta 0:01:41\n",
      "     ------------- -------------------------- 0.8/2.5 GB 14.2 MB/s eta 0:01:57\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.2 MB/s eta 0:01:36\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:36\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:36\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:36\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.2 MB/s eta 0:01:36\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:36\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:36\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:36\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:38\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:36\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:38\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:38\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:38\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:38\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:38\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:38\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:38\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:35\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.2 MB/s eta 0:01:35\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.2 MB/s eta 0:01:35\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.2 MB/s eta 0:01:35\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:38\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.2 MB/s eta 0:01:35\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:35\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:35\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:35\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:38\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:35\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:35\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.3 MB/s eta 0:01:35\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.2 MB/s eta 0:01:35\n",
      "     ------------- -------------------------- 0.8/2.5 GB 22.6 MB/s eta 0:01:12\n",
      "     ------------- -------------------------- 0.8/2.5 GB 22.6 MB/s eta 0:01:12\n",
      "     ------------- -------------------------- 0.8/2.5 GB 22.6 MB/s eta 0:01:12\n",
      "     ------------- -------------------------- 0.8/2.5 GB 22.6 MB/s eta 0:01:12\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:37\n",
      "     ------------- -------------------------- 0.8/2.5 GB 22.6 MB/s eta 0:01:12\n",
      "     ------------- -------------------------- 0.8/2.5 GB 22.6 MB/s eta 0:01:12\n",
      "     ------------- -------------------------- 0.8/2.5 GB 22.6 MB/s eta 0:01:12\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.2 MB/s eta 0:01:35\n",
      "     ------------- -------------------------- 0.8/2.5 GB 22.6 MB/s eta 0:01:12\n",
      "     ------------- -------------------------- 0.8/2.5 GB 22.6 MB/s eta 0:01:12\n",
      "     ------------- -------------------------- 0.8/2.5 GB 22.6 MB/s eta 0:01:12\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:37\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:37\n",
      "     ------------- -------------------------- 0.8/2.5 GB 22.6 MB/s eta 0:01:12\n",
      "     ------------- -------------------------- 0.8/2.5 GB 22.6 MB/s eta 0:01:12\n",
      "     ------------- -------------------------- 0.8/2.5 GB 22.6 MB/s eta 0:01:12\n",
      "     ------------- -------------------------- 0.8/2.5 GB 17.2 MB/s eta 0:01:34\n",
      "     ------------- -------------------------- 0.8/2.5 GB 22.6 MB/s eta 0:01:12\n",
      "     ------------- -------------------------- 0.8/2.5 GB 22.6 MB/s eta 0:01:12\n",
      "     ------------- -------------------------- 0.8/2.5 GB 22.6 MB/s eta 0:01:12\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:37\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:36\n",
      "     ------------- -------------------------- 0.8/2.5 GB 21.8 MB/s eta 0:01:14\n",
      "     ------------- -------------------------- 0.8/2.5 GB 21.8 MB/s eta 0:01:14\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:36\n",
      "     ------------- -------------------------- 0.8/2.5 GB 16.8 MB/s eta 0:01:36\n",
      "     ------------- -------------------------- 0.9/2.5 GB 21.8 MB/s eta 0:01:14\n",
      "     ------------- -------------------------- 0.9/2.5 GB 21.8 MB/s eta 0:01:14\n",
      "     ------------- -------------------------- 0.9/2.5 GB 21.8 MB/s eta 0:01:14\n",
      "     ------------- -------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:34\n",
      "     ------------- -------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:33\n",
      "     ------------- -------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:33\n",
      "     ------------- -------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:33\n",
      "     ------------- -------------------------- 0.9/2.5 GB 17.3 MB/s eta 0:01:33\n",
      "     ------------- -------------------------- 0.9/2.5 GB 17.3 MB/s eta 0:01:33\n",
      "     ------------- -------------------------- 0.9/2.5 GB 22.6 MB/s eta 0:01:11\n",
      "     ------------- -------------------------- 0.9/2.5 GB 22.6 MB/s eta 0:01:11\n",
      "     ------------- -------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:33\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:33\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.9 MB/s eta 0:01:13\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.9 MB/s eta 0:01:13\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.9 MB/s eta 0:01:13\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:33\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.8 MB/s eta 0:01:13\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.8 MB/s eta 0:01:13\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.8 MB/s eta 0:01:13\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:33\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:33\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.8 MB/s eta 0:01:13\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.8 MB/s eta 0:01:13\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:33\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:33\n",
      "     -------------- ------------------------- 0.9/2.5 GB 16.8 MB/s eta 0:01:35\n",
      "     -------------- ------------------------- 0.9/2.5 GB 19.8 MB/s eta 0:01:20\n",
      "     -------------- ------------------------- 0.9/2.5 GB 19.8 MB/s eta 0:01:20\n",
      "     -------------- ------------------------- 0.9/2.5 GB 16.8 MB/s eta 0:01:35\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.3 MB/s eta 0:01:32\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.1 MB/s eta 0:01:15\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.1 MB/s eta 0:01:15\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:32\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:32\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.1 MB/s eta 0:01:15\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.1 MB/s eta 0:01:15\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.1 MB/s eta 0:01:15\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.3 MB/s eta 0:01:32\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.8 MB/s eta 0:01:13\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.8 MB/s eta 0:01:13\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.8 MB/s eta 0:01:13\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:32\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:32\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.8 MB/s eta 0:01:12\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.8 MB/s eta 0:01:12\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.7 MB/s eta 0:01:29\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:31\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.1 MB/s eta 0:01:15\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.1 MB/s eta 0:01:15\n",
      "     -------------- ------------------------- 0.9/2.5 GB 16.8 MB/s eta 0:01:34\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:31\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.1 MB/s eta 0:01:14\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.1 MB/s eta 0:01:14\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.1 MB/s eta 0:01:14\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:31\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.3 MB/s eta 0:01:31\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.3 MB/s eta 0:01:31\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.3 MB/s eta 0:01:31\n",
      "     -------------- ------------------------- 0.9/2.5 GB 16.8 MB/s eta 0:01:33\n",
      "     -------------- ------------------------- 0.9/2.5 GB 16.8 MB/s eta 0:01:33\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:31\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:31\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:31\n",
      "     -------------- ------------------------- 0.9/2.5 GB 16.8 MB/s eta 0:01:33\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:31\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:31\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:31\n",
      "     -------------- ------------------------- 0.9/2.5 GB 16.8 MB/s eta 0:01:33\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:30\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:30\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:30\n",
      "     -------------- ------------------------- 0.9/2.5 GB 13.9 MB/s eta 0:01:52\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:30\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.9 MB/s eta 0:01:11\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.9 MB/s eta 0:01:11\n",
      "     -------------- ------------------------- 0.9/2.5 GB 21.9 MB/s eta 0:01:11\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.2 MB/s eta 0:01:30\n",
      "     -------------- ------------------------- 0.9/2.5 GB 22.6 MB/s eta 0:01:09\n",
      "     -------------- ------------------------- 0.9/2.5 GB 22.6 MB/s eta 0:01:09\n",
      "     -------------- ------------------------- 0.9/2.5 GB 22.6 MB/s eta 0:01:09\n",
      "     -------------- ------------------------- 0.9/2.5 GB 16.8 MB/s eta 0:01:32\n",
      "     -------------- ------------------------- 0.9/2.5 GB 22.6 MB/s eta 0:01:09\n",
      "     -------------- ------------------------- 0.9/2.5 GB 22.6 MB/s eta 0:01:09\n",
      "     -------------- ------------------------- 0.9/2.5 GB 22.6 MB/s eta 0:01:09\n",
      "     -------------- ------------------------- 0.9/2.5 GB 22.6 MB/s eta 0:01:09\n",
      "     -------------- ------------------------- 0.9/2.5 GB 17.3 MB/s eta 0:01:30\n",
      "     -------------- ------------------------- 0.9/2.5 GB 22.6 MB/s eta 0:01:09\n",
      "     -------------- ------------------------- 0.9/2.5 GB 22.6 MB/s eta 0:01:09\n",
      "     -------------- ------------------------- 0.9/2.5 GB 22.6 MB/s eta 0:01:09\n",
      "     -------------- ------------------------- 0.9/2.5 GB 16.8 MB/s eta 0:01:32\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:08\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:08\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:08\n",
      "     --------------- ------------------------ 0.9/2.5 GB 16.8 MB/s eta 0:01:32\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:08\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:08\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:08\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:08\n",
      "     --------------- ------------------------ 0.9/2.5 GB 16.8 MB/s eta 0:01:32\n",
      "     --------------- ------------------------ 0.9/2.5 GB 21.9 MB/s eta 0:01:10\n",
      "     --------------- ------------------------ 0.9/2.5 GB 21.9 MB/s eta 0:01:10\n",
      "     --------------- ------------------------ 0.9/2.5 GB 21.9 MB/s eta 0:01:10\n",
      "     --------------- ------------------------ 0.9/2.5 GB 17.2 MB/s eta 0:01:29\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:08\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:08\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:08\n",
      "     --------------- ------------------------ 0.9/2.5 GB 17.2 MB/s eta 0:01:29\n",
      "     --------------- ------------------------ 0.9/2.5 GB 16.8 MB/s eta 0:01:31\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:08\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:08\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:08\n",
      "     --------------- ------------------------ 0.9/2.5 GB 16.8 MB/s eta 0:01:31\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:08\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:08\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:08\n",
      "     --------------- ------------------------ 0.9/2.5 GB 17.3 MB/s eta 0:01:28\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:07\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:07\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:07\n",
      "     --------------- ------------------------ 0.9/2.5 GB 17.2 MB/s eta 0:01:28\n",
      "     --------------- ------------------------ 0.9/2.5 GB 16.8 MB/s eta 0:01:31\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:07\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:07\n",
      "     --------------- ------------------------ 0.9/2.5 GB 22.6 MB/s eta 0:01:07\n",
      "     --------------- ------------------------ 0.9/2.5 GB 16.8 MB/s eta 0:01:30\n",
      "     --------------- ------------------------ 0.9/2.5 GB 21.8 MB/s eta 0:01:10\n",
      "     --------------- ------------------------ 0.9/2.5 GB 21.8 MB/s eta 0:01:10\n",
      "     --------------- ------------------------ 0.9/2.5 GB 21.8 MB/s eta 0:01:10\n",
      "     --------------- ------------------------ 1.0/2.5 GB 16.8 MB/s eta 0:01:30\n",
      "     --------------- ------------------------ 1.0/2.5 GB 22.6 MB/s eta 0:01:07\n",
      "     --------------- ------------------------ 1.0/2.5 GB 22.6 MB/s eta 0:01:07\n",
      "     --------------- ------------------------ 1.0/2.5 GB 22.6 MB/s eta 0:01:07\n",
      "     --------------- ------------------------ 1.0/2.5 GB 17.2 MB/s eta 0:01:28\n",
      "     --------------- ------------------------ 1.0/2.5 GB 17.2 MB/s eta 0:01:28\n",
      "     --------------- ------------------------ 1.0/2.5 GB 22.6 MB/s eta 0:01:07\n",
      "     --------------- ------------------------ 1.0/2.5 GB 22.6 MB/s eta 0:01:07\n",
      "     --------------- ------------------------ 1.0/2.5 GB 22.6 MB/s eta 0:01:07\n",
      "     --------------- ------------------------ 1.0/2.5 GB 16.8 MB/s eta 0:01:30\n",
      "     --------------- ------------------------ 1.0/2.5 GB 22.6 MB/s eta 0:01:07\n",
      "     --------------- ------------------------ 1.0/2.5 GB 22.6 MB/s eta 0:01:07\n",
      "     --------------- ------------------------ 1.0/2.5 GB 22.6 MB/s eta 0:01:07\n",
      "     --------------- ------------------------ 1.0/2.5 GB 17.2 MB/s eta 0:01:27\n",
      "     --------------- ------------------------ 1.0/2.5 GB 17.2 MB/s eta 0:01:27\n",
      "     --------------- ------------------------ 1.0/2.5 GB 22.6 MB/s eta 0:01:07\n",
      "     --------------- ------------------------ 1.0/2.5 GB 22.6 MB/s eta 0:01:07\n",
      "     --------------- ------------------------ 1.0/2.5 GB 17.7 MB/s eta 0:01:25\n",
      "     --------------- ------------------------ 1.0/2.5 GB 17.3 MB/s eta 0:01:27\n",
      "     --------------- ------------------------ 1.0/2.5 GB 17.2 MB/s eta 0:01:27\n",
      "     --------------- ------------------------ 1.0/2.5 GB 17.2 MB/s eta 0:01:27\n",
      "     --------------- ------------------------ 1.0/2.5 GB 17.2 MB/s eta 0:01:27\n",
      "     --------------- ------------------------ 1.0/2.5 GB 17.2 MB/s eta 0:01:27\n",
      "     --------------- ------------------------ 1.0/2.5 GB 17.2 MB/s eta 0:01:27\n",
      "     --------------- ------------------------ 1.0/2.5 GB 17.2 MB/s eta 0:01:27\n",
      "     --------------- ------------------------ 1.0/2.5 GB 17.2 MB/s eta 0:01:27\n",
      "     --------------- ------------------------ 1.0/2.5 GB 16.8 MB/s eta 0:01:29\n",
      "     --------------- ------------------------ 1.0/2.5 GB 16.8 MB/s eta 0:01:29\n",
      "     --------------- ------------------------ 1.0/2.5 GB 16.8 MB/s eta 0:01:29\n",
      "     --------------- ------------------------ 1.0/2.5 GB 16.8 MB/s eta 0:01:29\n",
      "     --------------- ------------------------ 1.0/2.5 GB 16.8 MB/s eta 0:01:29\n",
      "     --------------- ------------------------ 1.0/2.5 GB 17.2 MB/s eta 0:01:26\n",
      "     --------------- ------------------------ 1.0/2.5 GB 23.4 MB/s eta 0:01:04\n",
      "     --------------- ------------------------ 1.0/2.5 GB 23.4 MB/s eta 0:01:04\n",
      "     --------------- ------------------------ 1.0/2.5 GB 23.4 MB/s eta 0:01:04\n",
      "     --------------- ------------------------ 1.0/2.5 GB 17.7 MB/s eta 0:01:24\n",
      "     --------------- ------------------------ 1.0/2.5 GB 22.6 MB/s eta 0:01:06\n",
      "     --------------- ------------------------ 1.0/2.5 GB 22.6 MB/s eta 0:01:06\n",
      "     --------------- ------------------------ 1.0/2.5 GB 22.6 MB/s eta 0:01:06\n",
      "     --------------- ------------------------ 1.0/2.5 GB 17.2 MB/s eta 0:01:26\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 22.6 MB/s eta 0:01:06\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 22.6 MB/s eta 0:01:06\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 22.6 MB/s eta 0:01:06\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 22.6 MB/s eta 0:01:06\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 16.8 MB/s eta 0:01:28\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 21.8 MB/s eta 0:01:08\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 21.8 MB/s eta 0:01:08\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 21.8 MB/s eta 0:01:08\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 16.8 MB/s eta 0:01:28\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 22.6 MB/s eta 0:01:05\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 21.8 MB/s eta 0:01:08\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 21.8 MB/s eta 0:01:08\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 17.2 MB/s eta 0:01:25\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 16.8 MB/s eta 0:01:28\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 15.6 MB/s eta 0:01:34\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 14.6 MB/s eta 0:01:41\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 13.9 MB/s eta 0:01:45\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 13.1 MB/s eta 0:01:52\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 16.0 MB/s eta 0:01:32\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 20.5 MB/s eta 0:01:12\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 20.5 MB/s eta 0:01:12\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 16.8 MB/s eta 0:01:27\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 16.8 MB/s eta 0:01:27\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 22.5 MB/s eta 0:01:05\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 22.5 MB/s eta 0:01:05\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 17.2 MB/s eta 0:01:25\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 24.2 MB/s eta 0:01:00\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 25.1 MB/s eta 0:00:58\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 25.1 MB/s eta 0:00:58\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 25.1 MB/s eta 0:00:58\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 18.7 MB/s eta 0:01:18\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 18.2 MB/s eta 0:01:20\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 21.9 MB/s eta 0:01:07\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 21.9 MB/s eta 0:01:07\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 18.2 MB/s eta 0:01:20\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 16.8 MB/s eta 0:01:26\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:02\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:02\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 18.2 MB/s eta 0:01:20\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 16.8 MB/s eta 0:01:26\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 21.1 MB/s eta 0:01:09\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 21.1 MB/s eta 0:01:09\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 21.1 MB/s eta 0:01:09\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 21.1 MB/s eta 0:01:09\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 16.8 MB/s eta 0:01:26\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 19.9 MB/s eta 0:01:13\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 19.9 MB/s eta 0:01:13\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 19.9 MB/s eta 0:01:13\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 17.2 MB/s eta 0:01:24\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 21.1 MB/s eta 0:01:08\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 21.1 MB/s eta 0:01:08\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 21.1 MB/s eta 0:01:08\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 17.2 MB/s eta 0:01:24\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:02\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:02\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:02\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:02\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 17.2 MB/s eta 0:01:23\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:01\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:01\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:01\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 17.7 MB/s eta 0:01:21\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 22.6 MB/s eta 0:01:03\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 22.6 MB/s eta 0:01:03\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 22.6 MB/s eta 0:01:03\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 17.7 MB/s eta 0:01:21\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 22.6 MB/s eta 0:01:03\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:01\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:01\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:01\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 17.2 MB/s eta 0:01:23\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:01\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:01\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:01\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 17.2 MB/s eta 0:01:23\n",
      "     ----------------- ---------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:01\n",
      "     ----------------- ---------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:01\n",
      "     ----------------- ---------------------- 1.0/2.5 GB 23.4 MB/s eta 0:01:01\n",
      "     ----------------- ---------------------- 1.0/2.5 GB 18.2 MB/s eta 0:01:18\n",
      "     ----------------- ---------------------- 1.0/2.5 GB 17.7 MB/s eta 0:01:20\n",
      "     ----------------- ---------------------- 1.0/2.5 GB 21.8 MB/s eta 0:01:05\n",
      "     ----------------- ---------------------- 1.0/2.5 GB 21.8 MB/s eta 0:01:05\n",
      "     ----------------- ---------------------- 1.0/2.5 GB 21.8 MB/s eta 0:01:05\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 17.3 MB/s eta 0:01:22\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 22.6 MB/s eta 0:01:03\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 22.6 MB/s eta 0:01:03\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 22.6 MB/s eta 0:01:03\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 18.2 MB/s eta 0:01:18\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:01:00\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 21.8 MB/s eta 0:01:05\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 21.8 MB/s eta 0:01:05\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 18.7 MB/s eta 0:01:15\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 17.7 MB/s eta 0:01:19\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:01:00\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:01:00\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:01:00\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 17.2 MB/s eta 0:01:21\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 22.6 MB/s eta 0:01:02\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 22.6 MB/s eta 0:01:02\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 22.6 MB/s eta 0:01:02\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 22.6 MB/s eta 0:01:02\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 16.8 MB/s eta 0:01:23\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:01:00\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:01:00\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:01:00\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 16.8 MB/s eta 0:01:23\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:01:00\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:01:00\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:01:00\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:01:00\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:01:00\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 22.6 MB/s eta 0:01:02\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 22.6 MB/s eta 0:01:02\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 22.6 MB/s eta 0:01:02\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 17.7 MB/s eta 0:01:18\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 17.7 MB/s eta 0:01:18\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 17.7 MB/s eta 0:01:18\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 17.7 MB/s eta 0:01:18\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:59\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 17.7 MB/s eta 0:01:17\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ------------------ --------------------- 1.1/2.5 GB 17.2 MB/s eta 0:01:19\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ------------------ --------------------- 1.1/2.5 GB 17.2 MB/s eta 0:01:19\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ------------------ --------------------- 1.1/2.5 GB 17.7 MB/s eta 0:01:16\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ------------------ --------------------- 1.1/2.5 GB 17.2 MB/s eta 0:01:18\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:58\n",
      "     ------------------ --------------------- 1.1/2.5 GB 17.2 MB/s eta 0:01:18\n",
      "     ------------------ --------------------- 1.1/2.5 GB 22.6 MB/s eta 0:01:00\n",
      "     ------------------ --------------------- 1.1/2.5 GB 22.6 MB/s eta 0:01:00\n",
      "     ------------------ --------------------- 1.1/2.5 GB 22.6 MB/s eta 0:01:00\n",
      "     ------------------ --------------------- 1.1/2.5 GB 17.2 MB/s eta 0:01:18\n",
      "     ------------------ --------------------- 1.1/2.5 GB 22.6 MB/s eta 0:00:59\n",
      "     ------------------ --------------------- 1.1/2.5 GB 22.6 MB/s eta 0:00:59\n",
      "     ------------------ --------------------- 1.1/2.5 GB 22.6 MB/s eta 0:00:59\n",
      "     ------------------ --------------------- 1.1/2.5 GB 22.6 MB/s eta 0:00:59\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:57\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:57\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:57\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:57\n",
      "     ------------------ --------------------- 1.1/2.5 GB 17.2 MB/s eta 0:01:17\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:57\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:57\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:57\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:57\n",
      "     ------------------ --------------------- 1.1/2.5 GB 17.7 MB/s eta 0:01:15\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:57\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:57\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:57\n",
      "     ------------------ --------------------- 1.1/2.5 GB 17.3 MB/s eta 0:01:17\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:57\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:57\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:57\n",
      "     ------------------ --------------------- 1.1/2.5 GB 17.2 MB/s eta 0:01:17\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.1/2.5 GB 24.2 MB/s eta 0:00:54\n",
      "     ------------------ --------------------- 1.1/2.5 GB 24.2 MB/s eta 0:00:54\n",
      "     ------------------ --------------------- 1.1/2.5 GB 24.2 MB/s eta 0:00:54\n",
      "     ------------------ --------------------- 1.1/2.5 GB 17.2 MB/s eta 0:01:16\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.1/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.2/2.5 GB 17.2 MB/s eta 0:01:16\n",
      "     ------------------ --------------------- 1.2/2.5 GB 22.6 MB/s eta 0:00:58\n",
      "     ------------------ --------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.2/2.5 GB 17.7 MB/s eta 0:01:14\n",
      "     ------------------ --------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.2/2.5 GB 17.3 MB/s eta 0:01:16\n",
      "     ------------------ --------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:56\n",
      "     ------------------ --------------------- 1.2/2.5 GB 16.8 MB/s eta 0:01:17\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:55\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:55\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:55\n",
      "     ------------------- -------------------- 1.2/2.5 GB 17.7 MB/s eta 0:01:13\n",
      "     ------------------- -------------------- 1.2/2.5 GB 22.6 MB/s eta 0:00:57\n",
      "     ------------------- -------------------- 1.2/2.5 GB 22.5 MB/s eta 0:00:57\n",
      "     ------------------- -------------------- 1.2/2.5 GB 22.5 MB/s eta 0:00:57\n",
      "     ------------------- -------------------- 1.2/2.5 GB 22.5 MB/s eta 0:00:57\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:55\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:55\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:55\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:55\n",
      "     ------------------- -------------------- 1.2/2.5 GB 17.7 MB/s eta 0:01:13\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:55\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:55\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:55\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:55\n",
      "     ------------------- -------------------- 1.2/2.5 GB 17.7 MB/s eta 0:01:12\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:55\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:55\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:55\n",
      "     ------------------- -------------------- 1.2/2.5 GB 17.7 MB/s eta 0:01:12\n",
      "     ------------------- -------------------- 1.2/2.5 GB 24.3 MB/s eta 0:00:53\n",
      "     ------------------- -------------------- 1.2/2.5 GB 24.3 MB/s eta 0:00:53\n",
      "     ------------------- -------------------- 1.2/2.5 GB 24.3 MB/s eta 0:00:53\n",
      "     ------------------- -------------------- 1.2/2.5 GB 17.7 MB/s eta 0:01:12\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:55\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 17.7 MB/s eta 0:01:12\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 18.2 MB/s eta 0:01:10\n",
      "     ------------------- -------------------- 1.2/2.5 GB 24.2 MB/s eta 0:00:52\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 17.7 MB/s eta 0:01:11\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 24.2 MB/s eta 0:00:52\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:54\n",
      "     ------------------- -------------------- 1.2/2.5 GB 17.7 MB/s eta 0:01:11\n",
      "     ------------------- -------------------- 1.2/2.5 GB 24.2 MB/s eta 0:00:52\n",
      "     ------------------- -------------------- 1.2/2.5 GB 24.2 MB/s eta 0:00:52\n",
      "     ------------------- -------------------- 1.2/2.5 GB 24.2 MB/s eta 0:00:52\n",
      "     ------------------- -------------------- 1.2/2.5 GB 24.2 MB/s eta 0:00:52\n",
      "     ------------------- -------------------- 1.2/2.5 GB 22.6 MB/s eta 0:00:55\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:53\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:53\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:53\n",
      "     ------------------- -------------------- 1.2/2.5 GB 17.7 MB/s eta 0:01:10\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:53\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:53\n",
      "     ------------------- -------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:53\n",
      "     ------------------- -------------------- 1.2/2.5 GB 17.7 MB/s eta 0:01:10\n",
      "     -------------------- ------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:53\n",
      "     -------------------- ------------------- 1.2/2.5 GB 24.2 MB/s eta 0:00:51\n",
      "     -------------------- ------------------- 1.2/2.5 GB 24.2 MB/s eta 0:00:51\n",
      "     -------------------- ------------------- 1.2/2.5 GB 24.2 MB/s eta 0:00:51\n",
      "     -------------------- ------------------- 1.2/2.5 GB 17.2 MB/s eta 0:01:12\n",
      "     -------------------- ------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:53\n",
      "     -------------------- ------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:53\n",
      "     -------------------- ------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:53\n",
      "     -------------------- ------------------- 1.2/2.5 GB 16.8 MB/s eta 0:01:13\n",
      "     -------------------- ------------------- 1.2/2.5 GB 22.6 MB/s eta 0:00:54\n",
      "     -------------------- ------------------- 1.2/2.5 GB 22.5 MB/s eta 0:00:55\n",
      "     -------------------- ------------------- 1.2/2.5 GB 22.5 MB/s eta 0:00:55\n",
      "     -------------------- ------------------- 1.2/2.5 GB 22.5 MB/s eta 0:00:55\n",
      "     -------------------- ------------------- 1.2/2.5 GB 17.3 MB/s eta 0:01:11\n",
      "     -------------------- ------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.2/2.5 GB 17.2 MB/s eta 0:01:11\n",
      "     -------------------- ------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.2/2.5 GB 24.2 MB/s eta 0:00:50\n",
      "     -------------------- ------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.2/2.5 GB 23.4 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.2/2.5 GB 17.7 MB/s eta 0:01:09\n",
      "     -------------------- ------------------- 1.3/2.5 GB 23.4 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.3/2.5 GB 23.4 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.3/2.5 GB 23.4 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.3/2.5 GB 17.7 MB/s eta 0:01:08\n",
      "     -------------------- ------------------- 1.3/2.5 GB 23.4 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.3/2.5 GB 24.2 MB/s eta 0:00:50\n",
      "     -------------------- ------------------- 1.3/2.5 GB 24.2 MB/s eta 0:00:50\n",
      "     -------------------- ------------------- 1.3/2.5 GB 24.2 MB/s eta 0:00:50\n",
      "     -------------------- ------------------- 1.3/2.5 GB 17.7 MB/s eta 0:01:08\n",
      "     -------------------- ------------------- 1.3/2.5 GB 24.2 MB/s eta 0:00:50\n",
      "     -------------------- ------------------- 1.3/2.5 GB 24.2 MB/s eta 0:00:50\n",
      "     -------------------- ------------------- 1.3/2.5 GB 24.2 MB/s eta 0:00:50\n",
      "     -------------------- ------------------- 1.3/2.5 GB 17.7 MB/s eta 0:01:08\n",
      "     -------------------- ------------------- 1.3/2.5 GB 23.4 MB/s eta 0:00:51\n",
      "     -------------------- ------------------- 1.3/2.5 GB 23.4 MB/s eta 0:00:51\n",
      "     -------------------- ------------------- 1.3/2.5 GB 23.4 MB/s eta 0:00:51\n",
      "     -------------------- ------------------- 1.3/2.5 GB 23.4 MB/s eta 0:00:51\n",
      "     -------------------- ------------------- 1.3/2.5 GB 23.4 MB/s eta 0:00:51\n",
      "     -------------------- ------------------- 1.3/2.5 GB 24.2 MB/s eta 0:00:49\n",
      "     -------------------- ------------------- 1.3/2.5 GB 24.2 MB/s eta 0:00:49\n",
      "     -------------------- ------------------- 1.3/2.5 GB 24.2 MB/s eta 0:00:49\n",
      "     -------------------- ------------------- 1.3/2.5 GB 17.2 MB/s eta 0:01:09\n",
      "     -------------------- ------------------- 1.3/2.5 GB 23.4 MB/s eta 0:00:51\n",
      "     -------------------- ------------------- 1.3/2.5 GB 22.6 MB/s eta 0:00:53\n",
      "     -------------------- ------------------- 1.3/2.5 GB 22.6 MB/s eta 0:00:53\n",
      "     -------------------- ------------------- 1.3/2.5 GB 22.6 MB/s eta 0:00:53\n",
      "     -------------------- ------------------- 1.3/2.5 GB 23.4 MB/s eta 0:00:51\n",
      "     -------------------- ------------------- 1.3/2.5 GB 24.2 MB/s eta 0:00:49\n",
      "     -------------------- ------------------- 1.3/2.5 GB 24.2 MB/s eta 0:00:49\n",
      "     -------------------- ------------------- 1.3/2.5 GB 24.2 MB/s eta 0:00:49\n",
      "     -------------------- ------------------- 1.3/2.5 GB 18.2 MB/s eta 0:01:05\n",
      "     -------------------- ------------------- 1.3/2.5 GB 23.4 MB/s eta 0:00:51\n",
      "     -------------------- ------------------- 1.3/2.5 GB 23.4 MB/s eta 0:00:51\n",
      "     -------------------- ------------------- 1.3/2.5 GB 23.4 MB/s eta 0:00:51\n",
      "     -------------------- ------------------- 1.3/2.5 GB 17.7 MB/s eta 0:01:07\n",
      "     -------------------- ------------------- 1.3/2.5 GB 22.6 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.3/2.5 GB 22.6 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.3/2.5 GB 22.6 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.3/2.5 GB 22.6 MB/s eta 0:00:52\n",
      "     -------------------- ------------------- 1.3/2.5 GB 24.2 MB/s eta 0:00:49\n",
      "     -------------------- ------------------- 1.3/2.5 GB 24.2 MB/s eta 0:00:49\n",
      "     -------------------- ------------------- 1.3/2.5 GB 24.2 MB/s eta 0:00:49\n",
      "     -------------------- ------------------- 1.3/2.5 GB 24.2 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 17.7 MB/s eta 0:01:06\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:50\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:50\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:50\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:50\n",
      "     --------------------- ------------------ 1.3/2.5 GB 17.7 MB/s eta 0:01:06\n",
      "     --------------------- ------------------ 1.3/2.5 GB 22.6 MB/s eta 0:00:52\n",
      "     --------------------- ------------------ 1.3/2.5 GB 22.6 MB/s eta 0:00:52\n",
      "     --------------------- ------------------ 1.3/2.5 GB 22.6 MB/s eta 0:00:52\n",
      "     --------------------- ------------------ 1.3/2.5 GB 17.2 MB/s eta 0:01:08\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:50\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:50\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:50\n",
      "     --------------------- ------------------ 1.3/2.5 GB 17.7 MB/s eta 0:01:06\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:50\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:50\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:50\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:50\n",
      "     --------------------- ------------------ 1.3/2.5 GB 17.7 MB/s eta 0:01:05\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 17.7 MB/s eta 0:01:05\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 24.2 MB/s eta 0:00:48\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 17.7 MB/s eta 0:01:05\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 17.7 MB/s eta 0:01:05\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 22.6 MB/s eta 0:00:51\n",
      "     --------------------- ------------------ 1.3/2.5 GB 22.6 MB/s eta 0:00:51\n",
      "     --------------------- ------------------ 1.3/2.5 GB 22.6 MB/s eta 0:00:51\n",
      "     --------------------- ------------------ 1.3/2.5 GB 17.7 MB/s eta 0:01:04\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:49\n",
      "     --------------------- ------------------ 1.3/2.5 GB 18.2 MB/s eta 0:01:02\n",
      "     --------------------- ------------------ 1.3/2.5 GB 24.2 MB/s eta 0:00:47\n",
      "     --------------------- ------------------ 1.3/2.5 GB 24.2 MB/s eta 0:00:47\n",
      "     --------------------- ------------------ 1.3/2.5 GB 24.2 MB/s eta 0:00:47\n",
      "     --------------------- ------------------ 1.3/2.5 GB 24.2 MB/s eta 0:00:47\n",
      "     --------------------- ------------------ 1.3/2.5 GB 24.2 MB/s eta 0:00:47\n",
      "     --------------------- ------------------ 1.3/2.5 GB 24.3 MB/s eta 0:00:47\n",
      "     --------------------- ------------------ 1.3/2.5 GB 24.3 MB/s eta 0:00:47\n",
      "     --------------------- ------------------ 1.3/2.5 GB 24.3 MB/s eta 0:00:47\n",
      "     --------------------- ------------------ 1.3/2.5 GB 17.7 MB/s eta 0:01:04\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:48\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:48\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:48\n",
      "     --------------------- ------------------ 1.3/2.5 GB 17.2 MB/s eta 0:01:05\n",
      "     --------------------- ------------------ 1.3/2.5 GB 22.6 MB/s eta 0:00:50\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:48\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:48\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:48\n",
      "     --------------------- ------------------ 1.3/2.5 GB 17.7 MB/s eta 0:01:03\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:48\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:48\n",
      "     --------------------- ------------------ 1.3/2.5 GB 23.4 MB/s eta 0:00:48\n",
      "     --------------------- ------------------ 1.3/2.5 GB 17.7 MB/s eta 0:01:03\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:48\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:48\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:48\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 17.7 MB/s eta 0:01:03\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 17.7 MB/s eta 0:01:02\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 17.7 MB/s eta 0:01:02\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 24.2 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 24.2 MB/s eta 0:00:45\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 22.6 MB/s eta 0:00:49\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 22.6 MB/s eta 0:00:49\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 22.6 MB/s eta 0:00:49\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 17.7 MB/s eta 0:01:02\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 17.7 MB/s eta 0:01:02\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 22.6 MB/s eta 0:00:48\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 17.2 MB/s eta 0:01:03\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 17.7 MB/s eta 0:01:01\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 17.7 MB/s eta 0:01:01\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 18.2 MB/s eta 0:00:59\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 24.2 MB/s eta 0:00:44\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 24.2 MB/s eta 0:00:44\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 24.2 MB/s eta 0:00:44\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 24.2 MB/s eta 0:00:44\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 17.7 MB/s eta 0:01:00\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:46\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 17.7 MB/s eta 0:01:00\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 22.5 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 22.5 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 22.5 MB/s eta 0:00:47\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 17.7 MB/s eta 0:01:00\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 17.7 MB/s eta 0:00:59\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 17.7 MB/s eta 0:00:59\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 24.2 MB/s eta 0:00:43\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:45\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:44\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:44\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:44\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:44\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 17.7 MB/s eta 0:00:58\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:44\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:44\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:44\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:44\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 24.2 MB/s eta 0:00:43\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:44\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:44\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:44\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 17.7 MB/s eta 0:00:58\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:44\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:44\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:44\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 17.2 MB/s eta 0:00:59\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:44\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 24.2 MB/s eta 0:00:42\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 24.2 MB/s eta 0:00:42\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 24.2 MB/s eta 0:00:42\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 17.7 MB/s eta 0:00:57\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:43\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:43\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 23.4 MB/s eta 0:00:43\n",
      "     ----------------------- ---------------- 1.4/2.5 GB 17.7 MB/s eta 0:00:57\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:43\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 24.2 MB/s eta 0:00:42\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 24.2 MB/s eta 0:00:42\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 24.2 MB/s eta 0:00:42\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:43\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:43\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:43\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:43\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 18.2 MB/s eta 0:00:55\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 24.2 MB/s eta 0:00:42\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 24.2 MB/s eta 0:00:42\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 24.2 MB/s eta 0:00:42\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 18.2 MB/s eta 0:00:55\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:43\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:43\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:43\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:43\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 17.7 MB/s eta 0:00:56\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:43\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:43\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:43\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 17.7 MB/s eta 0:00:56\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:42\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:42\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:42\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:42\n",
      "     ------------------------ --------------- 1.5/2.5 GB 24.2 MB/s eta 0:00:41\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:42\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:42\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:42\n",
      "     ------------------------ --------------- 1.5/2.5 GB 17.2 MB/s eta 0:00:57\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:44\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:44\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:44\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:44\n",
      "     ------------------------ --------------- 1.5/2.5 GB 17.7 MB/s eta 0:00:56\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:44\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:44\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:44\n",
      "     ------------------------ --------------- 1.5/2.5 GB 17.2 MB/s eta 0:00:57\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:42\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:42\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:42\n",
      "     ------------------------ --------------- 1.5/2.5 GB 17.7 MB/s eta 0:00:55\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:42\n",
      "     ------------------------ --------------- 1.5/2.5 GB 24.3 MB/s eta 0:00:40\n",
      "     ------------------------ --------------- 1.5/2.5 GB 24.3 MB/s eta 0:00:40\n",
      "     ------------------------ --------------- 1.5/2.5 GB 24.3 MB/s eta 0:00:40\n",
      "     ------------------------ --------------- 1.5/2.5 GB 17.2 MB/s eta 0:00:56\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:42\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:42\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:42\n",
      "     ------------------------ --------------- 1.5/2.5 GB 17.7 MB/s eta 0:00:55\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:41\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:41\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:41\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:41\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:43\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:43\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:43\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:43\n",
      "     ------------------------ --------------- 1.5/2.5 GB 17.2 MB/s eta 0:00:56\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:43\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:43\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:43\n",
      "     ------------------------ --------------- 1.5/2.5 GB 17.7 MB/s eta 0:00:54\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:42\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:42\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:42\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:42\n",
      "     ------------------------ --------------- 1.5/2.5 GB 17.7 MB/s eta 0:00:54\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:41\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:41\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:41\n",
      "     ------------------------ --------------- 1.5/2.5 GB 17.7 MB/s eta 0:00:54\n",
      "     ------------------------ --------------- 1.5/2.5 GB 24.2 MB/s eta 0:00:39\n",
      "     ------------------------ --------------- 1.5/2.5 GB 24.2 MB/s eta 0:00:39\n",
      "     ------------------------ --------------- 1.5/2.5 GB 24.2 MB/s eta 0:00:39\n",
      "     ------------------------ --------------- 1.5/2.5 GB 24.2 MB/s eta 0:00:39\n",
      "     ------------------------ --------------- 1.5/2.5 GB 24.2 MB/s eta 0:00:39\n",
      "     ------------------------ --------------- 1.5/2.5 GB 24.3 MB/s eta 0:00:39\n",
      "     ------------------------ --------------- 1.5/2.5 GB 24.3 MB/s eta 0:00:39\n",
      "     ------------------------ --------------- 1.5/2.5 GB 24.3 MB/s eta 0:00:39\n",
      "     ------------------------ --------------- 1.5/2.5 GB 17.7 MB/s eta 0:00:53\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:40\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:40\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:40\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:40\n",
      "     ------------------------ --------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:40\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:41\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:41\n",
      "     ------------------------ --------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:41\n",
      "     ------------------------ --------------- 1.5/2.5 GB 17.7 MB/s eta 0:00:53\n",
      "     ------------------------- -------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:40\n",
      "     ------------------------- -------------- 1.5/2.5 GB 21.8 MB/s eta 0:00:43\n",
      "     ------------------------- -------------- 1.5/2.5 GB 21.8 MB/s eta 0:00:43\n",
      "     ------------------------- -------------- 1.5/2.5 GB 17.7 MB/s eta 0:00:52\n",
      "     ------------------------- -------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:40\n",
      "     ------------------------- -------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:40\n",
      "     ------------------------- -------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:40\n",
      "     ------------------------- -------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:40\n",
      "     ------------------------- -------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:40\n",
      "     ------------------------- -------------- 1.5/2.5 GB 24.3 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.5/2.5 GB 24.3 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.5/2.5 GB 24.3 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.5/2.5 GB 17.7 MB/s eta 0:00:52\n",
      "     ------------------------- -------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:39\n",
      "     ------------------------- -------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:41\n",
      "     ------------------------- -------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:41\n",
      "     ------------------------- -------------- 1.5/2.5 GB 22.6 MB/s eta 0:00:41\n",
      "     ------------------------- -------------- 1.5/2.5 GB 23.4 MB/s eta 0:00:39\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:39\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:39\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:39\n",
      "     ------------------------- -------------- 1.6/2.5 GB 17.2 MB/s eta 0:00:53\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:39\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:39\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:39\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:39\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:39\n",
      "     ------------------------- -------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:37\n",
      "     ------------------------- -------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:37\n",
      "     ------------------------- -------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:37\n",
      "     ------------------------- -------------- 1.6/2.5 GB 17.7 MB/s eta 0:00:51\n",
      "     ------------------------- -------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:37\n",
      "     ------------------------- -------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:37\n",
      "     ------------------------- -------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:37\n",
      "     ------------------------- -------------- 1.6/2.5 GB 17.7 MB/s eta 0:00:51\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:37\n",
      "     ------------------------- -------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:37\n",
      "     ------------------------- -------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:37\n",
      "     ------------------------- -------------- 1.6/2.5 GB 17.7 MB/s eta 0:00:51\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.6/2.5 GB 17.7 MB/s eta 0:00:50\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.6/2.5 GB 17.2 MB/s eta 0:00:51\n",
      "     ------------------------- -------------- 1.6/2.5 GB 22.6 MB/s eta 0:00:39\n",
      "     ------------------------- -------------- 1.6/2.5 GB 22.6 MB/s eta 0:00:39\n",
      "     ------------------------- -------------- 1.6/2.5 GB 22.6 MB/s eta 0:00:39\n",
      "     ------------------------- -------------- 1.6/2.5 GB 17.7 MB/s eta 0:00:50\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.6/2.5 GB 17.7 MB/s eta 0:00:50\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:36\n",
      "     ------------------------- -------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:36\n",
      "     ------------------------- -------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:36\n",
      "     ------------------------- -------------- 1.6/2.5 GB 17.2 MB/s eta 0:00:51\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:38\n",
      "     ------------------------- -------------- 1.6/2.5 GB 17.2 MB/s eta 0:00:51\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:37\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:37\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:37\n",
      "     ------------------------- -------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:37\n",
      "     -------------------------- ------------- 1.6/2.5 GB 17.7 MB/s eta 0:00:49\n",
      "     -------------------------- ------------- 1.6/2.5 GB 22.6 MB/s eta 0:00:38\n",
      "     -------------------------- ------------- 1.6/2.5 GB 22.6 MB/s eta 0:00:38\n",
      "     -------------------------- ------------- 1.6/2.5 GB 22.6 MB/s eta 0:00:38\n",
      "     -------------------------- ------------- 1.6/2.5 GB 17.7 MB/s eta 0:00:49\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:37\n",
      "     -------------------------- ------------- 1.6/2.5 GB 21.8 MB/s eta 0:00:40\n",
      "     -------------------------- ------------- 1.6/2.5 GB 21.8 MB/s eta 0:00:40\n",
      "     -------------------------- ------------- 1.6/2.5 GB 21.8 MB/s eta 0:00:40\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:37\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:37\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:37\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:37\n",
      "     -------------------------- ------------- 1.6/2.5 GB 17.2 MB/s eta 0:00:50\n",
      "     -------------------------- ------------- 1.6/2.5 GB 21.8 MB/s eta 0:00:39\n",
      "     -------------------------- ------------- 1.6/2.5 GB 21.8 MB/s eta 0:00:39\n",
      "     -------------------------- ------------- 1.6/2.5 GB 21.8 MB/s eta 0:00:39\n",
      "     -------------------------- ------------- 1.6/2.5 GB 17.2 MB/s eta 0:00:50\n",
      "     -------------------------- ------------- 1.6/2.5 GB 18.2 MB/s eta 0:00:47\n",
      "     -------------------------- ------------- 1.6/2.5 GB 21.8 MB/s eta 0:00:39\n",
      "     -------------------------- ------------- 1.6/2.5 GB 21.8 MB/s eta 0:00:39\n",
      "     -------------------------- ------------- 1.6/2.5 GB 21.8 MB/s eta 0:00:39\n",
      "     -------------------------- ------------- 1.6/2.5 GB 16.8 MB/s eta 0:00:51\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:36\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:36\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:36\n",
      "     -------------------------- ------------- 1.6/2.5 GB 17.7 MB/s eta 0:00:48\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:36\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:36\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:36\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:36\n",
      "     -------------------------- ------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:35\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:36\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:36\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:36\n",
      "     -------------------------- ------------- 1.6/2.5 GB 18.2 MB/s eta 0:00:46\n",
      "     -------------------------- ------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:35\n",
      "     -------------------------- ------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:35\n",
      "     -------------------------- ------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:35\n",
      "     -------------------------- ------------- 1.6/2.5 GB 17.7 MB/s eta 0:00:47\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:36\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:36\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:36\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:36\n",
      "     -------------------------- ------------- 1.6/2.5 GB 17.7 MB/s eta 0:00:47\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:36\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:36\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:36\n",
      "     -------------------------- ------------- 1.6/2.5 GB 17.7 MB/s eta 0:00:47\n",
      "     -------------------------- ------------- 1.6/2.5 GB 22.6 MB/s eta 0:00:37\n",
      "     -------------------------- ------------- 1.6/2.5 GB 22.5 MB/s eta 0:00:37\n",
      "     -------------------------- ------------- 1.6/2.5 GB 22.5 MB/s eta 0:00:37\n",
      "     -------------------------- ------------- 1.6/2.5 GB 22.5 MB/s eta 0:00:37\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:35\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:35\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:35\n",
      "     -------------------------- ------------- 1.6/2.5 GB 23.4 MB/s eta 0:00:35\n",
      "     -------------------------- ------------- 1.6/2.5 GB 17.2 MB/s eta 0:00:47\n",
      "     -------------------------- ------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:34\n",
      "     -------------------------- ------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:34\n",
      "     -------------------------- ------------- 1.6/2.5 GB 24.2 MB/s eta 0:00:34\n",
      "     -------------------------- ------------- 1.6/2.5 GB 18.2 MB/s eta 0:00:45\n",
      "     -------------------------- ------------- 1.7/2.5 GB 23.4 MB/s eta 0:00:35\n",
      "     -------------------------- ------------- 1.7/2.5 GB 23.4 MB/s eta 0:00:35\n",
      "     -------------------------- ------------- 1.7/2.5 GB 23.4 MB/s eta 0:00:35\n",
      "     -------------------------- ------------- 1.7/2.5 GB 23.4 MB/s eta 0:00:35\n",
      "     -------------------------- ------------- 1.7/2.5 GB 17.7 MB/s eta 0:00:46\n",
      "     -------------------------- ------------- 1.7/2.5 GB 23.4 MB/s eta 0:00:35\n",
      "     -------------------------- ------------- 1.7/2.5 GB 23.4 MB/s eta 0:00:35\n",
      "     -------------------------- ------------- 1.7/2.5 GB 23.4 MB/s eta 0:00:35\n",
      "     --------------------------- ------------ 1.7/2.5 GB 17.7 MB/s eta 0:00:46\n",
      "     --------------------------- ------------ 1.7/2.5 GB 24.2 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 24.3 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 17.7 MB/s eta 0:00:45\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 17.3 MB/s eta 0:00:46\n",
      "     --------------------------- ------------ 1.7/2.5 GB 17.3 MB/s eta 0:00:46\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 17.2 MB/s eta 0:00:46\n",
      "     --------------------------- ------------ 1.7/2.5 GB 17.2 MB/s eta 0:00:46\n",
      "     --------------------------- ------------ 1.7/2.5 GB 17.2 MB/s eta 0:00:46\n",
      "     --------------------------- ------------ 1.7/2.5 GB 17.2 MB/s eta 0:00:46\n",
      "     --------------------------- ------------ 1.7/2.5 GB 16.8 MB/s eta 0:00:47\n",
      "     --------------------------- ------------ 1.7/2.5 GB 22.5 MB/s eta 0:00:35\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 17.2 MB/s eta 0:00:45\n",
      "     --------------------------- ------------ 1.7/2.5 GB 22.6 MB/s eta 0:00:35\n",
      "     --------------------------- ------------ 1.7/2.5 GB 22.6 MB/s eta 0:00:35\n",
      "     --------------------------- ------------ 1.7/2.5 GB 22.6 MB/s eta 0:00:35\n",
      "     --------------------------- ------------ 1.7/2.5 GB 17.2 MB/s eta 0:00:45\n",
      "     --------------------------- ------------ 1.7/2.5 GB 24.2 MB/s eta 0:00:32\n",
      "     --------------------------- ------------ 1.7/2.5 GB 24.2 MB/s eta 0:00:32\n",
      "     --------------------------- ------------ 1.7/2.5 GB 24.2 MB/s eta 0:00:32\n",
      "     --------------------------- ------------ 1.7/2.5 GB 24.2 MB/s eta 0:00:32\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 24.2 MB/s eta 0:00:32\n",
      "     --------------------------- ------------ 1.7/2.5 GB 24.2 MB/s eta 0:00:32\n",
      "     --------------------------- ------------ 1.7/2.5 GB 24.2 MB/s eta 0:00:32\n",
      "     --------------------------- ------------ 1.7/2.5 GB 17.7 MB/s eta 0:00:44\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 17.7 MB/s eta 0:00:43\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 17.7 MB/s eta 0:00:43\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 17.7 MB/s eta 0:00:43\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 17.2 MB/s eta 0:00:44\n",
      "     --------------------------- ------------ 1.7/2.5 GB 22.5 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 22.5 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 22.5 MB/s eta 0:00:34\n",
      "     --------------------------- ------------ 1.7/2.5 GB 17.7 MB/s eta 0:00:43\n",
      "     --------------------------- ------------ 1.7/2.5 GB 22.6 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 22.6 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 22.6 MB/s eta 0:00:33\n",
      "     --------------------------- ------------ 1.7/2.5 GB 17.3 MB/s eta 0:00:43\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:32\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:32\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:32\n",
      "     --------------------------- ------------ 1.7/2.5 GB 23.4 MB/s eta 0:00:32\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 17.7 MB/s eta 0:00:42\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:32\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:32\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:32\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 17.7 MB/s eta 0:00:42\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:32\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:32\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:32\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:32\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 17.2 MB/s eta 0:00:43\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:32\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:32\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:32\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 16.8 MB/s eta 0:00:44\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:31\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:31\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:31\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 17.7 MB/s eta 0:00:41\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 22.6 MB/s eta 0:00:32\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 22.6 MB/s eta 0:00:32\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 22.6 MB/s eta 0:00:32\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 22.6 MB/s eta 0:00:32\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 17.7 MB/s eta 0:00:41\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:31\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:31\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:31\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 17.2 MB/s eta 0:00:42\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 22.6 MB/s eta 0:00:32\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:31\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:31\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:31\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 17.7 MB/s eta 0:00:41\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:31\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:31\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 23.4 MB/s eta 0:00:31\n",
      "     ---------------------------- ----------- 1.7/2.5 GB 17.7 MB/s eta 0:00:40\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 24.2 MB/s eta 0:00:29\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 24.2 MB/s eta 0:00:29\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 24.2 MB/s eta 0:00:29\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 24.2 MB/s eta 0:00:29\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 23.4 MB/s eta 0:00:30\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 22.6 MB/s eta 0:00:31\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 22.6 MB/s eta 0:00:31\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 22.6 MB/s eta 0:00:31\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 17.7 MB/s eta 0:00:40\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 23.4 MB/s eta 0:00:30\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 23.4 MB/s eta 0:00:30\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 23.4 MB/s eta 0:00:30\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 17.7 MB/s eta 0:00:40\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 23.4 MB/s eta 0:00:30\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 23.4 MB/s eta 0:00:30\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 23.4 MB/s eta 0:00:30\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 23.4 MB/s eta 0:00:30\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 17.7 MB/s eta 0:00:39\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 24.2 MB/s eta 0:00:29\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 24.2 MB/s eta 0:00:29\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 24.2 MB/s eta 0:00:29\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 18.2 MB/s eta 0:00:38\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 23.4 MB/s eta 0:00:30\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 24.2 MB/s eta 0:00:29\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 24.2 MB/s eta 0:00:29\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 24.2 MB/s eta 0:00:29\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 23.4 MB/s eta 0:00:30\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 23.4 MB/s eta 0:00:29\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 23.4 MB/s eta 0:00:29\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 23.4 MB/s eta 0:00:29\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 17.7 MB/s eta 0:00:39\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:28\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:28\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:28\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 17.7 MB/s eta 0:00:39\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 22.6 MB/s eta 0:00:30\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:29\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:29\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:29\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 17.2 MB/s eta 0:00:39\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 22.6 MB/s eta 0:00:30\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 22.6 MB/s eta 0:00:30\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 22.6 MB/s eta 0:00:30\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 17.2 MB/s eta 0:00:39\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:29\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 22.5 MB/s eta 0:00:30\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 22.5 MB/s eta 0:00:30\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 22.5 MB/s eta 0:00:30\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 17.3 MB/s eta 0:00:39\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:29\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:29\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:29\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 17.7 MB/s eta 0:00:38\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:28\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:29\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:29\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:29\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 17.7 MB/s eta 0:00:37\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.3 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.3 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.3 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 17.7 MB/s eta 0:00:37\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 17.7 MB/s eta 0:00:37\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 17.7 MB/s eta 0:00:37\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:28\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 24.2 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 17.7 MB/s eta 0:00:36\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 17.2 MB/s eta 0:00:37\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 17.2 MB/s eta 0:00:37\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 21.8 MB/s eta 0:00:29\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 21.8 MB/s eta 0:00:29\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 21.8 MB/s eta 0:00:29\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 16.8 MB/s eta 0:00:38\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 17.2 MB/s eta 0:00:36\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 23.4 MB/s eta 0:00:27\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 22.6 MB/s eta 0:00:28\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 22.6 MB/s eta 0:00:28\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 22.6 MB/s eta 0:00:28\n",
      "     ------------------------------ --------- 1.8/2.5 GB 23.4 MB/s eta 0:00:27\n",
      "     ------------------------------ --------- 1.8/2.5 GB 23.4 MB/s eta 0:00:27\n",
      "     ------------------------------ --------- 1.8/2.5 GB 23.4 MB/s eta 0:00:27\n",
      "     ------------------------------ --------- 1.8/2.5 GB 23.4 MB/s eta 0:00:27\n",
      "     ------------------------------ --------- 1.8/2.5 GB 17.2 MB/s eta 0:00:36\n",
      "     ------------------------------ --------- 1.8/2.5 GB 23.4 MB/s eta 0:00:26\n",
      "     ------------------------------ --------- 1.8/2.5 GB 23.4 MB/s eta 0:00:26\n",
      "     ------------------------------ --------- 1.8/2.5 GB 23.4 MB/s eta 0:00:26\n",
      "     ------------------------------ --------- 1.8/2.5 GB 17.3 MB/s eta 0:00:36\n",
      "     ------------------------------ --------- 1.8/2.5 GB 22.6 MB/s eta 0:00:27\n",
      "     ------------------------------ --------- 1.9/2.5 GB 22.6 MB/s eta 0:00:27\n",
      "     ------------------------------ --------- 1.9/2.5 GB 22.6 MB/s eta 0:00:27\n",
      "     ------------------------------ --------- 1.9/2.5 GB 22.6 MB/s eta 0:00:27\n",
      "     ------------------------------ --------- 1.9/2.5 GB 17.7 MB/s eta 0:00:35\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:26\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:26\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:26\n",
      "     ------------------------------ --------- 1.9/2.5 GB 17.7 MB/s eta 0:00:34\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:26\n",
      "     ------------------------------ --------- 1.9/2.5 GB 22.6 MB/s eta 0:00:27\n",
      "     ------------------------------ --------- 1.9/2.5 GB 22.6 MB/s eta 0:00:27\n",
      "     ------------------------------ --------- 1.9/2.5 GB 22.6 MB/s eta 0:00:27\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:26\n",
      "     ------------------------------ --------- 1.9/2.5 GB 24.2 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 24.2 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 24.2 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 17.7 MB/s eta 0:00:34\n",
      "     ------------------------------ --------- 1.9/2.5 GB 24.2 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 24.2 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 24.2 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 24.2 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:26\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 17.7 MB/s eta 0:00:33\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 17.7 MB/s eta 0:00:33\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 17.7 MB/s eta 0:00:33\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 17.7 MB/s eta 0:00:33\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 22.6 MB/s eta 0:00:26\n",
      "     ------------------------------ --------- 1.9/2.5 GB 22.6 MB/s eta 0:00:26\n",
      "     ------------------------------ --------- 1.9/2.5 GB 22.6 MB/s eta 0:00:26\n",
      "     ------------------------------ --------- 1.9/2.5 GB 17.7 MB/s eta 0:00:32\n",
      "     ------------------------------ --------- 1.9/2.5 GB 24.2 MB/s eta 0:00:24\n",
      "     ------------------------------ --------- 1.9/2.5 GB 24.2 MB/s eta 0:00:24\n",
      "     ------------------------------ --------- 1.9/2.5 GB 24.2 MB/s eta 0:00:24\n",
      "     ------------------------------ --------- 1.9/2.5 GB 17.2 MB/s eta 0:00:33\n",
      "     ------------------------------ --------- 1.9/2.5 GB 22.5 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 17.7 MB/s eta 0:00:32\n",
      "     ------------------------------ --------- 1.9/2.5 GB 22.6 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 22.6 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 22.6 MB/s eta 0:00:25\n",
      "     ------------------------------ --------- 1.9/2.5 GB 17.7 MB/s eta 0:00:32\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:24\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:24\n",
      "     ------------------------------ --------- 1.9/2.5 GB 23.4 MB/s eta 0:00:24\n",
      "     ------------------------------ --------- 1.9/2.5 GB 17.7 MB/s eta 0:00:32\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:24\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:24\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:24\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:24\n",
      "     ------------------------------- -------- 1.9/2.5 GB 17.2 MB/s eta 0:00:32\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:24\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:24\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:24\n",
      "     ------------------------------- -------- 1.9/2.5 GB 17.3 MB/s eta 0:00:32\n",
      "     ------------------------------- -------- 1.9/2.5 GB 22.5 MB/s eta 0:00:25\n",
      "     ------------------------------- -------- 1.9/2.5 GB 22.5 MB/s eta 0:00:25\n",
      "     ------------------------------- -------- 1.9/2.5 GB 22.5 MB/s eta 0:00:25\n",
      "     ------------------------------- -------- 1.9/2.5 GB 22.5 MB/s eta 0:00:25\n",
      "     ------------------------------- -------- 1.9/2.5 GB 17.2 MB/s eta 0:00:32\n",
      "     ------------------------------- -------- 1.9/2.5 GB 22.6 MB/s eta 0:00:24\n",
      "     ------------------------------- -------- 1.9/2.5 GB 22.6 MB/s eta 0:00:24\n",
      "     ------------------------------- -------- 1.9/2.5 GB 22.6 MB/s eta 0:00:24\n",
      "     ------------------------------- -------- 1.9/2.5 GB 16.8 MB/s eta 0:00:33\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 24.2 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 17.7 MB/s eta 0:00:30\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 17.3 MB/s eta 0:00:31\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 17.2 MB/s eta 0:00:31\n",
      "     ------------------------------- -------- 1.9/2.5 GB 22.6 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 22.6 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 22.6 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 17.2 MB/s eta 0:00:31\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 22.6 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 22.6 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 22.6 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 1.9/2.5 GB 24.2 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 1.9/2.5 GB 24.2 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 1.9/2.5 GB 24.2 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 1.9/2.5 GB 17.3 MB/s eta 0:00:30\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 1.9/2.5 GB 23.4 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 2.0/2.5 GB 17.7 MB/s eta 0:00:29\n",
      "     ------------------------------- -------- 2.0/2.5 GB 22.6 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 2.0/2.5 GB 22.6 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 2.0/2.5 GB 22.6 MB/s eta 0:00:23\n",
      "     ------------------------------- -------- 2.0/2.5 GB 17.2 MB/s eta 0:00:30\n",
      "     ------------------------------- -------- 2.0/2.5 GB 23.4 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 2.0/2.5 GB 23.4 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 2.0/2.5 GB 23.4 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 2.0/2.5 GB 23.4 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 2.0/2.5 GB 17.2 MB/s eta 0:00:29\n",
      "     ------------------------------- -------- 2.0/2.5 GB 23.4 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 2.0/2.5 GB 23.4 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 2.0/2.5 GB 23.4 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 2.0/2.5 GB 17.7 MB/s eta 0:00:28\n",
      "     ------------------------------- -------- 2.0/2.5 GB 23.4 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 2.0/2.5 GB 23.4 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 2.0/2.5 GB 23.4 MB/s eta 0:00:22\n",
      "     ------------------------------- -------- 2.0/2.5 GB 17.7 MB/s eta 0:00:28\n",
      "     -------------------------------- ------- 2.0/2.5 GB 17.3 MB/s eta 0:00:29\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:21\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:21\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:21\n",
      "     -------------------------------- ------- 2.0/2.5 GB 17.2 MB/s eta 0:00:29\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:21\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:21\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:21\n",
      "     -------------------------------- ------- 2.0/2.5 GB 17.7 MB/s eta 0:00:28\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:21\n",
      "     -------------------------------- ------- 2.0/2.5 GB 24.2 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 24.2 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 24.2 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 17.7 MB/s eta 0:00:27\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:21\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:21\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:21\n",
      "     -------------------------------- ------- 2.0/2.5 GB 17.7 MB/s eta 0:00:27\n",
      "     -------------------------------- ------- 2.0/2.5 GB 24.3 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 24.3 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 24.3 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 24.3 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:21\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:21\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:21\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:21\n",
      "     -------------------------------- ------- 2.0/2.5 GB 17.2 MB/s eta 0:00:28\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 17.2 MB/s eta 0:00:27\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 17.7 MB/s eta 0:00:26\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 17.2 MB/s eta 0:00:27\n",
      "     -------------------------------- ------- 2.0/2.5 GB 17.3 MB/s eta 0:00:27\n",
      "     -------------------------------- ------- 2.0/2.5 GB 22.6 MB/s eta 0:00:21\n",
      "     -------------------------------- ------- 2.0/2.5 GB 22.6 MB/s eta 0:00:21\n",
      "     -------------------------------- ------- 2.0/2.5 GB 22.6 MB/s eta 0:00:21\n",
      "     -------------------------------- ------- 2.0/2.5 GB 17.2 MB/s eta 0:00:27\n",
      "     -------------------------------- ------- 2.0/2.5 GB 22.6 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 22.6 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 22.6 MB/s eta 0:00:20\n",
      "     -------------------------------- ------- 2.0/2.5 GB 17.3 MB/s eta 0:00:26\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     -------------------------------- ------- 2.0/2.5 GB 24.2 MB/s eta 0:00:19\n",
      "     -------------------------------- ------- 2.0/2.5 GB 24.2 MB/s eta 0:00:19\n",
      "     -------------------------------- ------- 2.0/2.5 GB 24.2 MB/s eta 0:00:19\n",
      "     -------------------------------- ------- 2.0/2.5 GB 17.7 MB/s eta 0:00:25\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     -------------------------------- ------- 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     -------------------------------- ------- 2.0/2.5 GB 17.7 MB/s eta 0:00:25\n",
      "     --------------------------------- ------ 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     --------------------------------- ------ 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     --------------------------------- ------ 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     --------------------------------- ------ 2.0/2.5 GB 18.2 MB/s eta 0:00:24\n",
      "     --------------------------------- ------ 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     --------------------------------- ------ 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     --------------------------------- ------ 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     --------------------------------- ------ 2.0/2.5 GB 23.4 MB/s eta 0:00:19\n",
      "     --------------------------------- ------ 2.0/2.5 GB 17.2 MB/s eta 0:00:25\n",
      "     --------------------------------- ------ 2.0/2.5 GB 22.6 MB/s eta 0:00:19\n",
      "     --------------------------------- ------ 2.0/2.5 GB 22.6 MB/s eta 0:00:19\n",
      "     --------------------------------- ------ 2.0/2.5 GB 22.6 MB/s eta 0:00:19\n",
      "     --------------------------------- ------ 2.0/2.5 GB 16.8 MB/s eta 0:00:26\n",
      "     --------------------------------- ------ 2.0/2.5 GB 17.7 MB/s eta 0:00:24\n",
      "     --------------------------------- ------ 2.0/2.5 GB 17.7 MB/s eta 0:00:24\n",
      "     --------------------------------- ------ 2.0/2.5 GB 17.7 MB/s eta 0:00:24\n",
      "     --------------------------------- ------ 2.0/2.5 GB 13.9 MB/s eta 0:00:31\n",
      "     --------------------------------- ------ 2.0/2.5 GB 16.8 MB/s eta 0:00:25\n",
      "     --------------------------------- ------ 2.0/2.5 GB 23.4 MB/s eta 0:00:18\n",
      "     --------------------------------- ------ 2.0/2.5 GB 23.4 MB/s eta 0:00:18\n",
      "     --------------------------------- ------ 2.0/2.5 GB 23.4 MB/s eta 0:00:18\n",
      "     --------------------------------- ------ 2.0/2.5 GB 17.7 MB/s eta 0:00:24\n",
      "     --------------------------------- ------ 2.0/2.5 GB 23.4 MB/s eta 0:00:18\n",
      "     --------------------------------- ------ 2.0/2.5 GB 23.4 MB/s eta 0:00:18\n",
      "     --------------------------------- ------ 2.0/2.5 GB 23.4 MB/s eta 0:00:18\n",
      "     --------------------------------- ------ 2.0/2.5 GB 17.2 MB/s eta 0:00:24\n",
      "     --------------------------------- ------ 2.0/2.5 GB 23.4 MB/s eta 0:00:18\n",
      "     --------------------------------- ------ 2.0/2.5 GB 24.2 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.0/2.5 GB 24.2 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.0/2.5 GB 24.2 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 24.2 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 23.4 MB/s eta 0:00:18\n",
      "     --------------------------------- ------ 2.1/2.5 GB 23.4 MB/s eta 0:00:18\n",
      "     --------------------------------- ------ 2.1/2.5 GB 23.4 MB/s eta 0:00:18\n",
      "     --------------------------------- ------ 2.1/2.5 GB 17.7 MB/s eta 0:00:23\n",
      "     --------------------------------- ------ 2.1/2.5 GB 24.2 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 24.2 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 24.2 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 17.7 MB/s eta 0:00:23\n",
      "     --------------------------------- ------ 2.1/2.5 GB 23.4 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 23.4 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 23.4 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 23.4 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 17.2 MB/s eta 0:00:23\n",
      "     --------------------------------- ------ 2.1/2.5 GB 23.4 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 23.4 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 23.4 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 17.2 MB/s eta 0:00:23\n",
      "     --------------------------------- ------ 2.1/2.5 GB 22.6 MB/s eta 0:00:18\n",
      "     --------------------------------- ------ 2.1/2.5 GB 22.5 MB/s eta 0:00:18\n",
      "     --------------------------------- ------ 2.1/2.5 GB 22.5 MB/s eta 0:00:18\n",
      "     --------------------------------- ------ 2.1/2.5 GB 22.5 MB/s eta 0:00:18\n",
      "     --------------------------------- ------ 2.1/2.5 GB 17.3 MB/s eta 0:00:23\n",
      "     --------------------------------- ------ 2.1/2.5 GB 22.6 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 22.6 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 22.6 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 17.2 MB/s eta 0:00:23\n",
      "     --------------------------------- ------ 2.1/2.5 GB 24.2 MB/s eta 0:00:16\n",
      "     --------------------------------- ------ 2.1/2.5 GB 24.2 MB/s eta 0:00:16\n",
      "     --------------------------------- ------ 2.1/2.5 GB 24.2 MB/s eta 0:00:16\n",
      "     --------------------------------- ------ 2.1/2.5 GB 24.2 MB/s eta 0:00:16\n",
      "     --------------------------------- ------ 2.1/2.5 GB 17.7 MB/s eta 0:00:22\n",
      "     --------------------------------- ------ 2.1/2.5 GB 22.6 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 22.6 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 22.6 MB/s eta 0:00:17\n",
      "     --------------------------------- ------ 2.1/2.5 GB 17.7 MB/s eta 0:00:22\n",
      "     --------------------------------- ------ 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     --------------------------------- ------ 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     --------------------------------- ------ 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     --------------------------------- ------ 2.1/2.5 GB 17.2 MB/s eta 0:00:22\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 17.7 MB/s eta 0:00:21\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:16\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 17.7 MB/s eta 0:00:20\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 24.2 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 24.2 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 24.2 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 18.2 MB/s eta 0:00:20\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 24.2 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 24.2 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 24.2 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 24.2 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 17.7 MB/s eta 0:00:20\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 17.7 MB/s eta 0:00:20\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 24.2 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 24.2 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 24.2 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 24.2 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 17.7 MB/s eta 0:00:19\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 17.7 MB/s eta 0:00:19\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 22.6 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 22.6 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 22.6 MB/s eta 0:00:15\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 17.2 MB/s eta 0:00:19\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 17.7 MB/s eta 0:00:19\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 17.7 MB/s eta 0:00:18\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 17.3 MB/s eta 0:00:19\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 22.6 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 22.6 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 22.6 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 17.2 MB/s eta 0:00:19\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 23.4 MB/s eta 0:00:14\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 24.2 MB/s eta 0:00:13\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 24.2 MB/s eta 0:00:13\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 24.2 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.1/2.5 GB 23.4 MB/s eta 0:00:14\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 18.2 MB/s eta 0:00:17\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 24.2 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 24.2 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 24.2 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 18.2 MB/s eta 0:00:17\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 17.7 MB/s eta 0:00:17\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 24.2 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 24.2 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 24.2 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 17.7 MB/s eta 0:00:17\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 22.6 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 22.6 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 22.6 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 17.3 MB/s eta 0:00:17\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 22.6 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 22.6 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 22.6 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 16.8 MB/s eta 0:00:17\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 17.3 MB/s eta 0:00:17\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 22.6 MB/s eta 0:00:13\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 17.7 MB/s eta 0:00:16\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 17.7 MB/s eta 0:00:16\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 17.2 MB/s eta 0:00:16\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 17.7 MB/s eta 0:00:15\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 17.7 MB/s eta 0:00:15\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 23.4 MB/s eta 0:00:12\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 24.2 MB/s eta 0:00:11\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 24.2 MB/s eta 0:00:11\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 24.2 MB/s eta 0:00:11\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 17.7 MB/s eta 0:00:15\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 24.2 MB/s eta 0:00:11\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 24.2 MB/s eta 0:00:11\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 24.2 MB/s eta 0:00:11\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 18.2 MB/s eta 0:00:14\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 25.2 MB/s eta 0:00:10\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 24.2 MB/s eta 0:00:11\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 24.2 MB/s eta 0:00:11\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 24.2 MB/s eta 0:00:11\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 24.2 MB/s eta 0:00:11\n",
      "     ------------------------------------ --- 2.2/2.5 GB 24.2 MB/s eta 0:00:11\n",
      "     ------------------------------------ --- 2.2/2.5 GB 24.2 MB/s eta 0:00:11\n",
      "     ------------------------------------ --- 2.2/2.5 GB 24.2 MB/s eta 0:00:11\n",
      "     ------------------------------------ --- 2.2/2.5 GB 18.2 MB/s eta 0:00:14\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:11\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:11\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:11\n",
      "     ------------------------------------ --- 2.2/2.5 GB 17.7 MB/s eta 0:00:14\n",
      "     ------------------------------------ --- 2.2/2.5 GB 24.2 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:11\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:11\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:11\n",
      "     ------------------------------------ --- 2.2/2.5 GB 18.2 MB/s eta 0:00:13\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 18.2 MB/s eta 0:00:13\n",
      "     ------------------------------------ --- 2.2/2.5 GB 24.2 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 24.2 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 24.2 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 24.2 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 24.2 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 24.2 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 24.2 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 24.2 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 17.7 MB/s eta 0:00:13\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:10\n",
      "     ------------------------------------ --- 2.2/2.5 GB 17.7 MB/s eta 0:00:13\n",
      "     ------------------------------------ --- 2.2/2.5 GB 24.2 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.2/2.5 GB 24.2 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.2/2.5 GB 24.2 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.2/2.5 GB 18.2 MB/s eta 0:00:12\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.2/2.5 GB 23.4 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.2/2.5 GB 17.2 MB/s eta 0:00:12\n",
      "     ------------------------------------ --- 2.3/2.5 GB 23.4 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.3/2.5 GB 23.4 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.3/2.5 GB 23.4 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.3/2.5 GB 17.7 MB/s eta 0:00:12\n",
      "     ------------------------------------ --- 2.3/2.5 GB 22.6 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.3/2.5 GB 23.4 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.3/2.5 GB 23.4 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.3/2.5 GB 23.4 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.3/2.5 GB 24.2 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.3/2.5 GB 24.2 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.3/2.5 GB 24.2 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.3/2.5 GB 24.2 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.3/2.5 GB 18.2 MB/s eta 0:00:11\n",
      "     ------------------------------------ --- 2.3/2.5 GB 24.2 MB/s eta 0:00:08\n",
      "     ------------------------------------ --- 2.3/2.5 GB 24.2 MB/s eta 0:00:08\n",
      "     ------------------------------------ --- 2.3/2.5 GB 24.2 MB/s eta 0:00:08\n",
      "     ------------------------------------ --- 2.3/2.5 GB 18.2 MB/s eta 0:00:11\n",
      "     ------------------------------------ --- 2.3/2.5 GB 24.2 MB/s eta 0:00:08\n",
      "     ------------------------------------ --- 2.3/2.5 GB 24.2 MB/s eta 0:00:08\n",
      "     ------------------------------------ --- 2.3/2.5 GB 24.2 MB/s eta 0:00:08\n",
      "     ------------------------------------ --- 2.3/2.5 GB 24.2 MB/s eta 0:00:08\n",
      "     ------------------------------------ --- 2.3/2.5 GB 18.2 MB/s eta 0:00:11\n",
      "     ------------------------------------- -- 2.3/2.5 GB 24.3 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 24.3 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 24.3 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 18.2 MB/s eta 0:00:11\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 17.7 MB/s eta 0:00:10\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 17.7 MB/s eta 0:00:10\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 17.7 MB/s eta 0:00:10\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 17.7 MB/s eta 0:00:10\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 17.7 MB/s eta 0:00:10\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 17.7 MB/s eta 0:00:09\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 17.7 MB/s eta 0:00:09\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.6 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.6 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.6 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.6 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 16.8 MB/s eta 0:00:09\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.5 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.5 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.5 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 17.2 MB/s eta 0:00:09\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.6 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.6 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.6 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.6 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.5 GB 17.7 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.6 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.5 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.5 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.5 GB 17.7 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.5 GB 17.2 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.6 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.6 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.6 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.5 GB 17.2 MB/s eta 0:00:08\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.6 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.6 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.5 GB 22.6 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.5 GB 23.4 MB/s eta 0:00:06\n",
      "     -------------------------------------- - 2.3/2.5 GB 24.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.3/2.5 GB 24.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.3/2.5 GB 24.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.3/2.5 GB 18.2 MB/s eta 0:00:07\n",
      "     -------------------------------------- - 2.3/2.5 GB 24.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.3/2.5 GB 24.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.3/2.5 GB 24.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.3/2.5 GB 24.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.3/2.5 GB 24.3 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.3/2.5 GB 24.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.3/2.5 GB 24.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.3/2.5 GB 24.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.3/2.5 GB 17.7 MB/s eta 0:00:07\n",
      "     -------------------------------------- - 2.3/2.5 GB 24.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.3/2.5 GB 24.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.3/2.5 GB 24.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.3/2.5 GB 17.7 MB/s eta 0:00:07\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.4/2.5 GB 24.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.4/2.5 GB 24.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.4/2.5 GB 24.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.4/2.5 GB 24.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.4/2.5 GB 24.3 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.4/2.5 GB 24.3 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.4/2.5 GB 24.3 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.4/2.5 GB 17.7 MB/s eta 0:00:06\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.4/2.5 GB 17.2 MB/s eta 0:00:06\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 17.7 MB/s eta 0:00:06\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 17.7 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 18.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.4/2.5 GB 24.2 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 24.2 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 24.2 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 24.2 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 17.7 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.5 GB 17.2 MB/s eta 0:00:05\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:03\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:03\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:03\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:03\n",
      "     -------------------------------------- - 2.4/2.5 GB 24.2 MB/s eta 0:00:03\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:03\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:03\n",
      "     -------------------------------------- - 2.4/2.5 GB 23.4 MB/s eta 0:00:03\n",
      "     -------------------------------------- - 2.4/2.5 GB 17.7 MB/s eta 0:00:04\n",
      "     ---------------------------------------  2.4/2.5 GB 24.2 MB/s eta 0:00:03\n",
      "     ---------------------------------------  2.4/2.5 GB 24.2 MB/s eta 0:00:03\n",
      "     ---------------------------------------  2.4/2.5 GB 24.2 MB/s eta 0:00:03\n",
      "     ---------------------------------------  2.4/2.5 GB 17.7 MB/s eta 0:00:04\n",
      "     ---------------------------------------  2.4/2.5 GB 17.2 MB/s eta 0:00:04\n",
      "     ---------------------------------------  2.4/2.5 GB 22.6 MB/s eta 0:00:03\n",
      "     ---------------------------------------  2.4/2.5 GB 22.6 MB/s eta 0:00:03\n",
      "     ---------------------------------------  2.4/2.5 GB 22.6 MB/s eta 0:00:03\n",
      "     ---------------------------------------  2.4/2.5 GB 17.2 MB/s eta 0:00:04\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:03\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:03\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:03\n",
      "     ---------------------------------------  2.4/2.5 GB 17.2 MB/s eta 0:00:04\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:03\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:03\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:03\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:03\n",
      "     ---------------------------------------  2.4/2.5 GB 17.2 MB/s eta 0:00:03\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 17.7 MB/s eta 0:00:03\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 17.2 MB/s eta 0:00:03\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 24.2 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 18.2 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 24.2 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 24.2 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 24.2 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 24.2 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 17.7 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 17.7 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 17.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 24.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 24.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 24.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 18.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 24.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 24.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 24.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 18.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 24.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 24.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 24.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 17.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.5/2.5 GB 3.7 MB/s eta 0:00:00\n",
      "Collecting torchvision==0.17.2\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.2%2Bcu121-cp312-cp312-win_amd64.whl (5.7 MB)\n",
      "     ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.1/5.7 MB 219.0 kB/s eta 0:00:26\n",
      "     ---------------------------------------- 0.1/5.7 MB 219.0 kB/s eta 0:00:26\n",
      "     ---------------------------------------- 0.1/5.7 MB 219.0 kB/s eta 0:00:26\n",
      "     ---------------------------------------- 0.1/5.7 MB 219.0 kB/s eta 0:00:26\n",
      "      --------------------------------------- 0.1/5.7 MB 262.6 kB/s eta 0:00:22\n",
      "      --------------------------------------- 0.1/5.7 MB 281.2 kB/s eta 0:00:20\n",
      "      --------------------------------------- 0.1/5.7 MB 281.2 kB/s eta 0:00:20\n",
      "      --------------------------------------- 0.1/5.7 MB 281.2 kB/s eta 0:00:20\n",
      "      --------------------------------------- 0.1/5.7 MB 281.2 kB/s eta 0:00:20\n",
      "     - -------------------------------------- 0.3/5.7 MB 448.2 kB/s eta 0:00:13\n",
      "     -- ------------------------------------- 0.3/5.7 MB 442.4 kB/s eta 0:00:13\n",
      "     -- ------------------------------------- 0.3/5.7 MB 442.4 kB/s eta 0:00:13\n",
      "     -- ------------------------------------- 0.3/5.7 MB 442.4 kB/s eta 0:00:13\n",
      "     ---- ----------------------------------- 0.6/5.7 MB 719.5 kB/s eta 0:00:08\n",
      "     ---- ----------------------------------- 0.6/5.7 MB 717.3 kB/s eta 0:00:08\n",
      "     ---- ----------------------------------- 0.6/5.7 MB 717.3 kB/s eta 0:00:08\n",
      "     ---- ----------------------------------- 0.6/5.7 MB 717.3 kB/s eta 0:00:08\n",
      "     -------- ------------------------------- 1.2/5.7 MB 1.2 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 1.2/5.7 MB 1.2 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 1.2/5.7 MB 1.2 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 1.2/5.7 MB 1.2 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 1.2/5.7 MB 1.2 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 2.2/5.7 MB 1.8 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 2.5/5.7 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 2.5/5.7 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 2.5/5.7 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 4.2/5.7 MB 3.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 5.1/5.7 MB 3.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 5.1/5.7 MB 3.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 5.1/5.7 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.7/5.7 MB 3.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.7/5.7 MB 3.5 MB/s eta 0:00:00\n",
      "Collecting torchaudio==2.2.2\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.2.2%2Bcu121-cp312-cp312-win_amd64.whl (4.0 MB)\n",
      "     ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.1/4.0 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.1/4.0 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.1/4.0 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.1/4.0 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/4.0 MB 655.8 kB/s eta 0:00:06\n",
      "     - -------------------------------------- 0.1/4.0 MB 655.8 kB/s eta 0:00:06\n",
      "     - -------------------------------------- 0.1/4.0 MB 655.8 kB/s eta 0:00:06\n",
      "     - -------------------------------------- 0.1/4.0 MB 655.8 kB/s eta 0:00:06\n",
      "     -- ------------------------------------- 0.2/4.0 MB 529.7 kB/s eta 0:00:08\n",
      "     -- ------------------------------------- 0.3/4.0 MB 706.2 kB/s eta 0:00:06\n",
      "     -- ------------------------------------- 0.3/4.0 MB 706.2 kB/s eta 0:00:06\n",
      "     -- ------------------------------------- 0.3/4.0 MB 706.2 kB/s eta 0:00:06\n",
      "     --- ------------------------------------ 0.3/4.0 MB 531.5 kB/s eta 0:00:08\n",
      "     ------ --------------------------------- 0.6/4.0 MB 1.0 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 0.6/4.0 MB 1.0 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 0.6/4.0 MB 1.0 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 0.6/4.0 MB 1.0 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 1.3/4.0 MB 1.6 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 1.3/4.0 MB 1.6 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 1.3/4.0 MB 1.6 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 1.3/4.0 MB 1.6 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 1.4/4.0 MB 1.4 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 2.7/4.0 MB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 2.7/4.0 MB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 2.7/4.0 MB 2.6 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 2.8/4.0 MB 2.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.0/4.0 MB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.2) (2024.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision==0.17.2) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision==0.17.2) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch==2.2.2) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch==2.2.2) (1.3.0)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.0\n",
      "    Uninstalling torch-2.3.0:\n",
      "      Successfully uninstalled torch-2.3.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.18.0\n",
      "    Uninstalling torchvision-0.18.0:\n",
      "      Successfully uninstalled torchvision-0.18.0\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.3.0\n",
      "    Uninstalling torchaudio-2.3.0:\n",
      "      Successfully uninstalled torchaudio-2.3.0\n",
      "Successfully installed torch-2.2.2+cu121 torchaudio-2.2.2+cu121 torchvision-0.17.2+cu121\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e98bed-e180-4c09-91b8-261946666ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "\n",
    "# Create a torch tensor that requires gradients\n",
    "t = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Perform a tensor operation simulating \"forward propagation\"\n",
    "tensor_sum = t.sum()\n",
    "\n",
    "# Perform back propagation\n",
    "tensor_sum.backward()\n",
    "\n",
    "# View the gradients\n",
    "t.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7e7d6b-8868-40fe-a158-a4cf206b8463",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m,\u001b[38;5;241m2.0\u001b[39m,\u001b[38;5;241m3.0\u001b[39m], requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([1.0,2.0,3.0], requires_grad=True)\n",
    "tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "412dd087-4110-40f7-802b-026c4cf209cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100.1000, 3240.1000],\n",
       "        [-200.2000, -234.1000],\n",
       "        [5000.5000,  150.1000],\n",
       "        [6000.6000, -125.1000],\n",
       "        [9000.9000, -673.1000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create feature\n",
    "features = np.array([[-100.1, 3240.1],\n",
    "                     [-200.2, -234.1],\n",
    "                     [5000.5, 150.1],\n",
    "                     [6000.6, -125.1],\n",
    "                     [9000.9, -673.1]])\n",
    "\n",
    "# Create scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Convert to a tensor\n",
    "features_standardized_tensor = torch.from_numpy(features)\n",
    "\n",
    "# Show features\n",
    "features_standardized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "325f4130-6902-4d37-a42d-3932aa00660e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA\n",
      " cpu\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m ngpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\u001b[39m,device)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\cuda\\__init__.py:414\u001b[0m, in \u001b[0;36mget_device_name\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    403\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\cuda\\__init__.py:444\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \n\u001b[0;32m    437\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 444\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    445\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\cuda\\__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "flag = torch.cuda.is_available()\n",
    "if flag:\n",
    "    print(\"CUDA\")\n",
    "else:\n",
    "    print(\"CUDA\")\n",
    "\n",
    "ngpu= 1\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(\"\",device)\n",
    "print(\"GPU \",torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e2ea11-4eb5-4340-99a0-2eb86dc70319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu121\n",
      "True\n",
      "cpu 0.04201769828796387 tensor(141144.1875)\n",
      "cuda:0 0.02201366424560547 tensor(141288.1250, device='cuda:0')\n",
      "cuda:0 0.0 tensor(141288.1250, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import \ttorch\n",
    "import  time\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "# print('hello, world.')\n",
    "\n",
    "\n",
    "a = torch.randn(10000, 1000)\n",
    "b = torch.randn(1000, 2000)\n",
    "\n",
    "t0 = time.time()\n",
    "c = torch.matmul(a, b)\n",
    "t1 = time.time()\n",
    "print(a.device, t1 - t0, c.norm(2))\n",
    "\n",
    "device = torch.device('cuda')\n",
    "a = a.to(device)\n",
    "b = b.to(device)\n",
    "\n",
    "t0 = time.time()\n",
    "c = torch.matmul(a, b)\n",
    "t2 = time.time()\n",
    "print(a.device, t2 - t0, c.norm(2))\n",
    "\n",
    "t0 = time.time()\n",
    "c = torch.matmul(a, b)\n",
    "t2 = time.time()\n",
    "print(a.device, t2 - t0, c.norm(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97e73730-7c7c-424d-838b-3104a69082cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1254,  1.9643],\n",
       "        [-1.1533, -0.5007],\n",
       "        [ 0.2953, -0.2281],\n",
       "        [ 0.5739, -0.4234],\n",
       "        [ 1.4096, -0.8122]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "import torch\n",
    "\n",
    "# Create features\n",
    "torch_features = torch.tensor([[-100.1, 3240.1],\n",
    "                               [-200.2, -234.1],\n",
    "                               [5000.5, 150.1],\n",
    "                               [6000.6, -125.1],\n",
    "                               [9000.9, -673.1]], requires_grad=True)\n",
    "\n",
    "# Compute the mean and standard deviation\n",
    "mean = torch_features.mean(0, keepdim=True)\n",
    "standard_deviation = torch_features.std(0, unbiased=False, keepdim=True)\n",
    "\n",
    "# Standardize the features using the mean and standard deviation\n",
    "torch_features_standardized = torch_features - mean\n",
    "torch_features_standardized /= standard_deviation\n",
    "\n",
    "# Show standardized features\n",
    "torch_features_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5235deeb-e4c8-4049-86bc-50967f7a8ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNeuralNet(\n",
       "  (fc1): Linear(in_features=10, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a neural network\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "loss_criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.RMSprop(network.parameters())\n",
    "\n",
    "# Show the network\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bbc4401-191e-4d1b-9b5c-ab88f466a6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNeuralNet(\n",
       "  (sequential): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate and view the network\n",
    "SimpleNeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26c046e1-bdd0-4a36-8f46-7ff6c8ab0c21",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dynamo is not supported on Python 3.12+",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Compile the model using torch 2.0's optimizer\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m network \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Train neural network\u001b[39;00m\n\u001b[0;32m     58\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:1801\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(model, fullgraph, dynamic, backend, mode, options, disable)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;66;03m# Temporary until we get proper support for python 3.12\u001b[39;00m\n\u001b[0;32m   1800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m12\u001b[39m):\n\u001b[1;32m-> 1801\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDynamo is not supported on Python 3.12+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;66;03m# Decorator mode\u001b[39;00m\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dynamo is not supported on Python 3.12+"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training and test sets\n",
    "features, target = make_classification(n_classes=2, n_features=10,\n",
    "    n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(target_train).float().view(-1, 1)\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(target_test).float().view(-1, 1)\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Initialize neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = RMSprop(network.parameters())\n",
    "\n",
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "network = torch.compile(network)\n",
    "\n",
    "# Train neural network\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n",
    "\n",
    "# Evaluate neural network\n",
    "with torch.no_grad():\n",
    "    output = network(x_test)\n",
    "    test_loss = criterion(output, y_test)\n",
    "    test_accuracy = (output.round() == y_test).float().mean()\n",
    "    print(\"Test Loss:\", test_loss.item(), \"\\tTest Accuracy:\",\n",
    "        test_accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a43ced9-d12e-4f36-85af-a883d6592b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- ---------------\n",
      "aiobotocore               2.12.1\n",
      "aiohttp                   3.9.3\n",
      "aioitertools              0.11.0\n",
      "aiosignal                 1.3.1\n",
      "anyio                     4.3.0\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n",
      "attrs                     23.2.0\n",
      "Babel                     2.14.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.1.0\n",
      "botocore                  1.34.51\n",
      "certifi                   2024.2.2\n",
      "cffi                      1.16.0\n",
      "charset-normalizer        3.3.2\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.2\n",
      "contourpy                 1.2.0\n",
      "cramjam                   2.8.2\n",
      "cycler                    0.12.1\n",
      "debugpy                   1.8.1\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "docker                    7.0.0\n",
      "et-xmlfile                1.1.0\n",
      "executing                 2.0.1\n",
      "faiss-cpu                 1.8.0\n",
      "fastavro                  1.9.4\n",
      "fastjsonschema            2.19.1\n",
      "fastparquet               2024.2.0\n",
      "filelock                  3.14.0\n",
      "fonttools                 4.50.0\n",
      "fqdn                      1.5.1\n",
      "frozenlist                1.4.1\n",
      "fsspec                    2024.2.0\n",
      "graphviz                  0.20.3\n",
      "greenlet                  3.0.3\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.4\n",
      "httpx                     0.27.0\n",
      "idna                      3.6\n",
      "intel-openmp              2021.4.0\n",
      "ipykernel                 6.29.3\n",
      "ipython                   8.22.2\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.3\n",
      "jmespath                  1.0.1\n",
      "joblib                    1.3.2\n",
      "json5                     0.9.22\n",
      "jsonpointer               2.4\n",
      "jsonschema                4.21.1\n",
      "jsonschema-specifications 2023.12.1\n",
      "jupyter_client            8.6.1\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.9.1\n",
      "jupyter-lsp               2.2.4\n",
      "jupyter_server            2.13.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.1.4\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.25.4\n",
      "kiwisolver                1.4.5\n",
      "lightgbm                  4.3.0\n",
      "MarkupSafe                2.1.5\n",
      "matplotlib                3.8.3\n",
      "matplotlib-inline         0.1.6\n",
      "mistune                   3.0.2\n",
      "mkl                       2021.4.0\n",
      "mpmath                    1.3.0\n",
      "multidict                 6.0.5\n",
      "nbclient                  0.10.0\n",
      "nbconvert                 7.16.2\n",
      "nbformat                  5.10.2\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.3\n",
      "notebook                  7.1.1\n",
      "notebook_shim             0.2.4\n",
      "numpy                     1.26.4\n",
      "openpyxl                  3.1.2\n",
      "overrides                 7.7.0\n",
      "packaging                 24.0\n",
      "pandas                    2.2.1\n",
      "pandavro                  1.8.0\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.3\n",
      "pillow                    10.2.0\n",
      "pip                       24.0\n",
      "platformdirs              4.2.0\n",
      "prometheus_client         0.20.0\n",
      "prompt-toolkit            3.0.43\n",
      "psutil                    5.9.8\n",
      "pure-eval                 0.2.2\n",
      "pyarrow                   15.0.1\n",
      "pycparser                 2.21\n",
      "pydotplus                 2.0.2\n",
      "Pygments                  2.17.2\n",
      "PyMySQL                   1.1.0\n",
      "pyparsing                 3.1.2\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        2.0.7\n",
      "pytz                      2024.1\n",
      "pywin32                   306\n",
      "pywinpty                  2.0.13\n",
      "PyYAML                    6.0.1\n",
      "pyzmq                     25.1.2\n",
      "referencing               0.33.0\n",
      "requests                  2.31.0\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.18.0\n",
      "s3fs                      2024.2.0\n",
      "scikit-learn              1.4.1.post1\n",
      "scipy                     1.12.0\n",
      "seaborn                   0.13.2\n",
      "Send2Trash                1.8.2\n",
      "six                       1.16.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.5\n",
      "SQLAlchemy                2.0.28\n",
      "stack-data                0.6.3\n",
      "sympy                     1.12\n",
      "tbb                       2021.12.0\n",
      "terminado                 0.18.1\n",
      "threadpoolctl             3.3.0\n",
      "tinycss2                  1.2.1\n",
      "torch                     2.2.2+cu121\n",
      "torchaudio                2.2.2+cu121\n",
      "torchvision               0.17.2+cu121\n",
      "tornado                   6.4\n",
      "tqdm                      4.66.4\n",
      "traitlets                 5.14.2\n",
      "types-python-dateutil     2.8.19.20240311\n",
      "typing_extensions         4.10.0\n",
      "tzdata                    2024.1\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.0.7\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 1.13\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.7.0\n",
      "wrapt                     1.16.0\n",
      "xgboost                   2.0.3\n",
      "yarl                      1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "512d8b6f-bcac-4d00-8aa8-f148c368b4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d004e6be-c00b-447e-aa85-19b38923d62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ea28a50-0bc8-4670-bfc4-bd3a74dcf258",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dynamo is not supported on Python 3.12+",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Compile the model using torch 2.0's optimizer\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m network \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Train neural network\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:1801\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(model, fullgraph, dynamic, backend, mode, options, disable)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;66;03m# Temporary until we get proper support for python 3.12\u001b[39;00m\n\u001b[0;32m   1800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m12\u001b[39m):\n\u001b[1;32m-> 1801\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDynamo is not supported on Python 3.12+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;66;03m# Decorator mode\u001b[39;00m\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dynamo is not supported on Python 3.12+"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N_CLASSES=3\n",
    "EPOCHS=3\n",
    "\n",
    "# Create training and test sets\n",
    "features, target = make_classification(n_classes=N_CLASSES, n_informative=9,\n",
    "    n_redundant=0, n_features=10, n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.nn.functional.one_hot(torch.from_numpy(target_train).long(),\n",
    "    num_classes=N_CLASSES).float()\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.nn.functional.one_hot(torch.from_numpy(target_test).long(),\n",
    "    num_classes=N_CLASSES).float()\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,3),\n",
    "            torch.nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Initialize neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = RMSprop(network.parameters())\n",
    "\n",
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "network = torch.compile(network)\n",
    "\n",
    "# Train neural network\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n",
    "\n",
    "# Evaluate neural network\n",
    "with torch.no_grad():\n",
    "    output = network(x_test)\n",
    "    test_loss = criterion(output, y_test)\n",
    "    test_accuracy = (output.round() == y_test).float().mean()\n",
    "    print(\"Test Loss:\", test_loss.item(), \"\\tTest Accuracy:\",\n",
    "        test_accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2cd5409-1167-4c12-a10a-ca9711b1e651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement python==3.11.9 (from versions: none)\n",
      "ERROR: No matching distribution found for python==3.11.9\n"
     ]
    }
   ],
   "source": [
    "pip install python==3.11.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c48b4e-2aa3-4ac8-a907-b01a0e98dcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: You must give at least one requirement to install (see \"pip help install\")\n"
     ]
    }
   ],
   "source": [
    "pip install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc33484-d160-4bf3-9b45-1323f29ff0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- ---------------\n",
      "aiobotocore               2.12.1\n",
      "aiohttp                   3.9.3\n",
      "aioitertools              0.11.0\n",
      "aiosignal                 1.3.1\n",
      "anyio                     4.3.0\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n",
      "attrs                     23.2.0\n",
      "Babel                     2.14.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.1.0\n",
      "botocore                  1.34.51\n",
      "certifi                   2024.2.2\n",
      "cffi                      1.16.0\n",
      "charset-normalizer        3.3.2\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.2\n",
      "contourpy                 1.2.0\n",
      "cramjam                   2.8.2\n",
      "cycler                    0.12.1\n",
      "debugpy                   1.8.1\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "docker                    7.0.0\n",
      "et-xmlfile                1.1.0\n",
      "executing                 2.0.1\n",
      "faiss-cpu                 1.8.0\n",
      "fastavro                  1.9.4\n",
      "fastjsonschema            2.19.1\n",
      "fastparquet               2024.2.0\n",
      "filelock                  3.14.0\n",
      "fonttools                 4.50.0\n",
      "fqdn                      1.5.1\n",
      "frozenlist                1.4.1\n",
      "fsspec                    2024.2.0\n",
      "graphviz                  0.20.3\n",
      "greenlet                  3.0.3\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.4\n",
      "httpx                     0.27.0\n",
      "idna                      3.6\n",
      "intel-openmp              2021.4.0\n",
      "ipykernel                 6.29.3\n",
      "ipython                   8.22.2\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.3\n",
      "jmespath                  1.0.1\n",
      "joblib                    1.3.2\n",
      "json5                     0.9.22\n",
      "jsonpointer               2.4\n",
      "jsonschema                4.21.1\n",
      "jsonschema-specifications 2023.12.1\n",
      "jupyter_client            8.6.1\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.9.1\n",
      "jupyter-lsp               2.2.4\n",
      "jupyter_server            2.13.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.1.4\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.25.4\n",
      "kiwisolver                1.4.5\n",
      "lightgbm                  4.3.0\n",
      "MarkupSafe                2.1.5\n",
      "matplotlib                3.8.3\n",
      "matplotlib-inline         0.1.6\n",
      "mistune                   3.0.2\n",
      "mkl                       2021.4.0\n",
      "mpmath                    1.3.0\n",
      "multidict                 6.0.5\n",
      "nbclient                  0.10.0\n",
      "nbconvert                 7.16.2\n",
      "nbformat                  5.10.2\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.3\n",
      "notebook                  7.1.1\n",
      "notebook_shim             0.2.4\n",
      "numpy                     1.26.4\n",
      "openpyxl                  3.1.2\n",
      "overrides                 7.7.0\n",
      "packaging                 24.0\n",
      "pandas                    2.2.1\n",
      "pandavro                  1.8.0\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.3\n",
      "pillow                    10.2.0\n",
      "pip                       24.0\n",
      "platformdirs              4.2.0\n",
      "prometheus_client         0.20.0\n",
      "prompt-toolkit            3.0.43\n",
      "psutil                    5.9.8\n",
      "pure-eval                 0.2.2\n",
      "pyarrow                   15.0.1\n",
      "pycparser                 2.21\n",
      "pydotplus                 2.0.2\n",
      "pyenv-win                 3.1.1\n",
      "Pygments                  2.17.2\n",
      "PyMySQL                   1.1.0\n",
      "pyparsing                 3.1.2\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        2.0.7\n",
      "pytz                      2024.1\n",
      "pywin32                   306\n",
      "pywinpty                  2.0.13\n",
      "PyYAML                    6.0.1\n",
      "pyzmq                     25.1.2\n",
      "referencing               0.33.0\n",
      "requests                  2.31.0\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.18.0\n",
      "s3fs                      2024.2.0\n",
      "scikit-learn              1.4.1.post1\n",
      "scipy                     1.12.0\n",
      "seaborn                   0.13.2\n",
      "Send2Trash                1.8.2\n",
      "six                       1.16.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.5\n",
      "SQLAlchemy                2.0.28\n",
      "stack-data                0.6.3\n",
      "sympy                     1.12\n",
      "tbb                       2021.12.0\n",
      "terminado                 0.18.1\n",
      "threadpoolctl             3.3.0\n",
      "tinycss2                  1.2.1\n",
      "torch                     2.2.2+cu121\n",
      "torchaudio                2.2.2+cu121\n",
      "torchvision               0.17.2+cu121\n",
      "tornado                   6.4\n",
      "tqdm                      4.66.4\n",
      "traitlets                 5.14.2\n",
      "types-python-dateutil     2.8.19.20240311\n",
      "typing_extensions         4.10.0\n",
      "tzdata                    2024.1\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.0.7\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 1.13\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.7.0\n",
      "wrapt                     1.16.0\n",
      "xgboost                   2.0.3\n",
      "yarl                      1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d51b6e-adfb-4a55-a18f-edab1c751ce4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dynamo is not supported on Python 3.12+",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Compile the model using torch 2.0's optimizer\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m network \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Train neural network\u001b[39;00m\n\u001b[0;32m     58\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:1801\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(model, fullgraph, dynamic, backend, mode, options, disable)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;66;03m# Temporary until we get proper support for python 3.12\u001b[39;00m\n\u001b[0;32m   1800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m12\u001b[39m):\n\u001b[1;32m-> 1801\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDynamo is not supported on Python 3.12+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;66;03m# Decorator mode\u001b[39;00m\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dynamo is not supported on Python 3.12+"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training and test sets\n",
    "features, target = make_classification(n_classes=2, n_features=10,\n",
    "    n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(target_train).float().view(-1, 1)\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(target_test).float().view(-1, 1)\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Initialize neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = RMSprop(network.parameters())\n",
    "\n",
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "network = torch.compile(network)\n",
    "\n",
    "# Train neural network\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n",
    "\n",
    "# Evaluate neural network\n",
    "with torch.no_grad():\n",
    "    output = network(x_test)\n",
    "    test_loss = criterion(output, y_test)\n",
    "    test_accuracy = (output.round() == y_test).float().mean()\n",
    "    print(\"Test Loss:\", test_loss.item(), \"\\tTest Accuracy:\",\n",
    "        test_accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c04ab5ab-f00f-42f8-a6bf-66c1de149f56",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpython\u001b[49m \n",
      "\u001b[1;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": [
    "python \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6feffdf-043b-4dcd-ae0a-1f0a4c579e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a92bb9-b09d-428b-bc3f-b834e19d8452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "anyio                     4.3.0\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n",
      "attrs                     23.2.0\n",
      "Babel                     2.15.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.1.0\n",
      "certifi                   2024.2.2\n",
      "cffi                      1.16.0\n",
      "charset-normalizer        3.3.2\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.2\n",
      "debugpy                   1.8.1\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "executing                 2.0.1\n",
      "fastjsonschema            2.19.1\n",
      "filelock                  3.14.0\n",
      "fqdn                      1.5.1\n",
      "fsspec                    2024.5.0\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.5\n",
      "httpx                     0.27.0\n",
      "idna                      3.7\n",
      "intel-openmp              2021.4.0\n",
      "ipykernel                 6.29.4\n",
      "ipython                   8.24.0\n",
      "ipywidgets                8.1.2\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.4\n",
      "json5                     0.9.25\n",
      "jsonpointer               2.4\n",
      "jsonschema                4.22.0\n",
      "jsonschema-specifications 2023.12.1\n",
      "jupyter                   1.0.0\n",
      "jupyter_client            8.6.2\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.10.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.14.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.2.1\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.2\n",
      "jupyterlab_widgets        3.0.10\n",
      "MarkupSafe                2.1.5\n",
      "matplotlib-inline         0.1.7\n",
      "mistune                   3.0.2\n",
      "mkl                       2021.4.0\n",
      "mpmath                    1.3.0\n",
      "nbclient                  0.10.0\n",
      "nbconvert                 7.16.4\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.3\n",
      "notebook                  7.2.0\n",
      "notebook_shim             0.2.4\n",
      "overrides                 7.7.0\n",
      "packaging                 24.0\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pip                       22.3\n",
      "platformdirs              4.2.2\n",
      "prometheus_client         0.20.0\n",
      "prompt-toolkit            3.0.43\n",
      "psutil                    5.9.8\n",
      "pure-eval                 0.2.2\n",
      "pycparser                 2.22\n",
      "Pygments                  2.18.0\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        2.0.7\n",
      "pywin32                   306\n",
      "pywinpty                  2.0.13\n",
      "PyYAML                    6.0.1\n",
      "pyzmq                     26.0.3\n",
      "qtconsole                 5.5.2\n",
      "QtPy                      2.4.1\n",
      "referencing               0.35.1\n",
      "requests                  2.32.2\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.18.1\n",
      "Send2Trash                1.8.3\n",
      "setuptools                65.5.0\n",
      "six                       1.16.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.5\n",
      "stack-data                0.6.3\n",
      "sympy                     1.12\n",
      "tbb                       2021.12.0\n",
      "terminado                 0.18.1\n",
      "tinycss2                  1.3.0\n",
      "torch                     2.3.0\n",
      "tornado                   6.4\n",
      "traitlets                 5.14.3\n",
      "types-python-dateutil     2.9.0.20240316\n",
      "typing_extensions         4.12.0\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.1\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 1.13\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "widgetsnbextension        4.0.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb5a03e4-63bc-411c-a0a8-d70f9ae69a2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RMSprop\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training and test sets\n",
    "features, target = make_classification(n_classes=2, n_features=10,\n",
    "    n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(target_train).float().view(-1, 1)\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(target_test).float().view(-1, 1)\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Initialize neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = RMSprop(network.parameters())\n",
    "\n",
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "network = torch.compile(network)\n",
    "\n",
    "# Train neural network\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n",
    "\n",
    "# Evaluate neural network\n",
    "with torch.no_grad():\n",
    "    output = network(x_test)\n",
    "    test_loss = criterion(output, y_test)\n",
    "    test_accuracy = (output.round() == y_test).float().mean()\n",
    "    print(\"Test Loss:\", test_loss.item(), \"\\tTest Accuracy:\",\n",
    "        test_accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d39fc508-90ab-4f81-9e4e-1be0de3c4a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "     ---------------------------------------- 15.8/15.8 MB 6.8 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67db8436-afe8-4d1b-8b92-dd58465f9229",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RMSprop\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_classification\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Create training and test sets\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training and test sets\n",
    "features, target = make_classification(n_classes=2, n_features=10,\n",
    "    n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(target_train).float().view(-1, 1)\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(target_test).float().view(-1, 1)\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Initialize neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = RMSprop(network.parameters())\n",
    "\n",
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "network = torch.compile(network)\n",
    "\n",
    "# Train neural network\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n",
    "\n",
    "# Evaluate neural network\n",
    "with torch.no_grad():\n",
    "    output = network(x_test)\n",
    "    test_loss = criterion(output, y_test)\n",
    "    test_accuracy = (output.round() == y_test).float().mean()\n",
    "    print(\"Test Loss:\", test_loss.item(), \"\\tTest Accuracy:\",\n",
    "        test_accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f985c3-dc2a-4249-9c9d-088d5fcb5fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.0-cp311-cp311-win_amd64.whl (11.0 MB)\n",
      "     --------------------------------------- 11.0/11.0 MB 19.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.0\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "     --------------------------------------- 46.2/46.2 MB 19.9 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "     ------------------------------------- 301.8/301.8 kB 18.2 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.0 scipy-1.13.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0c0581-5875-4688-aaef-70e875dd8022",
   "metadata": {},
   "outputs": [
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nInvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     61\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 62\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m     64\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:921\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[1;34m(frame, cache_entry, frame_state)\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(frame, cache_entry, hooks, frame_state)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:786\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[1;34m(frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m    784\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 786\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43minner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    789\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:400\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[1;34m(frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m    386\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[0;32m    388\u001b[0m signpost_event(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     },\n\u001b[0;32m    398\u001b[0m )\n\u001b[1;32m--> 400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:676\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[0;32m    674\u001b[0m fail_user_frame_lineno: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 676\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    679\u001b[0m     Unsupported,\n\u001b[0;32m    680\u001b[0m     TorchRuntimeError,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    687\u001b[0m     BisectValidationException,\n\u001b[0;32m    688\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:535\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[1;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[0;32m    533\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 535\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py:1036\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[1;34m(code, transformations, safe)\u001b[0m\n\u001b[0;32m   1033\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[0;32m   1034\u001b[0m propagate_line_nums(instructions)\n\u001b[1;32m-> 1036\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:165\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m cleanup \u001b[38;5;241m=\u001b[39m setup_compile_debug()\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    167\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:500\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[1;34m(instructions, code_options)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[1;32m--> 500\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[0;32m    502\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2149\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 2149\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:810\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[0;32m    808\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m--> 810\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m     ):\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:773\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    769\u001b[0m         unimplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    770\u001b[0m     TracingContext\u001b[38;5;241m.\u001b[39mset_current_loc(\n\u001b[0;32m    771\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_filename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineno, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\n\u001b[0;32m    772\u001b[0m     )\n\u001b[1;32m--> 773\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2268\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m   2263\u001b[0m _step_logger()(\n\u001b[0;32m   2264\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (RETURN_VALUE)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2266\u001b[0m )\n\u001b[0;32m   2267\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2268\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2271\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   2272\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2273\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:971\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[1;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[0;32m    968\u001b[0m     append_prefix_insts()\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;66;03m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[1;32m--> 971\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m         \u001b[38;5;241m+\u001b[39m [create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNPACK_SEQUENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m, arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(stack_values))]\n\u001b[0;32m    973\u001b[0m     )\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    975\u001b[0m     graph_output_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_var(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph_out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:1168\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[1;34m(self, tx, rv, root)\u001b[0m\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[1;32m-> 1168\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1169\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m disable(compiled_fn)\n\u001b[0;32m   1171\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_graphs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:1241\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_fn, e)\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[0;32m   1242\u001b[0m         e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[0;32m   1243\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m signpost_event(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1253\u001b[0m     },\n\u001b[0;32m   1254\u001b[0m )\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:1222\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverify_correctness:\n\u001b[0;32m   1221\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[1;32m-> 1222\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1223\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_fn did not return callable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py:117\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[1;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py:1729\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[1;34m(self, model_, inputs_)\u001b[0m\n\u001b[0;32m   1726\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[0;32m   1727\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[1;32m-> 1729\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1330\u001b[0m, in \u001b[0;36mcompile_fx\u001b[1;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[0;32m   1325\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m inference_compiler(unlifted_gm, example_inputs_)\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode), torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(\n\u001b[0;32m   1328\u001b[0m     tracing_context\n\u001b[0;32m   1329\u001b[0m ), compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[1;32m-> 1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py:58\u001b[0m, in \u001b[0;36maot_autograd.<locals>.compiler_fn\u001b[1;34m(gm, example_inputs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[1;32m---> 58\u001b[0m         cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m         counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:903\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[1;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler)\u001b[0m\n\u001b[0;32m    887\u001b[0m aot_config \u001b[38;5;241m=\u001b[39m AOTConfig(\n\u001b[0;32m    888\u001b[0m     fw_compiler\u001b[38;5;241m=\u001b[39mfw_compiler,\n\u001b[0;32m    889\u001b[0m     bw_compiler\u001b[38;5;241m=\u001b[39mbw_compiler,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    899\u001b[0m     no_tangents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    900\u001b[0m )\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[1;32m--> 903\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;66;03m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;66;03m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;66;03m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# convention.  This should get fixed...\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39mruntime_args):\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:628\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[1;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[0;32m    625\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m partial(aot_wrapper_dedupe, compiler_fn\u001b[38;5;241m=\u001b[39mcompiler_fn)\n\u001b[0;32m    626\u001b[0m \u001b[38;5;66;03m# You can put more passes here\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aot_config\u001b[38;5;241m.\u001b[39mis_export:\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# During export, we don't get back a callable - we get back the raw fx graph\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# (either a joint or an inference-only graph)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mGraphModule)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py:443\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[1;34m(flat_fn, flat_args, aot_config, compiler_fn, fw_metadata)\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ok:\n\u001b[1;32m--> 443\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleaf_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m requires_subclass_dispatch(leaf_flat_args, fw_metadata):\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    447\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"\\\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03mEncountered duplicate inputs that are mutated in the graph, but at least one input/output\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03mto the graph is a tensor subclass. This is not supported today. You can try to\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03mremove the aliasing yourself as a workaround, or otherwise file an issue on github.\"\"\"\u001b[39;00m\n\u001b[0;32m    451\u001b[0m         )\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py:648\u001b[0m, in \u001b[0;36maot_wrapper_synthetic_base\u001b[1;34m(flat_fn, flat_args, aot_config, fw_metadata, needs_autograd, compiler_fn)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;66;03m# Happy path: we don't need synthetic bases\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synthetic_base_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;66;03m# export path: ban synthetic bases for now, add later if requested.\u001b[39;00m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m requires_subclass_dispatch(flat_args, fw_metadata):\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py:352\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[1;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[0;32m    349\u001b[0m     tracing_context\u001b[38;5;241m.\u001b[39mfw_metadata \u001b[38;5;241m=\u001b[39m inner_meta\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TracingContext\u001b[38;5;241m.\u001b[39mreport_output_strides() \u001b[38;5;28;01mas\u001b[39;00m fwd_output_strides:\n\u001b[1;32m--> 352\u001b[0m     compiled_fw_func \u001b[38;5;241m=\u001b[39m \u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfw_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjusted_flat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(compiled_fw_func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    354\u001b[0m     compiled_fw_func \u001b[38;5;241m=\u001b[39m make_boxed_func(compiled_fw_func)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1257\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.fw_compiler_base\u001b[1;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m orig_output_end_idx \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m num_model_outputs\n\u001b[0;32m   1251\u001b[0m     user_visible_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1252\u001b[0m         n\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m   1253\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m model_outputs[original_output_start_index:orig_output_end_idx]\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mNode)\n\u001b[0;32m   1255\u001b[0m     }\n\u001b[1;32m-> 1257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_fixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxed_forward_device_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforward_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_visible_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_visible_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py:83\u001b[0m, in \u001b[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[1;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# Call the compiler_fn - which is either aot_autograd or inductor\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# with fake inputs\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     inner_compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# TODO: Failures here are troublesome because no real inputs,\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# need a different serialization strategy\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py:304\u001b[0m, in \u001b[0;36mDebugContext.wrap.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m DebugContext():\n\u001b[1;32m--> 304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:438\u001b[0m, in \u001b[0;36mcompile_fx_inner\u001b[1;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[0;32m    434\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m FxGraphCache\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    435\u001b[0m         fx_codegen_and_compile, gm, example_inputs, graph_kwargs\n\u001b[0;32m    436\u001b[0m     )\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 438\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFX codegen and compilation took \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# check cudagraph disabling reasons from inductor lowering\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:714\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[1;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[0;32m    711\u001b[0m             output_strides\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    713\u001b[0m metrics_helper \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mCachedMetricsHelper()\n\u001b[1;32m--> 714\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m V\u001b[38;5;241m.\u001b[39maot_compilation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py:1307\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_fn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AotCodeCompiler\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;28mself\u001b[39m, code, serialized_extern_kernel_nodes, cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda\n\u001b[0;32m   1305\u001b[0m     )\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcall\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py:1250\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;129m@dynamo_timed\u001b[39m(phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyCodeCache\n\u001b[0;32m   1249\u001b[0m     code, linemap \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1250\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1251\u001b[0m     )\n\u001b[0;32m   1252\u001b[0m     linemap \u001b[38;5;241m=\u001b[39m [(line_no, node\u001b[38;5;241m.\u001b[39mstack_trace) \u001b[38;5;28;01mfor\u001b[39;00m line_no, node \u001b[38;5;129;01min\u001b[39;00m linemap]\n\u001b[0;32m   1253\u001b[0m     key, path \u001b[38;5;241m=\u001b[39m PyCodeCache\u001b[38;5;241m.\u001b[39mwrite(code)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py:1208\u001b[0m, in \u001b[0;36mGraphLowering.codegen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m Scheduler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffers)\n\u001b[0;32m   1206\u001b[0m V\u001b[38;5;241m.\u001b[39mdebug\u001b[38;5;241m.\u001b[39mdraw_orig_fx_graph(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_gm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mnodes)\n\u001b[1;32m-> 1208\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper_code\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_inference)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:2339\u001b[0m, in \u001b[0;36mScheduler.codegen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_backend(device)\u001b[38;5;241m.\u001b[39mcodegen_foreach(node)  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n\u001b[0;32m   2338\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, (FusedSchedulerNode, SchedulerNode)):\n\u001b[1;32m-> 2339\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n\u001b[0;32m   2340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2341\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, NopKernelSchedulerNode)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:3623\u001b[0m, in \u001b[0;36mCppScheduling.codegen_nodes\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m   3620\u001b[0m cpp_kernel_proxy \u001b[38;5;241m=\u001b[39m CppKernelProxy(kernel_group)\n\u001b[0;32m   3621\u001b[0m cpp_kernel_proxy\u001b[38;5;241m.\u001b[39mcodegen_nodes(nodes)\n\u001b[1;32m-> 3623\u001b[0m \u001b[43mkernel_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpp_kernel_proxy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3625\u001b[0m args_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_scheduled_num_args()\n\u001b[0;32m   3626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args_num \u001b[38;5;241m>\u001b[39m CppScheduling\u001b[38;5;241m.\u001b[39mMAX_FUSED_KERNEL_ARGS_NUM:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:3661\u001b[0m, in \u001b[0;36mKernelGroup.finalize_kernel\u001b[1;34m(self, new_kernel, nodes)\u001b[0m\n\u001b[0;32m   3659\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloops_code\n\u001b[0;32m   3660\u001b[0m ws \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mws\n\u001b[1;32m-> 3661\u001b[0m \u001b[43mnew_kernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_loops\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mws\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:3458\u001b[0m, in \u001b[0;36mCppKernelProxy.codegen_loops\u001b[1;34m(self, code, worksharing)\u001b[0m\n\u001b[0;32m   3457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcodegen_loops\u001b[39m(\u001b[38;5;28mself\u001b[39m, code, worksharing):\n\u001b[1;32m-> 3458\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_loops_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop_nest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworksharing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:1832\u001b[0m, in \u001b[0;36mCppKernel.codegen_loops_impl\u001b[1;34m(self, loop_nest, code, worksharing)\u001b[0m\n\u001b[0;32m   1830\u001b[0m stack\u001b[38;5;241m.\u001b[39menter_context(code\u001b[38;5;241m.\u001b[39mindent())\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop_nest\u001b[38;5;241m.\u001b[39mroot:\n\u001b[1;32m-> 1832\u001b[0m     \u001b[43mgen_loops\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloop_nest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1834\u001b[0m     gen_kernel(loop_nest\u001b[38;5;241m.\u001b[39mkernel)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:1804\u001b[0m, in \u001b[0;36mCppKernel.codegen_loops_impl.<locals>.gen_loops\u001b[1;34m(loops, in_reduction)\u001b[0m\n\u001b[0;32m   1801\u001b[0m         worksharing\u001b[38;5;241m.\u001b[39mparallel(threads)\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loop \u001b[38;5;129;01min\u001b[39;00m loops:\n\u001b[1;32m-> 1804\u001b[0m     \u001b[43mgen_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_reduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loops:\n\u001b[0;32m   1807\u001b[0m     loop \u001b[38;5;241m=\u001b[39m loops[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:1817\u001b[0m, in \u001b[0;36mCppKernel.codegen_loops_impl.<locals>.gen_loop\u001b[1;34m(loop, in_reduction)\u001b[0m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen_loop\u001b[39m(loop: LoopLevel, in_reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mExitStack() \u001b[38;5;28;01mas\u001b[39;00m stack:\n\u001b[1;32m-> 1817\u001b[0m         loop_lines \u001b[38;5;241m=\u001b[39m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1818\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m loop_lines \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1819\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:3922\u001b[0m, in \u001b[0;36mLoopLevel.lines\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3920\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimd_omp:\n\u001b[0;32m   3921\u001b[0m     line1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#pragma omp \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mreduction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 3922\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction_var_map \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mcodecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_gcc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   3923\u001b[0m     line1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#pragma GCC ivdep\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3924\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py:1001\u001b[0m, in \u001b[0;36mis_gcc\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_gcc\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m-> 1001\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(gcc|g\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m+)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mcpp_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py:944\u001b[0m, in \u001b[0;36mcpp_compiler\u001b[1;34m()\u001b[0m\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    943\u001b[0m     search \u001b[38;5;241m=\u001b[39m (config\u001b[38;5;241m.\u001b[39mcpp\u001b[38;5;241m.\u001b[39mcxx,)\n\u001b[1;32m--> 944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcpp_compiler_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py:971\u001b[0m, in \u001b[0;36mcpp_compiler_search\u001b[1;34m(search)\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (subprocess\u001b[38;5;241m.\u001b[39mSubprocessError, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n\u001b[0;32m    970\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 971\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mInvalidCxxCompiler()\n",
      "\u001b[1;31mBackendCompilerFailed\u001b[0m: backend='inductor' raised:\nInvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training and test sets\n",
    "features, target = make_classification(n_classes=2, n_features=10,\n",
    "    n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(target_train).float().view(-1, 1)\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(target_test).float().view(-1, 1)\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Initialize neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = RMSprop(network.parameters())\n",
    "\n",
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "network = torch.compile(network)\n",
    "\n",
    "# Train neural network\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n",
    "\n",
    "# Evaluate neural network\n",
    "with torch.no_grad():\n",
    "    output = network(x_test)\n",
    "    test_loss = criterion(output, y_test)\n",
    "    test_accuracy = (output.round() == y_test).float().mean()\n",
    "    print(\"Test Loss:\", test_loss.item(), \"\\tTest Accuracy:\",\n",
    "        test_accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5106244b-b086-489c-a45f-e41a926201fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "anyio                     4.3.0\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n",
      "attrs                     23.2.0\n",
      "Babel                     2.15.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.1.0\n",
      "certifi                   2024.2.2\n",
      "cffi                      1.16.0\n",
      "charset-normalizer        3.3.2\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.2\n",
      "debugpy                   1.8.1\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "executing                 2.0.1\n",
      "fastjsonschema            2.19.1\n",
      "filelock                  3.14.0\n",
      "fqdn                      1.5.1\n",
      "fsspec                    2024.5.0\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.5\n",
      "httpx                     0.27.0\n",
      "idna                      3.7\n",
      "intel-openmp              2021.4.0\n",
      "ipykernel                 6.29.4\n",
      "ipython                   8.24.0\n",
      "ipywidgets                8.1.2\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.4\n",
      "joblib                    1.4.2\n",
      "json5                     0.9.25\n",
      "jsonpointer               2.4\n",
      "jsonschema                4.22.0\n",
      "jsonschema-specifications 2023.12.1\n",
      "jupyter                   1.0.0\n",
      "jupyter_client            8.6.2\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.10.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.14.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.2.1\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.2\n",
      "jupyterlab_widgets        3.0.10\n",
      "MarkupSafe                2.1.5\n",
      "matplotlib-inline         0.1.7\n",
      "mistune                   3.0.2\n",
      "mkl                       2021.4.0\n",
      "mpmath                    1.3.0\n",
      "nbclient                  0.10.0\n",
      "nbconvert                 7.16.4\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.3\n",
      "notebook                  7.2.0\n",
      "notebook_shim             0.2.4\n",
      "numpy                     1.26.4\n",
      "overrides                 7.7.0\n",
      "packaging                 24.0\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pip                       22.3\n",
      "platformdirs              4.2.2\n",
      "prometheus_client         0.20.0\n",
      "prompt-toolkit            3.0.43\n",
      "psutil                    5.9.8\n",
      "pure-eval                 0.2.2\n",
      "pycparser                 2.22\n",
      "Pygments                  2.18.0\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        2.0.7\n",
      "pywin32                   306\n",
      "pywinpty                  2.0.13\n",
      "PyYAML                    6.0.1\n",
      "pyzmq                     26.0.3\n",
      "qtconsole                 5.5.2\n",
      "QtPy                      2.4.1\n",
      "referencing               0.35.1\n",
      "requests                  2.32.2\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.18.1\n",
      "scikit-learn              1.5.0\n",
      "scipy                     1.13.1\n",
      "Send2Trash                1.8.3\n",
      "setuptools                65.5.0\n",
      "six                       1.16.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.5\n",
      "stack-data                0.6.3\n",
      "sympy                     1.12\n",
      "tbb                       2021.12.0\n",
      "terminado                 0.18.1\n",
      "threadpoolctl             3.5.0\n",
      "tinycss2                  1.3.0\n",
      "torch                     2.3.0\n",
      "tornado                   6.4\n",
      "traitlets                 5.14.3\n",
      "types-python-dateutil     2.9.0.20240316\n",
      "typing_extensions         4.12.0\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.1\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 1.13\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "widgetsnbextension        4.0.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fca3a08-8988-4e87-a2fb-a6241671af84",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define a neural network using `Sequential`\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSimpleNeuralNet\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28msuper\u001b[39m(SimpleNeuralNet, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate and view the network\n",
    "SimpleNeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e43cdbd-a798-4e27-806e-fb6daf2c5c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "\n",
    "# Create a torch tensor that requires gradients\n",
    "t = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Perform a tensor operation simulating \"forward propagation\"\n",
    "tensor_sum = t.sum()\n",
    "\n",
    "# Perform back propagation\n",
    "tensor_sum.backward()\n",
    "\n",
    "# View the gradients\n",
    "t.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9217bbe-760d-4846-8099-9a50a29f5355",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m,\u001b[38;5;241m2.0\u001b[39m,\u001b[38;5;241m3.0\u001b[39m], requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([1.0,2.0,3.0], requires_grad=True)\n",
    "tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83506b20-31e2-482e-a9ba-1e436a9d8827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100.1000, 3240.1000],\n",
       "        [-200.2000, -234.1000],\n",
       "        [5000.5000,  150.1000],\n",
       "        [6000.6000, -125.1000],\n",
       "        [9000.9000, -673.1000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create feature\n",
    "features = np.array([[-100.1, 3240.1],\n",
    "                     [-200.2, -234.1],\n",
    "                     [5000.5, 150.1],\n",
    "                     [6000.6, -125.1],\n",
    "                     [9000.9, -673.1]])\n",
    "\n",
    "# Create scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Convert to a tensor\n",
    "features_standardized_tensor = torch.from_numpy(features)\n",
    "\n",
    "# Show features\n",
    "features_standardized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7daa044d-e546-413a-b199-192d9c1322dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1254,  1.9643],\n",
       "        [-1.1533, -0.5007],\n",
       "        [ 0.2953, -0.2281],\n",
       "        [ 0.5739, -0.4234],\n",
       "        [ 1.4096, -0.8122]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "import torch\n",
    "\n",
    "# Create features\n",
    "torch_features = torch.tensor([[-100.1, 3240.1],\n",
    "                               [-200.2, -234.1],\n",
    "                               [5000.5, 150.1],\n",
    "                               [6000.6, -125.1],\n",
    "                               [9000.9, -673.1]], requires_grad=True)\n",
    "\n",
    "# Compute the mean and standard deviation\n",
    "mean = torch_features.mean(0, keepdim=True)\n",
    "standard_deviation = torch_features.std(0, unbiased=False, keepdim=True)\n",
    "\n",
    "# Standardize the features using the mean and standard deviation\n",
    "torch_features_standardized = torch_features - mean\n",
    "torch_features_standardized /= standard_deviation\n",
    "\n",
    "# Show standardized features\n",
    "torch_features_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fbff404-7fa3-480d-b6df-96f6bfd94882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNeuralNet(\n",
       "  (fc1): Linear(in_features=10, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a neural network\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "loss_criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.RMSprop(network.parameters())\n",
    "\n",
    "# Show the network\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fad410b-ed90-4105-b06b-9892672d1f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNeuralNet(\n",
       "  (sequential): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate and view the network\n",
    "SimpleNeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2a7ec1a-a8ec-40f6-bc31-592d290dca6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nInvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     61\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 62\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m     64\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:921\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[1;34m(frame, cache_entry, frame_state)\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(frame, cache_entry, hooks, frame_state)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:786\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[1;34m(frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m    784\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 786\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43minner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    789\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:400\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[1;34m(frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m    386\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[0;32m    388\u001b[0m signpost_event(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     },\n\u001b[0;32m    398\u001b[0m )\n\u001b[1;32m--> 400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:676\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[0;32m    674\u001b[0m fail_user_frame_lineno: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 676\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    679\u001b[0m     Unsupported,\n\u001b[0;32m    680\u001b[0m     TorchRuntimeError,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    687\u001b[0m     BisectValidationException,\n\u001b[0;32m    688\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:535\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[1;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[0;32m    533\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 535\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py:1036\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[1;34m(code, transformations, safe)\u001b[0m\n\u001b[0;32m   1033\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[0;32m   1034\u001b[0m propagate_line_nums(instructions)\n\u001b[1;32m-> 1036\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:165\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m cleanup \u001b[38;5;241m=\u001b[39m setup_compile_debug()\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    167\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:500\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[1;34m(instructions, code_options)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[1;32m--> 500\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[0;32m    502\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2149\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 2149\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:810\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[0;32m    808\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m--> 810\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m     ):\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:773\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    769\u001b[0m         unimplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    770\u001b[0m     TracingContext\u001b[38;5;241m.\u001b[39mset_current_loc(\n\u001b[0;32m    771\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_filename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineno, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\n\u001b[0;32m    772\u001b[0m     )\n\u001b[1;32m--> 773\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2268\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m   2263\u001b[0m _step_logger()(\n\u001b[0;32m   2264\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (RETURN_VALUE)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2266\u001b[0m )\n\u001b[0;32m   2267\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2268\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2271\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   2272\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2273\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:971\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[1;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[0;32m    968\u001b[0m     append_prefix_insts()\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;66;03m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[1;32m--> 971\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m         \u001b[38;5;241m+\u001b[39m [create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNPACK_SEQUENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m, arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(stack_values))]\n\u001b[0;32m    973\u001b[0m     )\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    975\u001b[0m     graph_output_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_var(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph_out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:1168\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[1;34m(self, tx, rv, root)\u001b[0m\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[1;32m-> 1168\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1169\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m disable(compiled_fn)\n\u001b[0;32m   1171\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_graphs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:1241\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_fn, e)\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[0;32m   1242\u001b[0m         e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[0;32m   1243\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m signpost_event(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1253\u001b[0m     },\n\u001b[0;32m   1254\u001b[0m )\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:1222\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverify_correctness:\n\u001b[0;32m   1221\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[1;32m-> 1222\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1223\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_fn did not return callable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py:117\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[1;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py:1729\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[1;34m(self, model_, inputs_)\u001b[0m\n\u001b[0;32m   1726\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[0;32m   1727\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[1;32m-> 1729\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1330\u001b[0m, in \u001b[0;36mcompile_fx\u001b[1;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[0;32m   1325\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m inference_compiler(unlifted_gm, example_inputs_)\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode), torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(\n\u001b[0;32m   1328\u001b[0m     tracing_context\n\u001b[0;32m   1329\u001b[0m ), compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[1;32m-> 1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py:58\u001b[0m, in \u001b[0;36maot_autograd.<locals>.compiler_fn\u001b[1;34m(gm, example_inputs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[1;32m---> 58\u001b[0m         cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m         counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:903\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[1;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler)\u001b[0m\n\u001b[0;32m    887\u001b[0m aot_config \u001b[38;5;241m=\u001b[39m AOTConfig(\n\u001b[0;32m    888\u001b[0m     fw_compiler\u001b[38;5;241m=\u001b[39mfw_compiler,\n\u001b[0;32m    889\u001b[0m     bw_compiler\u001b[38;5;241m=\u001b[39mbw_compiler,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    899\u001b[0m     no_tangents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    900\u001b[0m )\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[1;32m--> 903\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;66;03m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;66;03m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;66;03m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# convention.  This should get fixed...\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39mruntime_args):\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:628\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[1;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[0;32m    625\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m partial(aot_wrapper_dedupe, compiler_fn\u001b[38;5;241m=\u001b[39mcompiler_fn)\n\u001b[0;32m    626\u001b[0m \u001b[38;5;66;03m# You can put more passes here\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aot_config\u001b[38;5;241m.\u001b[39mis_export:\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# During export, we don't get back a callable - we get back the raw fx graph\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# (either a joint or an inference-only graph)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mGraphModule)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py:443\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[1;34m(flat_fn, flat_args, aot_config, compiler_fn, fw_metadata)\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ok:\n\u001b[1;32m--> 443\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleaf_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m requires_subclass_dispatch(leaf_flat_args, fw_metadata):\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    447\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"\\\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03mEncountered duplicate inputs that are mutated in the graph, but at least one input/output\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03mto the graph is a tensor subclass. This is not supported today. You can try to\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03mremove the aliasing yourself as a workaround, or otherwise file an issue on github.\"\"\"\u001b[39;00m\n\u001b[0;32m    451\u001b[0m         )\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py:648\u001b[0m, in \u001b[0;36maot_wrapper_synthetic_base\u001b[1;34m(flat_fn, flat_args, aot_config, fw_metadata, needs_autograd, compiler_fn)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;66;03m# Happy path: we don't need synthetic bases\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synthetic_base_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;66;03m# export path: ban synthetic bases for now, add later if requested.\u001b[39;00m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m requires_subclass_dispatch(flat_args, fw_metadata):\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py:352\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[1;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[0;32m    349\u001b[0m     tracing_context\u001b[38;5;241m.\u001b[39mfw_metadata \u001b[38;5;241m=\u001b[39m inner_meta\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TracingContext\u001b[38;5;241m.\u001b[39mreport_output_strides() \u001b[38;5;28;01mas\u001b[39;00m fwd_output_strides:\n\u001b[1;32m--> 352\u001b[0m     compiled_fw_func \u001b[38;5;241m=\u001b[39m \u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfw_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjusted_flat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(compiled_fw_func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    354\u001b[0m     compiled_fw_func \u001b[38;5;241m=\u001b[39m make_boxed_func(compiled_fw_func)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1257\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.fw_compiler_base\u001b[1;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m orig_output_end_idx \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m num_model_outputs\n\u001b[0;32m   1251\u001b[0m     user_visible_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1252\u001b[0m         n\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m   1253\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m model_outputs[original_output_start_index:orig_output_end_idx]\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mNode)\n\u001b[0;32m   1255\u001b[0m     }\n\u001b[1;32m-> 1257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_fixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxed_forward_device_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforward_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_visible_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_visible_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py:83\u001b[0m, in \u001b[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[1;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# Call the compiler_fn - which is either aot_autograd or inductor\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# with fake inputs\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     inner_compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# TODO: Failures here are troublesome because no real inputs,\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# need a different serialization strategy\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py:304\u001b[0m, in \u001b[0;36mDebugContext.wrap.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m DebugContext():\n\u001b[1;32m--> 304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:438\u001b[0m, in \u001b[0;36mcompile_fx_inner\u001b[1;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[0;32m    434\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m FxGraphCache\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    435\u001b[0m         fx_codegen_and_compile, gm, example_inputs, graph_kwargs\n\u001b[0;32m    436\u001b[0m     )\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 438\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFX codegen and compilation took \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# check cudagraph disabling reasons from inductor lowering\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:714\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[1;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[0;32m    711\u001b[0m             output_strides\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    713\u001b[0m metrics_helper \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mCachedMetricsHelper()\n\u001b[1;32m--> 714\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m V\u001b[38;5;241m.\u001b[39maot_compilation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py:1307\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_fn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AotCodeCompiler\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;28mself\u001b[39m, code, serialized_extern_kernel_nodes, cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda\n\u001b[0;32m   1305\u001b[0m     )\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcall\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py:1250\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;129m@dynamo_timed\u001b[39m(phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyCodeCache\n\u001b[0;32m   1249\u001b[0m     code, linemap \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1250\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1251\u001b[0m     )\n\u001b[0;32m   1252\u001b[0m     linemap \u001b[38;5;241m=\u001b[39m [(line_no, node\u001b[38;5;241m.\u001b[39mstack_trace) \u001b[38;5;28;01mfor\u001b[39;00m line_no, node \u001b[38;5;129;01min\u001b[39;00m linemap]\n\u001b[0;32m   1253\u001b[0m     key, path \u001b[38;5;241m=\u001b[39m PyCodeCache\u001b[38;5;241m.\u001b[39mwrite(code)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py:1208\u001b[0m, in \u001b[0;36mGraphLowering.codegen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m Scheduler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffers)\n\u001b[0;32m   1206\u001b[0m V\u001b[38;5;241m.\u001b[39mdebug\u001b[38;5;241m.\u001b[39mdraw_orig_fx_graph(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_gm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mnodes)\n\u001b[1;32m-> 1208\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper_code\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_inference)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:2339\u001b[0m, in \u001b[0;36mScheduler.codegen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_backend(device)\u001b[38;5;241m.\u001b[39mcodegen_foreach(node)  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n\u001b[0;32m   2338\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, (FusedSchedulerNode, SchedulerNode)):\n\u001b[1;32m-> 2339\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n\u001b[0;32m   2340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2341\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, NopKernelSchedulerNode)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:3623\u001b[0m, in \u001b[0;36mCppScheduling.codegen_nodes\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m   3620\u001b[0m cpp_kernel_proxy \u001b[38;5;241m=\u001b[39m CppKernelProxy(kernel_group)\n\u001b[0;32m   3621\u001b[0m cpp_kernel_proxy\u001b[38;5;241m.\u001b[39mcodegen_nodes(nodes)\n\u001b[1;32m-> 3623\u001b[0m \u001b[43mkernel_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpp_kernel_proxy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3625\u001b[0m args_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_scheduled_num_args()\n\u001b[0;32m   3626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args_num \u001b[38;5;241m>\u001b[39m CppScheduling\u001b[38;5;241m.\u001b[39mMAX_FUSED_KERNEL_ARGS_NUM:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:3661\u001b[0m, in \u001b[0;36mKernelGroup.finalize_kernel\u001b[1;34m(self, new_kernel, nodes)\u001b[0m\n\u001b[0;32m   3659\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloops_code\n\u001b[0;32m   3660\u001b[0m ws \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mws\n\u001b[1;32m-> 3661\u001b[0m \u001b[43mnew_kernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_loops\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mws\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:3458\u001b[0m, in \u001b[0;36mCppKernelProxy.codegen_loops\u001b[1;34m(self, code, worksharing)\u001b[0m\n\u001b[0;32m   3457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcodegen_loops\u001b[39m(\u001b[38;5;28mself\u001b[39m, code, worksharing):\n\u001b[1;32m-> 3458\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_loops_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop_nest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworksharing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:1832\u001b[0m, in \u001b[0;36mCppKernel.codegen_loops_impl\u001b[1;34m(self, loop_nest, code, worksharing)\u001b[0m\n\u001b[0;32m   1830\u001b[0m stack\u001b[38;5;241m.\u001b[39menter_context(code\u001b[38;5;241m.\u001b[39mindent())\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop_nest\u001b[38;5;241m.\u001b[39mroot:\n\u001b[1;32m-> 1832\u001b[0m     \u001b[43mgen_loops\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloop_nest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1834\u001b[0m     gen_kernel(loop_nest\u001b[38;5;241m.\u001b[39mkernel)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:1804\u001b[0m, in \u001b[0;36mCppKernel.codegen_loops_impl.<locals>.gen_loops\u001b[1;34m(loops, in_reduction)\u001b[0m\n\u001b[0;32m   1801\u001b[0m         worksharing\u001b[38;5;241m.\u001b[39mparallel(threads)\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loop \u001b[38;5;129;01min\u001b[39;00m loops:\n\u001b[1;32m-> 1804\u001b[0m     \u001b[43mgen_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_reduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loops:\n\u001b[0;32m   1807\u001b[0m     loop \u001b[38;5;241m=\u001b[39m loops[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:1817\u001b[0m, in \u001b[0;36mCppKernel.codegen_loops_impl.<locals>.gen_loop\u001b[1;34m(loop, in_reduction)\u001b[0m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen_loop\u001b[39m(loop: LoopLevel, in_reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mExitStack() \u001b[38;5;28;01mas\u001b[39;00m stack:\n\u001b[1;32m-> 1817\u001b[0m         loop_lines \u001b[38;5;241m=\u001b[39m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1818\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m loop_lines \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1819\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:3922\u001b[0m, in \u001b[0;36mLoopLevel.lines\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3920\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimd_omp:\n\u001b[0;32m   3921\u001b[0m     line1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#pragma omp \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mreduction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 3922\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction_var_map \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mcodecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_gcc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   3923\u001b[0m     line1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#pragma GCC ivdep\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3924\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py:1001\u001b[0m, in \u001b[0;36mis_gcc\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_gcc\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m-> 1001\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(gcc|g\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m+)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mcpp_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py:944\u001b[0m, in \u001b[0;36mcpp_compiler\u001b[1;34m()\u001b[0m\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    943\u001b[0m     search \u001b[38;5;241m=\u001b[39m (config\u001b[38;5;241m.\u001b[39mcpp\u001b[38;5;241m.\u001b[39mcxx,)\n\u001b[1;32m--> 944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcpp_compiler_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py:971\u001b[0m, in \u001b[0;36mcpp_compiler_search\u001b[1;34m(search)\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (subprocess\u001b[38;5;241m.\u001b[39mSubprocessError, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n\u001b[0;32m    970\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 971\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mInvalidCxxCompiler()\n",
      "\u001b[1;31mBackendCompilerFailed\u001b[0m: backend='inductor' raised:\nInvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training and test sets\n",
    "features, target = make_classification(n_classes=2, n_features=10,\n",
    "    n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(target_train).float().view(-1, 1)\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(target_test).float().view(-1, 1)\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Initialize neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = RMSprop(network.parameters())\n",
    "\n",
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "network = torch.compile(network)\n",
    "\n",
    "# Train neural network\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n",
    "\n",
    "# Evaluate neural network\n",
    "with torch.no_grad():\n",
    "    output = network(x_test)\n",
    "    test_loss = criterion(output, y_test)\n",
    "    test_accuracy = (output.round() == y_test).float().mean()\n",
    "    print(\"Test Loss:\", test_loss.item(), \"\\tTest Accuracy:\",\n",
    "        test_accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f37b8b5-1838-4bd2-b827-1e9a7346ee81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824] WON'T CONVERT forward C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_67216\\2229624549.py line 43 \n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824] due to: \n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 971, in compile_subgraph\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1241, in call_user_compiler\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1222, in call_user_compiler\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py\", line 1729, in __call__\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1208, in codegen\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     self.scheduler.codegen()\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 2339, in codegen\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     self.get_backend(device).codegen_nodes(node.get_nodes())  # type: ignore[possibly-undefined]\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3623, in codegen_nodes\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3661, in finalize_kernel\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     new_kernel.codegen_loops(code, ws)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3458, in codegen_loops\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     self.codegen_loops_impl(self.loop_nest, code, worksharing)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1832, in codegen_loops_impl\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     gen_loops(loop_nest.root)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1804, in gen_loops\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     gen_loop(loop, in_reduction)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1817, in gen_loop\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     loop_lines = loop.lines()\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3922, in lines\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     elif not self.reduction_var_map and codecache.is_gcc():\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                                         ^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1001, in is_gcc\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler()))\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                                           ^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 944, in cpp_compiler\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return cpp_compiler_search(search)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 971, in cpp_compiler_search\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     raise exc.InvalidCxxCompiler()\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824] InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 971, in compile_subgraph\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1241, in call_user_compiler\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1222, in call_user_compiler\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py\", line 1729, in __call__\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1208, in codegen\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     self.scheduler.codegen()\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 2339, in codegen\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     self.get_backend(device).codegen_nodes(node.get_nodes())  # type: ignore[possibly-undefined]\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3623, in codegen_nodes\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3661, in finalize_kernel\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     new_kernel.codegen_loops(code, ws)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3458, in codegen_loops\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     self.codegen_loops_impl(self.loop_nest, code, worksharing)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1832, in codegen_loops_impl\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     gen_loops(loop_nest.root)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1804, in gen_loops\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     gen_loop(loop, in_reduction)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1817, in gen_loop\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     loop_lines = loop.lines()\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3922, in lines\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     elif not self.reduction_var_map and codecache.is_gcc():\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                                         ^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1001, in is_gcc\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler()))\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]                                           ^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 944, in cpp_compiler\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     return cpp_compiler_search(search)\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 971, in cpp_compiler_search\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824]     raise exc.InvalidCxxCompiler()\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824] InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0524 21:26:45.768000 60880 torch\\_dynamo\\convert_frame.py:824] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 0.19006994366645813\n",
      "Epoch: 2 \tLoss: 0.14092367887496948\n",
      "Epoch: 3 \tLoss: 0.03935524821281433\n",
      "Test Loss: 0.06877756118774414 \tTest Accuracy: 0.9700000286102295\n"
     ]
    }
   ],
   "source": [
    "import torch._dynamo\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# Create training and test sets\n",
    "features, target = make_classification(n_classes=2, n_features=10,\n",
    "    n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(target_train).float().view(-1, 1)\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(target_test).float().view(-1, 1)\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Initialize neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = RMSprop(network.parameters())\n",
    "\n",
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "network = torch.compile(network)\n",
    "\n",
    "# Train neural network\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n",
    "\n",
    "# Evaluate neural network\n",
    "with torch.no_grad():\n",
    "    output = network(x_test)\n",
    "    test_loss = criterion(output, y_test)\n",
    "    test_accuracy = (output.round() == y_test).float().mean()\n",
    "    print(\"Test Loss:\", test_loss.item(), \"\\tTest Accuracy:\",\n",
    "        test_accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17be7105-6d6d-459b-bd6b-940fb457096a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824] WON'T CONVERT forward C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_67216\\2567128700.py line 44 \n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824] due to: \n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 971, in compile_subgraph\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1241, in call_user_compiler\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1222, in call_user_compiler\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py\", line 1729, in __call__\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1208, in codegen\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     self.scheduler.codegen()\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 2339, in codegen\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     self.get_backend(device).codegen_nodes(node.get_nodes())  # type: ignore[possibly-undefined]\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3623, in codegen_nodes\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3661, in finalize_kernel\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     new_kernel.codegen_loops(code, ws)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3458, in codegen_loops\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     self.codegen_loops_impl(self.loop_nest, code, worksharing)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1832, in codegen_loops_impl\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     gen_loops(loop_nest.root)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1804, in gen_loops\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     gen_loop(loop, in_reduction)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1817, in gen_loop\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     loop_lines = loop.lines()\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3922, in lines\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     elif not self.reduction_var_map and codecache.is_gcc():\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                                         ^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1001, in is_gcc\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler()))\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                                           ^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 944, in cpp_compiler\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return cpp_compiler_search(search)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 971, in cpp_compiler_search\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     raise exc.InvalidCxxCompiler()\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824] InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 971, in compile_subgraph\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1241, in call_user_compiler\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1222, in call_user_compiler\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py\", line 1729, in __call__\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1208, in codegen\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     self.scheduler.codegen()\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 2339, in codegen\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     self.get_backend(device).codegen_nodes(node.get_nodes())  # type: ignore[possibly-undefined]\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3623, in codegen_nodes\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3661, in finalize_kernel\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     new_kernel.codegen_loops(code, ws)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3458, in codegen_loops\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     self.codegen_loops_impl(self.loop_nest, code, worksharing)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1832, in codegen_loops_impl\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     gen_loops(loop_nest.root)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1804, in gen_loops\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     gen_loop(loop, in_reduction)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1817, in gen_loop\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     loop_lines = loop.lines()\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3922, in lines\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     elif not self.reduction_var_map and codecache.is_gcc():\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                                         ^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1001, in is_gcc\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler()))\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]                                           ^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 944, in cpp_compiler\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     return cpp_compiler_search(search)\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 971, in cpp_compiler_search\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824]     raise exc.InvalidCxxCompiler()\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824] InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0524 21:26:59.838000 60880 torch\\_dynamo\\convert_frame.py:824] \n",
      "C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 0.8022039532661438\n",
      "Epoch: 2 \tLoss: 0.7756164073944092\n",
      "Epoch: 3 \tLoss: 0.7751266360282898\n",
      "Test Loss: 0.8105319738388062 \tTest Accuracy: 0.8199999928474426\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N_CLASSES=3\n",
    "EPOCHS=3\n",
    "\n",
    "# Create training and test sets\n",
    "features, target = make_classification(n_classes=N_CLASSES, n_informative=9,\n",
    "    n_redundant=0, n_features=10, n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.nn.functional.one_hot(torch.from_numpy(target_train).long(),\n",
    "    num_classes=N_CLASSES).float()\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.nn.functional.one_hot(torch.from_numpy(target_test).long(),\n",
    "    num_classes=N_CLASSES).float()\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,3),\n",
    "            torch.nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Initialize neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = RMSprop(network.parameters())\n",
    "\n",
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "network = torch.compile(network)\n",
    "\n",
    "# Train neural network\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n",
    "\n",
    "# Evaluate neural network\n",
    "with torch.no_grad():\n",
    "    output = network(x_test)\n",
    "    test_loss = criterion(output, y_test)\n",
    "    test_accuracy = (output.round() == y_test).float().mean()\n",
    "    print(\"Test Loss:\", test_loss.item(), \"\\tTest Accuracy:\",\n",
    "        test_accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e2803eb-16e5-4628-a3b9-1cf1978738f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View target matrix\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb2dcb25-149f-4d3a-b3de-06a6edbd2768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824] WON'T CONVERT forward C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_67216\\4065691556.py line 39 \n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824] due to: \n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 971, in compile_subgraph\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1241, in call_user_compiler\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1222, in call_user_compiler\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py\", line 1729, in __call__\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1208, in codegen\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     self.scheduler.codegen()\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 2339, in codegen\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     self.get_backend(device).codegen_nodes(node.get_nodes())  # type: ignore[possibly-undefined]\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3623, in codegen_nodes\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3661, in finalize_kernel\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     new_kernel.codegen_loops(code, ws)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3458, in codegen_loops\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     self.codegen_loops_impl(self.loop_nest, code, worksharing)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1832, in codegen_loops_impl\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     gen_loops(loop_nest.root)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1804, in gen_loops\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     gen_loop(loop, in_reduction)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1817, in gen_loop\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     loop_lines = loop.lines()\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3922, in lines\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     elif not self.reduction_var_map and codecache.is_gcc():\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                                         ^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1001, in is_gcc\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler()))\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                                           ^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 944, in cpp_compiler\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return cpp_compiler_search(search)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 971, in cpp_compiler_search\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     raise exc.InvalidCxxCompiler()\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824] InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 971, in compile_subgraph\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1241, in call_user_compiler\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1222, in call_user_compiler\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py\", line 1729, in __call__\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1208, in codegen\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     self.scheduler.codegen()\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 2339, in codegen\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     self.get_backend(device).codegen_nodes(node.get_nodes())  # type: ignore[possibly-undefined]\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3623, in codegen_nodes\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3661, in finalize_kernel\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     new_kernel.codegen_loops(code, ws)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3458, in codegen_loops\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     self.codegen_loops_impl(self.loop_nest, code, worksharing)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1832, in codegen_loops_impl\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     gen_loops(loop_nest.root)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1804, in gen_loops\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     gen_loop(loop, in_reduction)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1817, in gen_loop\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     loop_lines = loop.lines()\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3922, in lines\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     elif not self.reduction_var_map and codecache.is_gcc():\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                                         ^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1001, in is_gcc\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler()))\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]                                           ^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 944, in cpp_compiler\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     return cpp_compiler_search(search)\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 971, in cpp_compiler_search\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824]     raise exc.InvalidCxxCompiler()\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824] InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0524 21:32:47.858000 60880 torch\\_dynamo\\convert_frame.py:824] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 10764.02734375\n",
      "Epoch: 2 \tLoss: 1356.51025390625\n",
      "Epoch: 3 \tLoss: 504.9664306640625\n",
      "Epoch: 4 \tLoss: 199.11312866210938\n",
      "Epoch: 5 \tLoss: 191.20828247070312\n",
      "Test MSE: 162.2450408935547\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "EPOCHS=5\n",
    "\n",
    "# Create training and test sets\n",
    "features, target = make_regression(n_features=10, n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(target_train).float().view(-1,1)\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(target_test).float().view(-1,1)\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Initialize neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = RMSprop(network.parameters())\n",
    "\n",
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "network = torch.compile(network)\n",
    "\n",
    "# Train neural network\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n",
    "\n",
    "# Evaluate neural network\n",
    "with torch.no_grad():\n",
    "    output = network(x_test)\n",
    "    test_loss = float(criterion(output, y_test))\n",
    "    print(\"Test MSE:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0837435-cb56-4d91-aa64-9bdd72356860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824] WON'T CONVERT forward C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_67216\\1055360726.py line 39 \n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824] due to: \n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 971, in compile_subgraph\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1241, in call_user_compiler\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1222, in call_user_compiler\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py\", line 1729, in __call__\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1208, in codegen\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     self.scheduler.codegen()\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 2339, in codegen\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     self.get_backend(device).codegen_nodes(node.get_nodes())  # type: ignore[possibly-undefined]\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3623, in codegen_nodes\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3661, in finalize_kernel\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     new_kernel.codegen_loops(code, ws)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3458, in codegen_loops\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     self.codegen_loops_impl(self.loop_nest, code, worksharing)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1832, in codegen_loops_impl\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     gen_loops(loop_nest.root)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1804, in gen_loops\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     gen_loop(loop, in_reduction)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1817, in gen_loop\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     loop_lines = loop.lines()\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3922, in lines\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     elif not self.reduction_var_map and codecache.is_gcc():\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                                         ^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1001, in is_gcc\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler()))\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                                           ^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 944, in cpp_compiler\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return cpp_compiler_search(search)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 971, in cpp_compiler_search\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     raise exc.InvalidCxxCompiler()\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824] InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 971, in compile_subgraph\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1241, in call_user_compiler\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1222, in call_user_compiler\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py\", line 1729, in __call__\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1208, in codegen\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     self.scheduler.codegen()\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 2339, in codegen\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     self.get_backend(device).codegen_nodes(node.get_nodes())  # type: ignore[possibly-undefined]\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3623, in codegen_nodes\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3661, in finalize_kernel\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     new_kernel.codegen_loops(code, ws)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3458, in codegen_loops\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     self.codegen_loops_impl(self.loop_nest, code, worksharing)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1832, in codegen_loops_impl\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     gen_loops(loop_nest.root)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1804, in gen_loops\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     gen_loop(loop, in_reduction)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1817, in gen_loop\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     loop_lines = loop.lines()\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3922, in lines\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     elif not self.reduction_var_map and codecache.is_gcc():\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                                         ^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1001, in is_gcc\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler()))\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]                                           ^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 944, in cpp_compiler\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     return cpp_compiler_search(search)\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 971, in cpp_compiler_search\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824]     raise exc.InvalidCxxCompiler()\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824] InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0524 21:33:17.637000 60880 torch\\_dynamo\\convert_frame.py:824] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 0.19006994366645813\n",
      "Epoch: 2 \tLoss: 0.14092367887496948\n",
      "Epoch: 3 \tLoss: 0.03935524821281433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training and test sets\n",
    "features, target = make_classification(n_classes=2, n_features=10,\n",
    "    n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(target_train).float().view(-1, 1)\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(target_test).float().view(-1, 1)\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Initialize neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = RMSprop(network.parameters())\n",
    "\n",
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "network = torch.compile(network)\n",
    "\n",
    "# Train neural network\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n",
    "\n",
    "# Evaluate neural network\n",
    "with torch.no_grad():\n",
    "    predicted_class = network.forward(x_train).round()\n",
    "\n",
    "predicted_class[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5a847bf-bee4-4377-9707-63b3c8f389d2",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "\n",
    "# Create a torch tensor that requires gradients\n",
    "t = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Perform a tensor operation simulating \"forward propagation\"\n",
    "tensor_sum = t.sum()\n",
    "\n",
    "# Perform back propagation\n",
    "tensor_sum.backward()\n",
    "\n",
    "# View the gradients\n",
    "t.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad0ad2a-ca64-435c-a062-812052e3b89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "\n",
    "# Create a torch tensor that requires gradients\n",
    "t = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Perform a tensor operation simulating \"forward propagation\"\n",
    "tensor_sum = t.sum()\n",
    "\n",
    "# Perform back propagation\n",
    "tensor_sum.backward()\n",
    "\n",
    "# View the gradients\n",
    "t.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f03fdc1f-95c2-48aa-a755-c91c4a906b60",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m,\u001b[38;5;241m2.0\u001b[39m,\u001b[38;5;241m3.0\u001b[39m], requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([1.0,2.0,3.0], requires_grad=True)\n",
    "tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e9e4af-88d2-4221-9707-80ef2572b554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([1.0,2.0,3.0], requires_grad=True)\n",
    "tensor.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6130428-b821-4b7b-b989-a6736e50623f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100.1000, 3240.1000],\n",
       "        [-200.2000, -234.1000],\n",
       "        [5000.5000,  150.1000],\n",
       "        [6000.6000, -125.1000],\n",
       "        [9000.9000, -673.1000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Create feature\n",
    "features = np.array([[-100.1, 3240.1],\n",
    "                     [-200.2, -234.1],\n",
    "                     [5000.5, 150.1],\n",
    "                     [6000.6, -125.1],\n",
    "                     [9000.9, -673.1]])\n",
    "\n",
    "# Create scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Convert to a tensor\n",
    "features_standardized_tensor = torch.from_numpy(features)\n",
    "\n",
    "# Show features\n",
    "features_standardized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "593f07e0-bb88-40a3-bd7e-1af8ac32b3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1254,  1.9643],\n",
       "        [-1.1533, -0.5007],\n",
       "        [ 0.2953, -0.2281],\n",
       "        [ 0.5739, -0.4234],\n",
       "        [ 1.4096, -0.8122]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "import torch\n",
    "\n",
    "# Create features\n",
    "torch_features = torch.tensor([[-100.1, 3240.1],\n",
    "                               [-200.2, -234.1],\n",
    "                               [5000.5, 150.1],\n",
    "                               [6000.6, -125.1],\n",
    "                               [9000.9, -673.1]], requires_grad=True)\n",
    "\n",
    "# Compute the mean and standard deviation\n",
    "mean = torch_features.mean(0, keepdim=True)\n",
    "standard_deviation = torch_features.std(0, unbiased=False, keepdim=True)\n",
    "\n",
    "# Standardize the features using the mean and standard deviation\n",
    "torch_features_standardized = torch_features - mean\n",
    "torch_features_standardized /= standard_deviation\n",
    "\n",
    "# Show standardized features\n",
    "torch_features_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd22e1ed-484b-4ba0-b5de-3013a4506780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNeuralNet(\n",
       "  (fc1): Linear(in_features=10, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a neural network\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "loss_criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.RMSprop(network.parameters())\n",
    "\n",
    "# Show the network\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4896a655-f243-4c8d-bfbb-13f86d2b6ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNeuralNet(\n",
       "  (sequential): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate and view the network\n",
    "SimpleNeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce2eae76-af23-4fc5-9023-899e610fe909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824] WON'T CONVERT forward C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_53068\\1528128064.py line 40 \n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824] due to: \n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 971, in compile_subgraph\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1241, in call_user_compiler\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1222, in call_user_compiler\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py\", line 1729, in __call__\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1208, in codegen\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     self.scheduler.codegen()\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 2339, in codegen\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     self.get_backend(device).codegen_nodes(node.get_nodes())  # type: ignore[possibly-undefined]\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3623, in codegen_nodes\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3661, in finalize_kernel\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     new_kernel.codegen_loops(code, ws)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3458, in codegen_loops\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     self.codegen_loops_impl(self.loop_nest, code, worksharing)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1832, in codegen_loops_impl\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     gen_loops(loop_nest.root)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1804, in gen_loops\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     gen_loop(loop, in_reduction)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1817, in gen_loop\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     loop_lines = loop.lines()\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3922, in lines\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     elif not self.reduction_var_map and codecache.is_gcc():\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                                         ^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1001, in is_gcc\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler()))\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                                           ^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 944, in cpp_compiler\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return cpp_compiler_search(search)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 971, in cpp_compiler_search\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     raise exc.InvalidCxxCompiler()\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824] InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 971, in compile_subgraph\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1241, in call_user_compiler\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1222, in call_user_compiler\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py\", line 1729, in __call__\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1208, in codegen\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     self.scheduler.codegen()\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 2339, in codegen\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     self.get_backend(device).codegen_nodes(node.get_nodes())  # type: ignore[possibly-undefined]\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3623, in codegen_nodes\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3661, in finalize_kernel\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     new_kernel.codegen_loops(code, ws)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3458, in codegen_loops\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     self.codegen_loops_impl(self.loop_nest, code, worksharing)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1832, in codegen_loops_impl\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     gen_loops(loop_nest.root)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1804, in gen_loops\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     gen_loop(loop, in_reduction)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1817, in gen_loop\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     loop_lines = loop.lines()\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3922, in lines\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     elif not self.reduction_var_map and codecache.is_gcc():\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                                         ^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1001, in is_gcc\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler()))\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]                                           ^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 944, in cpp_compiler\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     return cpp_compiler_search(search)\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 971, in cpp_compiler_search\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824]     raise exc.InvalidCxxCompiler()\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824] InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0524 22:34:04.383000 47072 torch\\_dynamo\\convert_frame.py:824] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 0.19006994366645813\n",
      "Epoch: 2 \tLoss: 0.14092367887496948\n",
      "Epoch: 3 \tLoss: 0.03935524821281433\n",
      "Test Loss: 0.06877756118774414 \tTest Accuracy: 0.9700000286102295\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "# Create training and test sets\n",
    "features, target = make_classification(n_classes=2, n_features=10,\n",
    "    n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(target_train).float().view(-1, 1)\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(target_test).float().view(-1, 1)\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Initialize neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = RMSprop(network.parameters())\n",
    "\n",
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "network = torch.compile(network)\n",
    "\n",
    "# Train neural network\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n",
    "\n",
    "# Evaluate neural network\n",
    "with torch.no_grad():\n",
    "    output = network(x_test)\n",
    "    test_loss = criterion(output, y_test)\n",
    "    test_accuracy = (output.round() == y_test).float().mean()\n",
    "    print(\"Test Loss:\", test_loss.item(), \"\\tTest Accuracy:\",\n",
    "        test_accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b590d5de-6be4-4ec6-87ba-5471a0e38363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824] WON'T CONVERT forward C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_53068\\2567128700.py line 44 \n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824] due to: \n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 971, in compile_subgraph\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1241, in call_user_compiler\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1222, in call_user_compiler\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py\", line 1729, in __call__\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1208, in codegen\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     self.scheduler.codegen()\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 2339, in codegen\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     self.get_backend(device).codegen_nodes(node.get_nodes())  # type: ignore[possibly-undefined]\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3623, in codegen_nodes\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3661, in finalize_kernel\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     new_kernel.codegen_loops(code, ws)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3458, in codegen_loops\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     self.codegen_loops_impl(self.loop_nest, code, worksharing)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1832, in codegen_loops_impl\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     gen_loops(loop_nest.root)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1804, in gen_loops\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     gen_loop(loop, in_reduction)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1817, in gen_loop\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     loop_lines = loop.lines()\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3922, in lines\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     elif not self.reduction_var_map and codecache.is_gcc():\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                                         ^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1001, in is_gcc\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler()))\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                                           ^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 944, in cpp_compiler\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return cpp_compiler_search(search)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 971, in cpp_compiler_search\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     raise exc.InvalidCxxCompiler()\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824] InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 971, in compile_subgraph\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1241, in call_user_compiler\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1222, in call_user_compiler\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py\", line 1729, in __call__\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1208, in codegen\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     self.scheduler.codegen()\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 2339, in codegen\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     self.get_backend(device).codegen_nodes(node.get_nodes())  # type: ignore[possibly-undefined]\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3623, in codegen_nodes\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3661, in finalize_kernel\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     new_kernel.codegen_loops(code, ws)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3458, in codegen_loops\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     self.codegen_loops_impl(self.loop_nest, code, worksharing)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1832, in codegen_loops_impl\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     gen_loops(loop_nest.root)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1804, in gen_loops\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     gen_loop(loop, in_reduction)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1817, in gen_loop\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     loop_lines = loop.lines()\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3922, in lines\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     elif not self.reduction_var_map and codecache.is_gcc():\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                                         ^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1001, in is_gcc\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler()))\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]                                           ^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 944, in cpp_compiler\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     return cpp_compiler_search(search)\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 971, in cpp_compiler_search\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824]     raise exc.InvalidCxxCompiler()\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824] InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0524 22:43:12.713000 47072 torch\\_dynamo\\convert_frame.py:824] \n",
      "C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 0.8022039532661438\n",
      "Epoch: 2 \tLoss: 0.7756164073944092\n",
      "Epoch: 3 \tLoss: 0.7751266360282898\n",
      "Test Loss: 0.8105319738388062 \tTest Accuracy: 0.8199999928474426\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N_CLASSES=3\n",
    "EPOCHS=3\n",
    "\n",
    "# Create training and test sets\n",
    "features, target = make_classification(n_classes=N_CLASSES, n_informative=9,\n",
    "    n_redundant=0, n_features=10, n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.nn.functional.one_hot(torch.from_numpy(target_train).long(),\n",
    "    num_classes=N_CLASSES).float()\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.nn.functional.one_hot(torch.from_numpy(target_test).long(),\n",
    "    num_classes=N_CLASSES).float()\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,3),\n",
    "            torch.nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Initialize neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = RMSprop(network.parameters())\n",
    "\n",
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "network = torch.compile(network)\n",
    "\n",
    "# Train neural network\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n",
    "\n",
    "# Evaluate neural network\n",
    "with torch.no_grad():\n",
    "    output = network(x_test)\n",
    "    test_loss = criterion(output, y_test)\n",
    "    test_accuracy = (output.round() == y_test).float().mean()\n",
    "    print(\"Test Loss:\", test_loss.item(), \"\\tTest Accuracy:\",\n",
    "        test_accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24e7ee96-8063-42c4-8262-4e6ea15647d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View target matrix\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0464b5b-04a7-485e-afe9-538d3f1436a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824] WON'T CONVERT forward C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_53068\\4065691556.py line 39 \n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824] due to: \n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 971, in compile_subgraph\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1241, in call_user_compiler\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1222, in call_user_compiler\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py\", line 1729, in __call__\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1208, in codegen\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     self.scheduler.codegen()\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 2339, in codegen\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     self.get_backend(device).codegen_nodes(node.get_nodes())  # type: ignore[possibly-undefined]\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3623, in codegen_nodes\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3661, in finalize_kernel\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     new_kernel.codegen_loops(code, ws)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3458, in codegen_loops\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     self.codegen_loops_impl(self.loop_nest, code, worksharing)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1832, in codegen_loops_impl\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     gen_loops(loop_nest.root)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1804, in gen_loops\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     gen_loop(loop, in_reduction)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1817, in gen_loop\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     loop_lines = loop.lines()\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3922, in lines\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     elif not self.reduction_var_map and codecache.is_gcc():\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                                         ^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1001, in is_gcc\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler()))\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                                           ^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 944, in cpp_compiler\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return cpp_compiler_search(search)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 971, in cpp_compiler_search\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     raise exc.InvalidCxxCompiler()\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824] InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 971, in compile_subgraph\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1241, in call_user_compiler\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1222, in call_user_compiler\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py\", line 1729, in __call__\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1208, in codegen\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     self.scheduler.codegen()\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 2339, in codegen\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     self.get_backend(device).codegen_nodes(node.get_nodes())  # type: ignore[possibly-undefined]\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3623, in codegen_nodes\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3661, in finalize_kernel\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     new_kernel.codegen_loops(code, ws)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3458, in codegen_loops\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     self.codegen_loops_impl(self.loop_nest, code, worksharing)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1832, in codegen_loops_impl\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     gen_loops(loop_nest.root)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1804, in gen_loops\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     gen_loop(loop, in_reduction)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1817, in gen_loop\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     loop_lines = loop.lines()\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3922, in lines\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     elif not self.reduction_var_map and codecache.is_gcc():\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                                         ^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1001, in is_gcc\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler()))\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]                                           ^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 944, in cpp_compiler\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     return cpp_compiler_search(search)\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 971, in cpp_compiler_search\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824]     raise exc.InvalidCxxCompiler()\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824] InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0524 22:52:46.322000 47072 torch\\_dynamo\\convert_frame.py:824] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 10764.02734375\n",
      "Epoch: 2 \tLoss: 1356.51025390625\n",
      "Epoch: 3 \tLoss: 504.9664306640625\n",
      "Epoch: 4 \tLoss: 199.11312866210938\n",
      "Epoch: 5 \tLoss: 191.20828247070312\n",
      "Test MSE: 162.2450408935547\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "EPOCHS=5\n",
    "\n",
    "# Create training and test sets\n",
    "features, target = make_regression(n_features=10, n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(target_train).float().view(-1,1)\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(target_test).float().view(-1,1)\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Initialize neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = RMSprop(network.parameters())\n",
    "\n",
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "network = torch.compile(network)\n",
    "\n",
    "# Train neural network\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n",
    "\n",
    "# Evaluate neural network\n",
    "with torch.no_grad():\n",
    "    output = network(x_test)\n",
    "    test_loss = float(criterion(output, y_test))\n",
    "    print(\"Test MSE:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b335370a-5664-4824-8d6b-61fceb4607ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824] WON'T CONVERT forward C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_53068\\1055360726.py line 39 \n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824] due to: \n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 971, in compile_subgraph\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1241, in call_user_compiler\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1222, in call_user_compiler\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py\", line 1729, in __call__\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1208, in codegen\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     self.scheduler.codegen()\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 2339, in codegen\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     self.get_backend(device).codegen_nodes(node.get_nodes())  # type: ignore[possibly-undefined]\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3623, in codegen_nodes\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3661, in finalize_kernel\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     new_kernel.codegen_loops(code, ws)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3458, in codegen_loops\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     self.codegen_loops_impl(self.loop_nest, code, worksharing)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1832, in codegen_loops_impl\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     gen_loops(loop_nest.root)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1804, in gen_loops\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     gen_loop(loop, in_reduction)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1817, in gen_loop\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     loop_lines = loop.lines()\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3922, in lines\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     elif not self.reduction_var_map and codecache.is_gcc():\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                                         ^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1001, in is_gcc\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler()))\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                                           ^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 944, in cpp_compiler\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return cpp_compiler_search(search)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 971, in cpp_compiler_search\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     raise exc.InvalidCxxCompiler()\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824] InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824] Traceback (most recent call last):\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 786, in _convert_frame\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     result = inner_convert(\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]              ^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 400, in _convert_frame_assert\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return _compile(\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 676, in _compile\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 535, in compile_inner\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     out_code = transform_code_object(code, transform)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     transformations(instructions, code_options)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 165, in _fn\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 500, in transform\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     tracer.run()\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2149, in run\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     super().run()\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 810, in run\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     and self.step()\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 773, in step\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     getattr(self, inst.opname)(inst)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2268, in RETURN_VALUE\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     self.output.compile_subgraph(\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 971, in compile_subgraph\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1168, in compile_and_call_fx_graph\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1241, in call_user_compiler\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1222, in call_user_compiler\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 117, in debug_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\__init__.py\", line 1729, in __call__\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1330, in compile_fx\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return aot_autograd(\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 58, in compiler_fn\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 903, in aot_module_simplified\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = create_aot_dispatcher_function(\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 628, in create_aot_dispatcher_function\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 443, in aot_wrapper_dedupe\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 648, in aot_wrapper_synthetic_base\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 352, in aot_dispatch_autograd\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1257, in fw_compiler_base\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return inner_compile(\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 83, in debug_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\debug.py\", line 304, in inner\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return fn(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\contextlib.py\", line 81, in inner\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return func(*args, **kwds)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 438, in compile_fx_inner\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_graph = fx_codegen_and_compile(\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 714, in fx_codegen_and_compile\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     compiled_fn = graph.compile_to_fn()\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1307, in compile_to_fn\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return self.compile_to_module().call\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1250, in compile_to_module\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                                                              ^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1208, in codegen\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     self.scheduler.codegen()\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_dynamo\\utils.py\", line 262, in time_wrapper\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     r = func(*args, **kwargs)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 2339, in codegen\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     self.get_backend(device).codegen_nodes(node.get_nodes())  # type: ignore[possibly-undefined]\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3623, in codegen_nodes\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     kernel_group.finalize_kernel(cpp_kernel_proxy, nodes)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3661, in finalize_kernel\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     new_kernel.codegen_loops(code, ws)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3458, in codegen_loops\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     self.codegen_loops_impl(self.loop_nest, code, worksharing)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1832, in codegen_loops_impl\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     gen_loops(loop_nest.root)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1804, in gen_loops\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     gen_loop(loop, in_reduction)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 1817, in gen_loop\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     loop_lines = loop.lines()\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                  ^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3922, in lines\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     elif not self.reduction_var_map and codecache.is_gcc():\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                                         ^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1001, in is_gcc\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return bool(re.search(r\"(gcc|g\\+\\+)\", cpp_compiler()))\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]                                           ^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 944, in cpp_compiler\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     return cpp_compiler_search(search)\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]   File \"C:\\Users\\User\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 971, in cpp_compiler_search\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824]     raise exc.InvalidCxxCompiler()\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824] InvalidCxxCompiler: No working C++ compiler found in torch._inductor.config.cpp.cxx: (None, 'g++')\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824] \n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0524 23:02:00.616000 47072 torch\\_dynamo\\convert_frame.py:824] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 0.19006994366645813\n",
      "Epoch: 2 \tLoss: 0.14092367887496948\n",
      "Epoch: 3 \tLoss: 0.03935524821281433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training and test sets\n",
    "features, target = make_classification(n_classes=2, n_features=10,\n",
    "    n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(target_train).float().view(-1, 1)\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(target_test).float().view(-1, 1)\n",
    "\n",
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16,16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "\n",
    "# Initialize neural network\n",
    "network = SimpleNeuralNet()\n",
    "\n",
    "# Define loss function, optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = RMSprop(network.parameters())\n",
    "\n",
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "network = torch.compile(network)\n",
    "\n",
    "# Train neural network\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n",
    "\n",
    "# Evaluate neural network\n",
    "with torch.no_grad():\n",
    "    predicted_class = network.forward(x_train).round()\n",
    "\n",
    "predicted_class[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acfe5a19-f0c1-4721-b6e4-9f49f0b7d0dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_olivetti_faces\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Olivetti faces\n",
    "data = fetch_olivetti_faces()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class OlivettiClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OlivettiClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 40)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4096)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = OlivettiClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afcbacd4-6e98-425b-9211-9cbcbdbfcec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.0\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.0\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.0\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.0\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.0\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.0\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.0\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.0\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.0\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.0\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\.pyenv\\pyenv-win\\versions\\3.11.0\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df88f4a-ee2a-40d9-9782-63ec5f2ecc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 3.8369\n",
      "Epoch 2, loss: 3.7222\n",
      "Epoch 3, loss: 3.6645\n",
      "Epoch 4, loss: 3.6370\n",
      "Epoch 5, loss: 3.5912\n",
      "Epoch 6, loss: 3.5130\n",
      "Epoch 7, loss: 3.3975\n",
      "Epoch 8, loss: 3.2437\n",
      "Epoch 9, loss: 3.0615\n",
      "Epoch 10, loss: 2.8738\n",
      "Epoch 11, loss: 2.6820\n",
      "Epoch 12, loss: 2.4797\n",
      "Epoch 13, loss: 2.3010\n",
      "Epoch 14, loss: 2.0512\n",
      "Epoch 15, loss: 1.8503\n",
      "Epoch 16, loss: 1.7033\n",
      "Epoch 17, loss: 1.6107\n",
      "Epoch 18, loss: 1.4827\n",
      "Epoch 19, loss: 1.3628\n",
      "Epoch 20, loss: 1.1775\n",
      "Epoch 21, loss: 1.0541\n",
      "Epoch 22, loss: 1.0416\n",
      "Epoch 23, loss: 1.0205\n",
      "Epoch 24, loss: 0.9125\n",
      "Epoch 25, loss: 0.8106\n",
      "Epoch 26, loss: 0.7470\n",
      "Epoch 27, loss: 0.6744\n",
      "Epoch 28, loss: 0.6086\n",
      "Epoch 29, loss: 0.5694\n",
      "Epoch 30, loss: 0.5510\n",
      "Epoch 31, loss: 0.5136\n",
      "Epoch 32, loss: 0.4788\n",
      "Epoch 33, loss: 0.4156\n",
      "Epoch 34, loss: 0.4106\n",
      "Epoch 35, loss: 0.4377\n",
      "Epoch 36, loss: 0.4007\n",
      "Epoch 37, loss: 0.3386\n",
      "Epoch 38, loss: 0.3166\n",
      "Epoch 39, loss: 0.3047\n",
      "Epoch 40, loss: 0.2872\n",
      "Epoch 41, loss: 0.2841\n",
      "Epoch 42, loss: 0.2510\n",
      "Epoch 43, loss: 0.2385\n",
      "Epoch 44, loss: 0.2202\n",
      "Epoch 45, loss: 0.2098\n",
      "Epoch 46, loss: 0.2068\n",
      "Epoch 47, loss: 0.2051\n",
      "Epoch 48, loss: 0.2176\n",
      "Epoch 49, loss: 0.1935\n",
      "Epoch 50, loss: 0.2008\n",
      "Epoch 51, loss: 0.1743\n",
      "Epoch 52, loss: 0.1617\n",
      "Epoch 53, loss: 0.1337\n",
      "Epoch 54, loss: 0.1176\n",
      "Epoch 55, loss: 0.1360\n",
      "Epoch 56, loss: 0.1351\n",
      "Epoch 57, loss: 0.1149\n",
      "Epoch 58, loss: 0.1181\n",
      "Epoch 59, loss: 0.1487\n",
      "Epoch 60, loss: 0.1208\n",
      "Epoch 61, loss: 0.1149\n",
      "Epoch 62, loss: 0.0741\n",
      "Epoch 63, loss: 0.0793\n",
      "Epoch 64, loss: 0.0664\n",
      "Epoch 65, loss: 0.0646\n",
      "Epoch 66, loss: 0.0567\n",
      "Epoch 67, loss: 0.0564\n",
      "Epoch 68, loss: 0.0534\n",
      "Epoch 69, loss: 0.0501\n",
      "Epoch 70, loss: 0.0510\n",
      "Epoch 71, loss: 0.0434\n",
      "Epoch 72, loss: 0.0423\n",
      "Epoch 73, loss: 0.0412\n",
      "Epoch 74, loss: 0.0380\n",
      "Epoch 75, loss: 0.0355\n",
      "Epoch 76, loss: 0.0342\n",
      "Epoch 77, loss: 0.0342\n",
      "Epoch 78, loss: 0.0350\n",
      "Epoch 79, loss: 0.0313\n",
      "Epoch 80, loss: 0.0288\n",
      "Epoch 81, loss: 0.0330\n",
      "Epoch 82, loss: 0.0274\n",
      "Epoch 83, loss: 0.0256\n",
      "Epoch 84, loss: 0.0247\n",
      "Epoch 85, loss: 0.0227\n",
      "Epoch 86, loss: 0.0216\n",
      "Epoch 87, loss: 0.0202\n",
      "Epoch 88, loss: 0.0192\n",
      "Epoch 89, loss: 0.0194\n",
      "Epoch 90, loss: 0.0188\n",
      "Epoch 91, loss: 0.0196\n",
      "Epoch 92, loss: 0.0211\n",
      "Epoch 93, loss: 0.0198\n",
      "Epoch 94, loss: 0.0235\n",
      "Epoch 95, loss: 0.0189\n",
      "Epoch 96, loss: 0.0191\n",
      "Epoch 97, loss: 0.0168\n",
      "Epoch 98, loss: 0.0175\n",
      "Epoch 99, loss: 0.0150\n",
      "Epoch 100, loss: 0.0136\n",
      "Accuracy on test set: 88.75%\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Olivetti faces\n",
    "data = fetch_olivetti_faces()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class OlivettiClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OlivettiClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 40)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4096)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = OlivettiClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05c3fb7b-55b3-4e83-9e66-1612e93a5a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec0d614-0df3-45d5-bb5c-4c9150494b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 1, loss: 3.8756\n",
      "Epoch 2, loss: 3.7249\n",
      "Epoch 3, loss: 3.6781\n",
      "Epoch 4, loss: 3.6425\n",
      "Epoch 5, loss: 3.6032\n",
      "Epoch 6, loss: 3.5405\n",
      "Epoch 7, loss: 3.4496\n",
      "Epoch 8, loss: 3.3233\n",
      "Epoch 9, loss: 3.1424\n",
      "Epoch 10, loss: 2.9428\n",
      "Epoch 11, loss: 2.7138\n",
      "Epoch 12, loss: 2.5274\n",
      "Epoch 13, loss: 2.2895\n",
      "Epoch 14, loss: 2.0998\n",
      "Epoch 15, loss: 1.9362\n",
      "Epoch 16, loss: 1.7904\n",
      "Epoch 17, loss: 1.6218\n",
      "Epoch 18, loss: 1.4379\n",
      "Epoch 19, loss: 1.3467\n",
      "Epoch 20, loss: 1.2460\n",
      "Epoch 21, loss: 1.1367\n",
      "Epoch 22, loss: 1.0276\n",
      "Epoch 23, loss: 1.0904\n",
      "Epoch 24, loss: 0.9230\n",
      "Epoch 25, loss: 0.9247\n",
      "Epoch 26, loss: 0.8361\n",
      "Epoch 27, loss: 0.8059\n",
      "Epoch 28, loss: 0.7029\n",
      "Epoch 29, loss: 0.6568\n",
      "Epoch 30, loss: 0.6538\n",
      "Epoch 31, loss: 0.5761\n",
      "Epoch 32, loss: 0.5245\n",
      "Epoch 33, loss: 0.5389\n",
      "Epoch 34, loss: 0.4835\n",
      "Epoch 35, loss: 0.4591\n",
      "Epoch 36, loss: 0.4719\n",
      "Epoch 37, loss: 0.4603\n",
      "Epoch 38, loss: 0.3776\n",
      "Epoch 39, loss: 0.3215\n",
      "Epoch 40, loss: 0.3493\n",
      "Epoch 41, loss: 0.3231\n",
      "Epoch 42, loss: 0.3431\n",
      "Epoch 43, loss: 0.3442\n",
      "Epoch 44, loss: 0.2749\n",
      "Epoch 45, loss: 0.2421\n",
      "Epoch 46, loss: 0.2361\n",
      "Epoch 47, loss: 0.2034\n",
      "Epoch 48, loss: 0.1804\n",
      "Epoch 49, loss: 0.1693\n",
      "Epoch 50, loss: 0.1753\n",
      "Epoch 51, loss: 0.1493\n",
      "Epoch 52, loss: 0.1440\n",
      "Epoch 53, loss: 0.1301\n",
      "Epoch 54, loss: 0.1247\n",
      "Epoch 55, loss: 0.1186\n",
      "Epoch 56, loss: 0.1061\n",
      "Epoch 57, loss: 0.1101\n",
      "Epoch 58, loss: 0.1595\n",
      "Epoch 59, loss: 0.1273\n",
      "Epoch 60, loss: 0.0975\n",
      "Epoch 61, loss: 0.0862\n",
      "Epoch 62, loss: 0.0742\n",
      "Epoch 63, loss: 0.0647\n",
      "Epoch 64, loss: 0.0560\n",
      "Epoch 65, loss: 0.0540\n",
      "Epoch 66, loss: 0.0549\n",
      "Epoch 67, loss: 0.0583\n",
      "Epoch 68, loss: 0.0608\n",
      "Epoch 69, loss: 0.0564\n",
      "Epoch 70, loss: 0.0524\n",
      "Epoch 71, loss: 0.0454\n",
      "Epoch 72, loss: 0.0527\n",
      "Epoch 73, loss: 0.0501\n",
      "Epoch 74, loss: 0.0399\n",
      "Epoch 75, loss: 0.0455\n",
      "Epoch 76, loss: 0.0437\n",
      "Epoch 77, loss: 0.0363\n",
      "Epoch 78, loss: 0.0395\n",
      "Epoch 79, loss: 0.0313\n",
      "Epoch 80, loss: 0.0304\n",
      "Epoch 81, loss: 0.0279\n",
      "Epoch 82, loss: 0.0250\n",
      "Epoch 83, loss: 0.0241\n",
      "Epoch 84, loss: 0.0233\n",
      "Epoch 85, loss: 0.0215\n",
      "Epoch 86, loss: 0.0207\n",
      "Epoch 87, loss: 0.0190\n",
      "Epoch 88, loss: 0.0187\n",
      "Epoch 89, loss: 0.0179\n",
      "Epoch 90, loss: 0.0174\n",
      "Epoch 91, loss: 0.0175\n",
      "Epoch 92, loss: 0.0164\n",
      "Epoch 93, loss: 0.0155\n",
      "Epoch 94, loss: 0.0153\n",
      "Epoch 95, loss: 0.0152\n",
      "Epoch 96, loss: 0.0140\n",
      "Epoch 97, loss: 0.0135\n",
      "Epoch 98, loss: 0.0133\n",
      "Epoch 99, loss: 0.0141\n",
      "Epoch 100, loss: 0.0129\n",
      "Accuracy on test set: 87.50%\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Olivetti faces\n",
    "data = fetch_olivetti_faces()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class OlivettiClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OlivettiClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 40)\n",
    "        self.to(device)  # GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4096)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = OlivettiClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9005c277-aca6-40c1-9c1f-a07fb99f693c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 1, loss: 3.8291\n",
      "Epoch 2, loss: 3.7146\n",
      "Epoch 3, loss: 3.6745\n",
      "Epoch 4, loss: 3.6247\n",
      "Epoch 5, loss: 3.5741\n",
      "Epoch 6, loss: 3.4864\n",
      "Epoch 7, loss: 3.3434\n",
      "Epoch 8, loss: 3.1651\n",
      "Epoch 9, loss: 2.9084\n",
      "Epoch 10, loss: 2.6533\n",
      "Epoch 11, loss: 2.3728\n",
      "Epoch 12, loss: 2.1873\n",
      "Epoch 13, loss: 1.9836\n",
      "Epoch 14, loss: 1.8108\n",
      "Epoch 15, loss: 1.5943\n",
      "Epoch 16, loss: 1.4769\n",
      "Epoch 17, loss: 1.4065\n",
      "Epoch 18, loss: 1.3043\n",
      "Epoch 19, loss: 1.1584\n",
      "Epoch 20, loss: 1.0747\n",
      "Epoch 21, loss: 0.9903\n",
      "Epoch 22, loss: 0.9026\n",
      "Epoch 23, loss: 0.8351\n",
      "Epoch 24, loss: 0.8137\n",
      "Epoch 25, loss: 0.7287\n",
      "Epoch 26, loss: 0.6617\n",
      "Epoch 27, loss: 0.5830\n",
      "Epoch 28, loss: 0.5680\n",
      "Epoch 29, loss: 0.5259\n",
      "Epoch 30, loss: 0.4805\n",
      "Epoch 31, loss: 0.4274\n",
      "Epoch 32, loss: 0.4121\n",
      "Epoch 33, loss: 0.3805\n",
      "Epoch 34, loss: 0.3525\n",
      "Epoch 35, loss: 0.3121\n",
      "Epoch 36, loss: 0.2797\n",
      "Epoch 37, loss: 0.2797\n",
      "Epoch 38, loss: 0.2608\n",
      "Epoch 39, loss: 0.2406\n",
      "Epoch 40, loss: 0.2286\n",
      "Epoch 41, loss: 0.2064\n",
      "Epoch 42, loss: 0.2043\n",
      "Epoch 43, loss: 0.1979\n",
      "Epoch 44, loss: 0.1988\n",
      "Epoch 45, loss: 0.2229\n",
      "Epoch 46, loss: 0.2127\n",
      "Epoch 47, loss: 0.2303\n",
      "Epoch 48, loss: 0.2143\n",
      "Epoch 49, loss: 0.1789\n",
      "Epoch 50, loss: 0.1303\n",
      "Epoch 51, loss: 0.1039\n",
      "Epoch 52, loss: 0.1128\n",
      "Epoch 53, loss: 0.0936\n",
      "Epoch 54, loss: 0.0846\n",
      "Epoch 55, loss: 0.0829\n",
      "Epoch 56, loss: 0.0756\n",
      "Epoch 57, loss: 0.0645\n",
      "Epoch 58, loss: 0.0587\n",
      "Epoch 59, loss: 0.0602\n",
      "Epoch 60, loss: 0.0656\n",
      "Epoch 61, loss: 0.0576\n",
      "Epoch 62, loss: 0.0544\n",
      "Epoch 63, loss: 0.0472\n",
      "Epoch 64, loss: 0.0375\n",
      "Epoch 65, loss: 0.0353\n",
      "Epoch 66, loss: 0.0356\n",
      "Epoch 67, loss: 0.0367\n",
      "Epoch 68, loss: 0.0390\n",
      "Epoch 69, loss: 0.0372\n",
      "Epoch 70, loss: 0.0299\n",
      "Epoch 71, loss: 0.0302\n",
      "Epoch 72, loss: 0.0313\n",
      "Epoch 73, loss: 0.0259\n",
      "Epoch 74, loss: 0.0278\n",
      "Epoch 75, loss: 0.0251\n",
      "Epoch 76, loss: 0.0230\n",
      "Epoch 77, loss: 0.0221\n",
      "Epoch 78, loss: 0.0215\n",
      "Epoch 79, loss: 0.0199\n",
      "Epoch 80, loss: 0.0193\n",
      "Epoch 81, loss: 0.0181\n",
      "Epoch 82, loss: 0.0181\n",
      "Epoch 83, loss: 0.0180\n",
      "Epoch 84, loss: 0.0199\n",
      "Epoch 85, loss: 0.0173\n",
      "Epoch 86, loss: 0.0169\n",
      "Epoch 87, loss: 0.0168\n",
      "Epoch 88, loss: 0.0154\n",
      "Epoch 89, loss: 0.0139\n",
      "Epoch 90, loss: 0.0129\n",
      "Epoch 91, loss: 0.0123\n",
      "Epoch 92, loss: 0.0124\n",
      "Epoch 93, loss: 0.0123\n",
      "Epoch 94, loss: 0.0117\n",
      "Epoch 95, loss: 0.0119\n",
      "Epoch 96, loss: 0.0114\n",
      "Epoch 97, loss: 0.0105\n",
      "Epoch 98, loss: 0.0102\n",
      "Epoch 99, loss: 0.0099\n",
      "Epoch 100, loss: 0.0097\n",
      "Accuracy on test set: 90.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Olivetti faces\n",
    "data = fetch_olivetti_faces()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class OlivettiClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OlivettiClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 40)\n",
    "        self.to(device)  # GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4096)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = OlivettiClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \n",
    "epochs = 100\n",
    "losses = []  # epoch\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    print(f'Epoch {epoch+1}, loss: {epoch_loss:.4f}')\n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# \n",
    "plt.figure()\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eea396a-db52-4a33-989f-c66c1efa9860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 1, loss: 3.8474\n",
      "Epoch 2, loss: 3.7161\n",
      "Epoch 3, loss: 3.6910\n",
      "Epoch 4, loss: 3.6658\n",
      "Epoch 5, loss: 3.6452\n",
      "Epoch 6, loss: 3.6010\n",
      "Epoch 7, loss: 3.5581\n",
      "Epoch 8, loss: 3.4535\n",
      "Epoch 9, loss: 3.3282\n",
      "Epoch 10, loss: 3.1174\n",
      "Epoch 11, loss: 2.8577\n",
      "Epoch 12, loss: 2.6135\n",
      "Epoch 13, loss: 2.3399\n",
      "Epoch 14, loss: 2.1123\n",
      "Epoch 15, loss: 1.8694\n",
      "Epoch 16, loss: 1.6932\n",
      "Epoch 17, loss: 1.5933\n",
      "Epoch 18, loss: 1.4538\n",
      "Epoch 19, loss: 1.3306\n",
      "Epoch 20, loss: 1.1632\n",
      "Epoch 21, loss: 1.0710\n",
      "Epoch 22, loss: 1.0120\n",
      "Epoch 23, loss: 0.9465\n",
      "Epoch 24, loss: 0.8600\n",
      "Epoch 25, loss: 0.7997\n",
      "Epoch 26, loss: 0.7426\n",
      "Epoch 27, loss: 0.6798\n",
      "Epoch 28, loss: 0.6371\n",
      "Epoch 29, loss: 0.5655\n",
      "Epoch 30, loss: 0.5266\n",
      "Epoch 31, loss: 0.5114\n",
      "Epoch 32, loss: 0.4893\n",
      "Epoch 33, loss: 0.4776\n",
      "Epoch 34, loss: 0.4535\n",
      "Epoch 35, loss: 0.4288\n",
      "Epoch 36, loss: 0.3537\n",
      "Epoch 37, loss: 0.3450\n",
      "Epoch 38, loss: 0.3444\n",
      "Epoch 39, loss: 0.3836\n",
      "Epoch 40, loss: 0.3072\n",
      "Epoch 41, loss: 0.2411\n",
      "Epoch 42, loss: 0.2957\n",
      "Epoch 43, loss: 0.3203\n",
      "Epoch 44, loss: 0.3100\n",
      "Epoch 45, loss: 0.2142\n",
      "Epoch 46, loss: 0.2118\n",
      "Epoch 47, loss: 0.2001\n",
      "Epoch 48, loss: 0.1870\n",
      "Epoch 49, loss: 0.1597\n",
      "Epoch 50, loss: 0.1562\n",
      "Epoch 51, loss: 0.1313\n",
      "Epoch 52, loss: 0.1231\n",
      "Epoch 53, loss: 0.1222\n",
      "Epoch 54, loss: 0.1081\n",
      "Epoch 55, loss: 0.1081\n",
      "Epoch 56, loss: 0.0975\n",
      "Epoch 57, loss: 0.1084\n",
      "Epoch 58, loss: 0.0944\n",
      "Epoch 59, loss: 0.0903\n",
      "Epoch 60, loss: 0.0858\n",
      "Epoch 61, loss: 0.0855\n",
      "Epoch 62, loss: 0.0851\n",
      "Epoch 63, loss: 0.0803\n",
      "Epoch 64, loss: 0.0830\n",
      "Epoch 65, loss: 0.0766\n",
      "Epoch 66, loss: 0.0722\n",
      "Epoch 67, loss: 0.0855\n",
      "Epoch 68, loss: 0.0893\n",
      "Epoch 69, loss: 0.0793\n",
      "Epoch 70, loss: 0.0642\n",
      "Epoch 71, loss: 0.0459\n",
      "Epoch 72, loss: 0.0399\n",
      "Epoch 73, loss: 0.0373\n",
      "Epoch 74, loss: 0.0369\n",
      "Epoch 75, loss: 0.0394\n",
      "Epoch 76, loss: 0.0331\n",
      "Epoch 77, loss: 0.0316\n",
      "Epoch 78, loss: 0.0321\n",
      "Epoch 79, loss: 0.0342\n",
      "Epoch 80, loss: 0.0333\n",
      "Epoch 81, loss: 0.0295\n",
      "Epoch 82, loss: 0.0273\n",
      "Epoch 83, loss: 0.0266\n",
      "Epoch 84, loss: 0.0236\n",
      "Epoch 85, loss: 0.0216\n",
      "Epoch 86, loss: 0.0230\n",
      "Epoch 87, loss: 0.0228\n",
      "Epoch 88, loss: 0.0208\n",
      "Epoch 89, loss: 0.0206\n",
      "Epoch 90, loss: 0.0200\n",
      "Epoch 91, loss: 0.0179\n",
      "Epoch 92, loss: 0.0180\n",
      "Epoch 93, loss: 0.0191\n",
      "Epoch 94, loss: 0.0187\n",
      "Epoch 95, loss: 0.0162\n",
      "Epoch 96, loss: 0.0157\n",
      "Epoch 97, loss: 0.0170\n",
      "Epoch 98, loss: 0.0152\n",
      "Epoch 99, loss: 0.0141\n",
      "Epoch 100, loss: 0.0144\n",
      "Epoch 101, loss: 0.0135\n",
      "Epoch 102, loss: 0.0128\n",
      "Epoch 103, loss: 0.0132\n",
      "Epoch 104, loss: 0.0133\n",
      "Epoch 105, loss: 0.0131\n",
      "Epoch 106, loss: 0.0126\n",
      "Epoch 107, loss: 0.0117\n",
      "Epoch 108, loss: 0.0118\n",
      "Epoch 109, loss: 0.0108\n",
      "Epoch 110, loss: 0.0105\n",
      "Epoch 111, loss: 0.0102\n",
      "Epoch 112, loss: 0.0095\n",
      "Epoch 113, loss: 0.0096\n",
      "Epoch 114, loss: 0.0099\n",
      "Epoch 115, loss: 0.0094\n",
      "Epoch 116, loss: 0.0091\n",
      "Epoch 117, loss: 0.0091\n",
      "Epoch 118, loss: 0.0088\n",
      "Epoch 119, loss: 0.0084\n",
      "Epoch 120, loss: 0.0081\n",
      "Epoch 121, loss: 0.0080\n",
      "Epoch 122, loss: 0.0084\n",
      "Epoch 123, loss: 0.0075\n",
      "Epoch 124, loss: 0.0072\n",
      "Epoch 125, loss: 0.0072\n",
      "Epoch 126, loss: 0.0071\n",
      "Epoch 127, loss: 0.0070\n",
      "Epoch 128, loss: 0.0068\n",
      "Epoch 129, loss: 0.0066\n",
      "Epoch 130, loss: 0.0067\n",
      "Epoch 131, loss: 0.0066\n",
      "Epoch 132, loss: 0.0068\n",
      "Epoch 133, loss: 0.0066\n",
      "Epoch 134, loss: 0.0058\n",
      "Epoch 135, loss: 0.0059\n",
      "Epoch 136, loss: 0.0056\n",
      "Epoch 137, loss: 0.0056\n",
      "Epoch 138, loss: 0.0056\n",
      "Epoch 139, loss: 0.0055\n",
      "Epoch 140, loss: 0.0055\n",
      "Epoch 141, loss: 0.0054\n",
      "Epoch 142, loss: 0.0052\n",
      "Epoch 143, loss: 0.0052\n",
      "Epoch 144, loss: 0.0050\n",
      "Epoch 145, loss: 0.0049\n",
      "Epoch 146, loss: 0.0048\n",
      "Epoch 147, loss: 0.0048\n",
      "Epoch 148, loss: 0.0047\n",
      "Epoch 149, loss: 0.0045\n",
      "Epoch 150, loss: 0.0044\n",
      "Epoch 151, loss: 0.0043\n",
      "Epoch 152, loss: 0.0044\n",
      "Epoch 153, loss: 0.0043\n",
      "Epoch 154, loss: 0.0042\n",
      "Epoch 155, loss: 0.0042\n",
      "Epoch 156, loss: 0.0040\n",
      "Epoch 157, loss: 0.0039\n",
      "Epoch 158, loss: 0.0039\n",
      "Epoch 159, loss: 0.0039\n",
      "Epoch 160, loss: 0.0038\n",
      "Epoch 161, loss: 0.0037\n",
      "Epoch 162, loss: 0.0036\n",
      "Epoch 163, loss: 0.0036\n",
      "Epoch 164, loss: 0.0035\n",
      "Epoch 165, loss: 0.0035\n",
      "Epoch 166, loss: 0.0034\n",
      "Epoch 167, loss: 0.0033\n",
      "Epoch 168, loss: 0.0033\n",
      "Epoch 169, loss: 0.0033\n",
      "Epoch 170, loss: 0.0032\n",
      "Epoch 171, loss: 0.0031\n",
      "Epoch 172, loss: 0.0032\n",
      "Epoch 173, loss: 0.0031\n",
      "Epoch 174, loss: 0.0031\n",
      "Epoch 175, loss: 0.0030\n",
      "Epoch 176, loss: 0.0029\n",
      "Epoch 177, loss: 0.0029\n",
      "Epoch 178, loss: 0.0028\n",
      "Epoch 179, loss: 0.0028\n",
      "Epoch 180, loss: 0.0028\n",
      "Epoch 181, loss: 0.0027\n",
      "Epoch 182, loss: 0.0027\n",
      "Epoch 183, loss: 0.0027\n",
      "Epoch 184, loss: 0.0026\n",
      "Epoch 185, loss: 0.0026\n",
      "Epoch 186, loss: 0.0025\n",
      "Epoch 187, loss: 0.0025\n",
      "Epoch 188, loss: 0.0025\n",
      "Epoch 189, loss: 0.0025\n",
      "Epoch 190, loss: 0.0024\n",
      "Epoch 191, loss: 0.0024\n",
      "Epoch 192, loss: 0.0023\n",
      "Epoch 193, loss: 0.0023\n",
      "Epoch 194, loss: 0.0023\n",
      "Epoch 195, loss: 0.0023\n",
      "Epoch 196, loss: 0.0022\n",
      "Epoch 197, loss: 0.0022\n",
      "Epoch 198, loss: 0.0021\n",
      "Epoch 199, loss: 0.0021\n",
      "Epoch 200, loss: 0.0021\n",
      "Accuracy on test set: 90.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Olivetti faces\n",
    "data = fetch_olivetti_faces()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class OlivettiClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OlivettiClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 40)\n",
    "        self.to(device)  # GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4096)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = OlivettiClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \n",
    "epochs = 200\n",
    "losses = []  # epoch\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    print(f'Epoch {epoch+1}, loss: {epoch_loss:.4f}')\n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# \n",
    "plt.figure()\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a748fe3-4ccc-430c-a23c-763457cc2cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 1, loss: 3.7287\n",
      "Epoch 2, loss: 3.6999\n",
      "Epoch 3, loss: 3.6876\n",
      "Epoch 4, loss: 3.6847\n",
      "Epoch 5, loss: 3.6757\n",
      "Epoch 6, loss: 3.6708\n",
      "Epoch 7, loss: 3.6651\n",
      "Epoch 8, loss: 3.6512\n",
      "Epoch 9, loss: 3.6193\n",
      "Epoch 10, loss: 3.5139\n",
      "Epoch 11, loss: 3.3296\n",
      "Epoch 12, loss: 3.1438\n",
      "Epoch 13, loss: 2.8810\n",
      "Epoch 14, loss: 2.7372\n",
      "Epoch 15, loss: 2.7452\n",
      "Epoch 16, loss: 2.4108\n",
      "Epoch 17, loss: 2.1333\n",
      "Epoch 18, loss: 2.0301\n",
      "Epoch 19, loss: 1.8599\n",
      "Epoch 20, loss: 1.8085\n",
      "Epoch 21, loss: 1.6653\n",
      "Epoch 22, loss: 1.6587\n",
      "Epoch 23, loss: 1.4916\n",
      "Epoch 24, loss: 1.3501\n",
      "Epoch 25, loss: 1.3871\n",
      "Epoch 26, loss: 1.3614\n",
      "Epoch 27, loss: 1.4933\n",
      "Epoch 28, loss: 1.4647\n",
      "Epoch 29, loss: 1.4105\n",
      "Epoch 30, loss: 1.1834\n",
      "Epoch 31, loss: 1.2021\n",
      "Epoch 32, loss: 1.1614\n",
      "Epoch 33, loss: 0.9744\n",
      "Epoch 34, loss: 0.9486\n",
      "Epoch 35, loss: 0.8919\n",
      "Epoch 36, loss: 0.9646\n",
      "Epoch 37, loss: 1.2546\n",
      "Epoch 38, loss: 0.9768\n",
      "Epoch 39, loss: 0.9577\n",
      "Epoch 40, loss: 0.8742\n",
      "Epoch 41, loss: 0.8370\n",
      "Epoch 42, loss: 0.7920\n",
      "Epoch 43, loss: 0.7184\n",
      "Epoch 44, loss: 0.7098\n",
      "Epoch 45, loss: 0.7405\n",
      "Epoch 46, loss: 0.8477\n",
      "Epoch 47, loss: 0.8514\n",
      "Epoch 48, loss: 0.9103\n",
      "Epoch 49, loss: 0.6481\n",
      "Epoch 50, loss: 0.5541\n",
      "Epoch 51, loss: 0.5783\n",
      "Epoch 52, loss: 0.4974\n",
      "Epoch 53, loss: 0.5862\n",
      "Epoch 54, loss: 0.4943\n",
      "Epoch 55, loss: 0.5782\n",
      "Epoch 56, loss: 0.7276\n",
      "Epoch 57, loss: 0.5434\n",
      "Epoch 58, loss: 0.5192\n",
      "Epoch 59, loss: 0.6743\n",
      "Epoch 60, loss: 0.8431\n",
      "Epoch 61, loss: 0.6506\n",
      "Epoch 62, loss: 0.5132\n",
      "Epoch 63, loss: 0.4389\n",
      "Epoch 64, loss: 0.3791\n",
      "Epoch 65, loss: 0.2870\n",
      "Epoch 66, loss: 0.2642\n",
      "Epoch 67, loss: 0.2881\n",
      "Epoch 68, loss: 0.2872\n",
      "Epoch 69, loss: 0.2739\n",
      "Epoch 70, loss: 0.2528\n",
      "Epoch 71, loss: 0.2568\n",
      "Epoch 72, loss: 0.2013\n",
      "Epoch 73, loss: 0.1948\n",
      "Epoch 74, loss: 0.1966\n",
      "Epoch 75, loss: 0.1698\n",
      "Epoch 76, loss: 0.2190\n",
      "Epoch 77, loss: 0.2272\n",
      "Epoch 78, loss: 0.4094\n",
      "Epoch 79, loss: 0.3130\n",
      "Epoch 80, loss: 0.3122\n",
      "Epoch 81, loss: 0.2804\n",
      "Epoch 82, loss: 0.2511\n",
      "Epoch 83, loss: 0.2739\n",
      "Epoch 84, loss: 0.2140\n",
      "Epoch 85, loss: 0.1608\n",
      "Epoch 86, loss: 0.1331\n",
      "Epoch 87, loss: 0.1668\n",
      "Epoch 88, loss: 0.0988\n",
      "Epoch 89, loss: 0.1086\n",
      "Epoch 90, loss: 0.0883\n",
      "Epoch 91, loss: 0.0852\n",
      "Epoch 92, loss: 0.1023\n",
      "Epoch 93, loss: 0.1861\n",
      "Epoch 94, loss: 0.2174\n",
      "Epoch 95, loss: 0.4873\n",
      "Epoch 96, loss: 2.1273\n",
      "Epoch 97, loss: 0.9755\n",
      "Epoch 98, loss: 0.5560\n",
      "Epoch 99, loss: 0.3067\n",
      "Epoch 100, loss: 0.2188\n",
      "Epoch 101, loss: 0.1636\n",
      "Epoch 102, loss: 0.1564\n",
      "Epoch 103, loss: 0.1803\n",
      "Epoch 104, loss: 0.0941\n",
      "Epoch 105, loss: 0.0874\n",
      "Epoch 106, loss: 0.0782\n",
      "Epoch 107, loss: 0.0592\n",
      "Epoch 108, loss: 0.0657\n",
      "Epoch 109, loss: 0.0680\n",
      "Epoch 110, loss: 0.0388\n",
      "Epoch 111, loss: 0.0410\n",
      "Epoch 112, loss: 0.0348\n",
      "Epoch 113, loss: 0.0412\n",
      "Epoch 114, loss: 0.0393\n",
      "Epoch 115, loss: 0.0356\n",
      "Epoch 116, loss: 0.0408\n",
      "Epoch 117, loss: 0.0640\n",
      "Epoch 118, loss: 0.0728\n",
      "Epoch 119, loss: 0.0739\n",
      "Epoch 120, loss: 0.0524\n",
      "Epoch 121, loss: 0.0522\n",
      "Epoch 122, loss: 0.0459\n",
      "Epoch 123, loss: 0.0377\n",
      "Epoch 124, loss: 0.0297\n",
      "Epoch 125, loss: 0.0265\n",
      "Epoch 126, loss: 0.0286\n",
      "Epoch 127, loss: 0.0209\n",
      "Epoch 128, loss: 0.0167\n",
      "Epoch 129, loss: 0.0126\n",
      "Epoch 130, loss: 0.0125\n",
      "Epoch 131, loss: 0.0121\n",
      "Epoch 132, loss: 0.0096\n",
      "Epoch 133, loss: 0.0079\n",
      "Epoch 134, loss: 0.0071\n",
      "Epoch 135, loss: 0.0067\n",
      "Epoch 136, loss: 0.0065\n",
      "Epoch 137, loss: 0.0067\n",
      "Epoch 138, loss: 0.0060\n",
      "Epoch 139, loss: 0.0059\n",
      "Epoch 140, loss: 0.0057\n",
      "Epoch 141, loss: 0.0055\n",
      "Epoch 142, loss: 0.0057\n",
      "Epoch 143, loss: 0.0052\n",
      "Epoch 144, loss: 0.0050\n",
      "Epoch 145, loss: 0.0047\n",
      "Epoch 146, loss: 0.0047\n",
      "Epoch 147, loss: 0.0047\n",
      "Epoch 148, loss: 0.0045\n",
      "Epoch 149, loss: 0.0043\n",
      "Epoch 150, loss: 0.0042\n",
      "Epoch 151, loss: 0.0041\n",
      "Epoch 152, loss: 0.0040\n",
      "Epoch 153, loss: 0.0040\n",
      "Epoch 154, loss: 0.0040\n",
      "Epoch 155, loss: 0.0039\n",
      "Epoch 156, loss: 0.0038\n",
      "Epoch 157, loss: 0.0036\n",
      "Epoch 158, loss: 0.0037\n",
      "Epoch 159, loss: 0.0035\n",
      "Epoch 160, loss: 0.0035\n",
      "Epoch 161, loss: 0.0035\n",
      "Epoch 162, loss: 0.0034\n",
      "Epoch 163, loss: 0.0033\n",
      "Epoch 164, loss: 0.0033\n",
      "Epoch 165, loss: 0.0031\n",
      "Epoch 166, loss: 0.0030\n",
      "Epoch 167, loss: 0.0031\n",
      "Epoch 168, loss: 0.0030\n",
      "Epoch 169, loss: 0.0029\n",
      "Epoch 170, loss: 0.0028\n",
      "Epoch 171, loss: 0.0027\n",
      "Epoch 172, loss: 0.0027\n",
      "Epoch 173, loss: 0.0026\n",
      "Epoch 174, loss: 0.0027\n",
      "Epoch 175, loss: 0.0025\n",
      "Epoch 176, loss: 0.0025\n",
      "Epoch 177, loss: 0.0024\n",
      "Epoch 178, loss: 0.0024\n",
      "Epoch 179, loss: 0.0024\n",
      "Epoch 180, loss: 0.0023\n",
      "Epoch 181, loss: 0.0023\n",
      "Epoch 182, loss: 0.0022\n",
      "Epoch 183, loss: 0.0022\n",
      "Epoch 184, loss: 0.0022\n",
      "Epoch 185, loss: 0.0021\n",
      "Epoch 186, loss: 0.0021\n",
      "Epoch 187, loss: 0.0021\n",
      "Epoch 188, loss: 0.0020\n",
      "Epoch 189, loss: 0.0021\n",
      "Epoch 190, loss: 0.0020\n",
      "Epoch 191, loss: 0.0019\n",
      "Epoch 192, loss: 0.0019\n",
      "Epoch 193, loss: 0.0019\n",
      "Epoch 194, loss: 0.0018\n",
      "Epoch 195, loss: 0.0018\n",
      "Epoch 196, loss: 0.0018\n",
      "Epoch 197, loss: 0.0018\n",
      "Epoch 198, loss: 0.0018\n",
      "Epoch 199, loss: 0.0017\n",
      "Epoch 200, loss: 0.0017\n",
      "Accuracy on test set: 73.75%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Olivetti faces\n",
    "data = fetch_olivetti_faces()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class OlivettiClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OlivettiClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 40)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4096)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# \n",
    "model = OlivettiClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \n",
    "epochs = 200\n",
    "losses = []  # epoch\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    print(f'Epoch {epoch+1}, loss: {epoch_loss:.4f}')\n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# \n",
    "plt.figure()\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f355ec97-a747-4a7f-bd49-7eb7cf33b0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 1, loss: 3.6925\n",
      "Epoch 2, loss: 3.6873\n",
      "Epoch 3, loss: 3.6825\n",
      "Epoch 4, loss: 3.6797\n",
      "Epoch 5, loss: 3.6724\n",
      "Epoch 6, loss: 3.6662\n",
      "Epoch 7, loss: 3.6598\n",
      "Epoch 8, loss: 3.6543\n",
      "Epoch 9, loss: 3.6431\n",
      "Epoch 10, loss: 3.6341\n",
      "Epoch 11, loss: 3.6124\n",
      "Epoch 12, loss: 3.5950\n",
      "Epoch 13, loss: 3.5688\n",
      "Epoch 14, loss: 3.5351\n",
      "Epoch 15, loss: 3.4968\n",
      "Epoch 16, loss: 3.4701\n",
      "Epoch 17, loss: 3.3734\n",
      "Epoch 18, loss: 3.3031\n",
      "Epoch 19, loss: 3.2530\n",
      "Epoch 20, loss: 3.1410\n",
      "Epoch 21, loss: 3.0504\n",
      "Epoch 22, loss: 2.9736\n",
      "Epoch 23, loss: 2.8904\n",
      "Epoch 24, loss: 2.7658\n",
      "Epoch 25, loss: 2.6753\n",
      "Epoch 26, loss: 2.6224\n",
      "Epoch 27, loss: 2.5626\n",
      "Epoch 28, loss: 2.4733\n",
      "Epoch 29, loss: 2.3967\n",
      "Epoch 30, loss: 2.3123\n",
      "Epoch 31, loss: 2.2735\n",
      "Epoch 32, loss: 2.2154\n",
      "Epoch 33, loss: 2.1542\n",
      "Epoch 34, loss: 2.0836\n",
      "Epoch 35, loss: 2.0196\n",
      "Epoch 36, loss: 1.9650\n",
      "Epoch 37, loss: 1.9260\n",
      "Epoch 38, loss: 1.8729\n",
      "Epoch 39, loss: 1.8550\n",
      "Epoch 40, loss: 1.8317\n",
      "Epoch 41, loss: 1.8202\n",
      "Epoch 42, loss: 1.7241\n",
      "Epoch 43, loss: 1.6863\n",
      "Epoch 44, loss: 1.5901\n",
      "Epoch 45, loss: 1.5820\n",
      "Epoch 46, loss: 1.5325\n",
      "Epoch 47, loss: 1.5387\n",
      "Epoch 48, loss: 1.4730\n",
      "Epoch 49, loss: 1.4324\n",
      "Epoch 50, loss: 1.3677\n",
      "Epoch 51, loss: 1.3609\n",
      "Epoch 52, loss: 1.3094\n",
      "Epoch 53, loss: 1.2734\n",
      "Epoch 54, loss: 1.2451\n",
      "Epoch 55, loss: 1.2167\n",
      "Epoch 56, loss: 1.2701\n",
      "Epoch 57, loss: 1.1563\n",
      "Epoch 58, loss: 1.1192\n",
      "Epoch 59, loss: 1.1234\n",
      "Epoch 60, loss: 1.0638\n",
      "Epoch 61, loss: 1.0827\n",
      "Epoch 62, loss: 1.0216\n",
      "Epoch 63, loss: 0.9879\n",
      "Epoch 64, loss: 0.9991\n",
      "Epoch 65, loss: 0.9406\n",
      "Epoch 66, loss: 0.9683\n",
      "Epoch 67, loss: 0.9376\n",
      "Epoch 68, loss: 0.9078\n",
      "Epoch 69, loss: 0.8936\n",
      "Epoch 70, loss: 0.8800\n",
      "Epoch 71, loss: 0.8757\n",
      "Epoch 72, loss: 0.8651\n",
      "Epoch 73, loss: 0.8008\n",
      "Epoch 74, loss: 0.8341\n",
      "Epoch 75, loss: 0.8155\n",
      "Epoch 76, loss: 0.7759\n",
      "Epoch 77, loss: 0.7208\n",
      "Epoch 78, loss: 0.7489\n",
      "Epoch 79, loss: 0.7615\n",
      "Epoch 80, loss: 0.7334\n",
      "Epoch 81, loss: 0.7052\n",
      "Epoch 82, loss: 0.6643\n",
      "Epoch 83, loss: 0.6629\n",
      "Epoch 84, loss: 0.6751\n",
      "Epoch 85, loss: 0.6321\n",
      "Epoch 86, loss: 0.6057\n",
      "Epoch 87, loss: 0.5984\n",
      "Epoch 88, loss: 0.5919\n",
      "Epoch 89, loss: 0.5968\n",
      "Epoch 90, loss: 0.5544\n",
      "Epoch 91, loss: 0.5524\n",
      "Epoch 92, loss: 0.5388\n",
      "Epoch 93, loss: 0.5276\n",
      "Epoch 94, loss: 0.5096\n",
      "Epoch 95, loss: 0.5281\n",
      "Epoch 96, loss: 0.5056\n",
      "Epoch 97, loss: 0.5153\n",
      "Epoch 98, loss: 0.4873\n",
      "Epoch 99, loss: 0.5013\n",
      "Epoch 100, loss: 0.4941\n",
      "Epoch 101, loss: 0.4882\n",
      "Epoch 102, loss: 0.4683\n",
      "Epoch 103, loss: 0.4657\n",
      "Epoch 104, loss: 0.4636\n",
      "Epoch 105, loss: 0.4414\n",
      "Epoch 106, loss: 0.4097\n",
      "Epoch 107, loss: 0.4386\n",
      "Epoch 108, loss: 0.3983\n",
      "Epoch 109, loss: 0.3868\n",
      "Epoch 110, loss: 0.3857\n",
      "Epoch 111, loss: 0.3638\n",
      "Epoch 112, loss: 0.3832\n",
      "Epoch 113, loss: 0.3814\n",
      "Epoch 114, loss: 0.3522\n",
      "Epoch 115, loss: 0.3427\n",
      "Epoch 116, loss: 0.3479\n",
      "Epoch 117, loss: 0.3601\n",
      "Epoch 118, loss: 0.3462\n",
      "Epoch 119, loss: 0.3091\n",
      "Epoch 120, loss: 0.3154\n",
      "Epoch 121, loss: 0.3319\n",
      "Epoch 122, loss: 0.3175\n",
      "Epoch 123, loss: 0.3128\n",
      "Epoch 124, loss: 0.3101\n",
      "Epoch 125, loss: 0.2857\n",
      "Epoch 126, loss: 0.2826\n",
      "Epoch 127, loss: 0.2873\n",
      "Epoch 128, loss: 0.2886\n",
      "Epoch 129, loss: 0.3284\n",
      "Epoch 130, loss: 0.3315\n",
      "Epoch 131, loss: 0.2984\n",
      "Epoch 132, loss: 0.2994\n",
      "Epoch 133, loss: 0.2919\n",
      "Epoch 134, loss: 0.3188\n",
      "Epoch 135, loss: 0.2794\n",
      "Epoch 136, loss: 0.2695\n",
      "Epoch 137, loss: 0.2463\n",
      "Epoch 138, loss: 0.2455\n",
      "Epoch 139, loss: 0.2219\n",
      "Epoch 140, loss: 0.2404\n",
      "Epoch 141, loss: 0.2474\n",
      "Epoch 142, loss: 0.2170\n",
      "Epoch 143, loss: 0.2570\n",
      "Epoch 144, loss: 0.2181\n",
      "Epoch 145, loss: 0.2087\n",
      "Epoch 146, loss: 0.2065\n",
      "Epoch 147, loss: 0.1931\n",
      "Epoch 148, loss: 0.1881\n",
      "Epoch 149, loss: 0.1879\n",
      "Epoch 150, loss: 0.1805\n",
      "Epoch 151, loss: 0.1742\n",
      "Epoch 152, loss: 0.1768\n",
      "Epoch 153, loss: 0.1710\n",
      "Epoch 154, loss: 0.1736\n",
      "Epoch 155, loss: 0.1654\n",
      "Epoch 156, loss: 0.1671\n",
      "Epoch 157, loss: 0.1753\n",
      "Epoch 158, loss: 0.1768\n",
      "Epoch 159, loss: 0.1707\n",
      "Epoch 160, loss: 0.1593\n",
      "Epoch 161, loss: 0.1569\n",
      "Epoch 162, loss: 0.1463\n",
      "Epoch 163, loss: 0.1430\n",
      "Epoch 164, loss: 0.1579\n",
      "Epoch 165, loss: 0.1530\n",
      "Epoch 166, loss: 0.1484\n",
      "Epoch 167, loss: 0.1420\n",
      "Epoch 168, loss: 0.1424\n",
      "Epoch 169, loss: 0.1355\n",
      "Epoch 170, loss: 0.1314\n",
      "Epoch 171, loss: 0.1190\n",
      "Epoch 172, loss: 0.1241\n",
      "Epoch 173, loss: 0.1332\n",
      "Epoch 174, loss: 0.1446\n",
      "Epoch 175, loss: 0.1336\n",
      "Epoch 176, loss: 0.1286\n",
      "Epoch 177, loss: 0.1354\n",
      "Epoch 178, loss: 0.1414\n",
      "Epoch 179, loss: 0.1337\n",
      "Epoch 180, loss: 0.1164\n",
      "Epoch 181, loss: 0.1248\n",
      "Epoch 182, loss: 0.1268\n",
      "Epoch 183, loss: 0.1084\n",
      "Epoch 184, loss: 0.1037\n",
      "Epoch 185, loss: 0.0978\n",
      "Epoch 186, loss: 0.1036\n",
      "Epoch 187, loss: 0.1175\n",
      "Epoch 188, loss: 0.1058\n",
      "Epoch 189, loss: 0.1060\n",
      "Epoch 190, loss: 0.0916\n",
      "Epoch 191, loss: 0.0897\n",
      "Epoch 192, loss: 0.0941\n",
      "Epoch 193, loss: 0.0893\n",
      "Epoch 194, loss: 0.0844\n",
      "Epoch 195, loss: 0.0831\n",
      "Epoch 196, loss: 0.0831\n",
      "Epoch 197, loss: 0.0846\n",
      "Epoch 198, loss: 0.0809\n",
      "Epoch 199, loss: 0.0777\n",
      "Epoch 200, loss: 0.0786\n",
      "Accuracy on test set: 81.25%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Olivetti faces\n",
    "data = fetch_olivetti_faces()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class OlivettiClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OlivettiClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 40)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4096)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# \n",
    "model = OlivettiClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# \n",
    "epochs = 200\n",
    "losses = []  # epoch\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    print(f'Epoch {epoch+1}, loss: {epoch_loss:.4f}')\n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# \n",
    "plt.figure()\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7579399a-23f5-4473-a97e-86a4fa76cfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 10, loss: 3.5248\n",
      "Epoch 20, loss: 2.2061\n",
      "Epoch 30, loss: 1.3171\n",
      "Epoch 40, loss: 0.7230\n",
      "Epoch 50, loss: 0.5704\n",
      "Epoch 60, loss: 0.5461\n",
      "Epoch 70, loss: 0.2227\n",
      "Epoch 80, loss: 0.1282\n",
      "Epoch 90, loss: 0.1580\n",
      "Epoch 100, loss: 0.0295\n",
      "Epoch 110, loss: 0.0147\n",
      "Epoch 120, loss: 0.0102\n",
      "Epoch 130, loss: 0.0063\n",
      "Epoch 140, loss: 0.0050\n",
      "Epoch 150, loss: 0.0044\n",
      "Epoch 160, loss: 0.0032\n",
      "Epoch 170, loss: 0.0026\n",
      "Epoch 180, loss: 0.0021\n",
      "Epoch 190, loss: 0.0018\n",
      "Epoch 200, loss: 0.0016\n",
      "Epoch 210, loss: 0.0013\n",
      "Epoch 220, loss: 0.0012\n",
      "Epoch 230, loss: 0.0010\n",
      "Epoch 240, loss: 0.0009\n",
      "Epoch 250, loss: 0.0008\n",
      "Epoch 260, loss: 0.0007\n",
      "Epoch 270, loss: 0.0006\n",
      "Epoch 280, loss: 0.0006\n",
      "Epoch 290, loss: 0.0005\n",
      "Epoch 300, loss: 0.0005\n",
      "Accuracy on test set: 72.50%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Olivetti faces\n",
    "data = fetch_olivetti_faces()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class OlivettiClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OlivettiClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 40)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4096)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = OlivettiClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \n",
    "epochs = 300\n",
    "losses = [] # epoch\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, loss: {epoch_loss:.4f}')\n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# \n",
    "plt.figure()\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1288cc89-da3c-4c48-ad94-bba69982b169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 10, loss: 2.7730\n",
      "Epoch 20, loss: 1.2236\n",
      "Epoch 30, loss: 0.5490\n",
      "Epoch 40, loss: 0.2830\n",
      "Epoch 50, loss: 0.1079\n",
      "Epoch 60, loss: 0.0718\n",
      "Epoch 70, loss: 0.0353\n",
      "Epoch 80, loss: 0.0183\n",
      "Epoch 90, loss: 0.0113\n",
      "Epoch 100, loss: 0.0083\n",
      "Epoch 110, loss: 0.0067\n",
      "Epoch 120, loss: 0.0054\n",
      "Epoch 130, loss: 0.0043\n",
      "Epoch 140, loss: 0.0034\n",
      "Epoch 150, loss: 0.0029\n",
      "Epoch 160, loss: 0.0025\n",
      "Epoch 170, loss: 0.0021\n",
      "Epoch 180, loss: 0.0018\n",
      "Epoch 190, loss: 0.0016\n",
      "Epoch 200, loss: 0.0014\n",
      "Accuracy on test set: 86.25%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Olivetti faces\n",
    "data = fetch_olivetti_faces()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class OlivettiClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OlivettiClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 40)\n",
    "        self.to(device)  # GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4096)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = OlivettiClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0015)\n",
    "\n",
    "# \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# \n",
    "epochs = 200\n",
    "losses = []  # epoch\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, loss: {epoch_loss:.4f}')\n",
    "    \n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# \n",
    "plt.figure()\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0802c22d-77bd-4426-977a-870831b5f018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 10, loss: 3.6849\n",
      "Epoch 20, loss: 3.6572\n",
      "Epoch 30, loss: 3.6120\n",
      "Epoch 40, loss: 3.5624\n",
      "Epoch 50, loss: 3.4889\n",
      "Epoch 60, loss: 3.2695\n",
      "Epoch 70, loss: 3.0551\n",
      "Epoch 80, loss: 2.9051\n",
      "Epoch 90, loss: 2.6934\n",
      "Epoch 100, loss: 2.4535\n",
      "Epoch 110, loss: 2.3209\n",
      "Epoch 120, loss: 2.2682\n",
      "Epoch 130, loss: 2.1309\n",
      "Epoch 140, loss: 1.9795\n",
      "Epoch 150, loss: 1.8641\n",
      "Epoch 160, loss: 1.7338\n",
      "Epoch 170, loss: 1.6786\n",
      "Epoch 180, loss: 1.5482\n",
      "Epoch 190, loss: 1.5393\n",
      "Epoch 200, loss: 1.3732\n",
      "Epoch 210, loss: 1.3418\n",
      "Epoch 220, loss: 1.3073\n",
      "Epoch 230, loss: 1.1593\n",
      "Epoch 240, loss: 1.2700\n",
      "Epoch 250, loss: 1.1515\n",
      "Epoch 260, loss: 1.0983\n",
      "Epoch 270, loss: 1.0543\n",
      "Epoch 280, loss: 0.9356\n",
      "Epoch 290, loss: 0.9298\n",
      "Epoch 300, loss: 0.9476\n",
      "Epoch 310, loss: 0.8357\n",
      "Epoch 320, loss: 0.7528\n",
      "Epoch 330, loss: 0.7937\n",
      "Epoch 340, loss: 0.8478\n",
      "Epoch 350, loss: 0.8048\n",
      "Epoch 360, loss: 0.7813\n",
      "Epoch 370, loss: 0.6868\n",
      "Epoch 380, loss: 0.6224\n",
      "Epoch 390, loss: 0.6194\n",
      "Epoch 400, loss: 0.6821\n",
      "Epoch 410, loss: 0.6313\n",
      "Epoch 420, loss: 0.6367\n",
      "Epoch 430, loss: 0.5751\n",
      "Epoch 440, loss: 0.5587\n",
      "Epoch 450, loss: 0.6224\n",
      "Epoch 460, loss: 0.6098\n",
      "Epoch 470, loss: 0.5609\n",
      "Epoch 480, loss: 0.4389\n",
      "Epoch 490, loss: 0.4769\n",
      "Epoch 500, loss: 0.4249\n",
      "Epoch 510, loss: 0.5153\n",
      "Epoch 520, loss: 0.4068\n",
      "Epoch 530, loss: 0.3675\n",
      "Epoch 540, loss: 0.4422\n",
      "Epoch 550, loss: 0.4248\n",
      "Epoch 560, loss: 0.3694\n",
      "Epoch 570, loss: 0.4269\n",
      "Epoch 580, loss: 0.3879\n",
      "Epoch 590, loss: 0.3377\n",
      "Epoch 600, loss: 0.3750\n",
      "Epoch 610, loss: 0.3547\n",
      "Epoch 620, loss: 0.3368\n",
      "Epoch 630, loss: 0.3027\n",
      "Epoch 640, loss: 0.3696\n",
      "Epoch 650, loss: 0.3116\n",
      "Epoch 660, loss: 0.3759\n",
      "Epoch 670, loss: 0.2934\n",
      "Epoch 680, loss: 0.3425\n",
      "Epoch 690, loss: 0.3119\n",
      "Epoch 700, loss: 0.2561\n",
      "Epoch 710, loss: 0.2459\n",
      "Epoch 720, loss: 0.2424\n",
      "Epoch 730, loss: 0.2679\n",
      "Epoch 740, loss: 0.2474\n",
      "Epoch 750, loss: 0.2351\n",
      "Epoch 760, loss: 0.2163\n",
      "Epoch 770, loss: 0.2773\n",
      "Epoch 780, loss: 0.3142\n",
      "Epoch 790, loss: 0.2555\n",
      "Epoch 800, loss: 0.2074\n",
      "Epoch 810, loss: 0.2167\n",
      "Epoch 820, loss: 0.2250\n",
      "Epoch 830, loss: 0.1957\n",
      "Epoch 840, loss: 0.2375\n",
      "Epoch 850, loss: 0.2130\n",
      "Epoch 860, loss: 0.2200\n",
      "Epoch 870, loss: 0.1776\n",
      "Epoch 880, loss: 0.2216\n",
      "Epoch 890, loss: 0.2468\n",
      "Epoch 900, loss: 0.1730\n",
      "Epoch 910, loss: 0.1919\n",
      "Epoch 920, loss: 0.2005\n",
      "Epoch 930, loss: 0.1770\n",
      "Epoch 940, loss: 0.2002\n",
      "Epoch 950, loss: 0.1834\n",
      "Epoch 960, loss: 0.2217\n",
      "Epoch 970, loss: 0.1811\n",
      "Epoch 980, loss: 0.1638\n",
      "Epoch 990, loss: 0.1867\n",
      "Epoch 1000, loss: 0.1689\n",
      "Accuracy on test set: 86.25%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Olivetti faces\n",
    "data = fetch_olivetti_faces()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class OlivettiClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OlivettiClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        self.dropout1 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc3 = nn.Linear(256, 40)\n",
    "        self.to(device)  # GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4096)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = OlivettiClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "\n",
    "# \n",
    "epochs = 1000\n",
    "losses = []  # epoch\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, loss: {epoch_loss:.4f}')\n",
    "    \n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# \n",
    "plt.figure()\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1b6b567a-c2c7-422f-a51e-4def1d7dd0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 10, loss: 2.6073\n",
      "Epoch 20, loss: 0.9743\n",
      "Epoch 30, loss: 0.4174\n",
      "Epoch 40, loss: 0.1542\n",
      "Epoch 50, loss: 0.0694\n",
      "Epoch 60, loss: 0.0388\n",
      "Epoch 70, loss: 0.0215\n",
      "Epoch 80, loss: 0.0130\n",
      "Epoch 90, loss: 0.0096\n",
      "Epoch 100, loss: 0.0071\n",
      "Accuracy on test set: 93.75%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Olivetti faces\n",
    "data = fetch_olivetti_faces()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class OlivettiClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OlivettiClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, 40)\n",
    "        self.to(device)  # GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4096)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = OlivettiClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# \n",
    "epochs = 100\n",
    "losses = []  # epoch\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, loss: {epoch_loss:.4f}')\n",
    "    \n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# \n",
    "plt.figure()\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dfc08f26-4035-4638-bf2c-d378da08633d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 2, loss: 0.5842\n",
      "Epoch 4, loss: 0.3539\n",
      "Epoch 6, loss: 0.2288\n",
      "Epoch 8, loss: 0.1890\n",
      "Epoch 10, loss: 0.1087\n",
      "Epoch 12, loss: 0.1672\n",
      "Epoch 14, loss: 0.1245\n",
      "Epoch 16, loss: 0.1022\n",
      "Epoch 18, loss: 0.0789\n",
      "Epoch 20, loss: 0.0662\n",
      "Epoch 22, loss: 0.1143\n",
      "Epoch 24, loss: 0.1009\n",
      "Epoch 26, loss: 0.1377\n",
      "Epoch 28, loss: 0.0859\n",
      "Epoch 30, loss: 0.0558\n",
      "Epoch 32, loss: 0.1238\n",
      "Epoch 34, loss: 0.0725\n",
      "Epoch 36, loss: 0.0515\n",
      "Epoch 38, loss: 0.1023\n",
      "Epoch 40, loss: 0.0770\n",
      "Accuracy on test set: 82.44%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# 20 Newsgroups\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')\n",
    "X, y = newsgroups_data.data, newsgroups_data.target\n",
    "\n",
    "# CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=4096)  # \n",
    "X = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        #self.dropout1 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        #self.dropout2 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc3 = nn.Linear(256, 20)  # 2020\n",
    "        self.to(device)  # GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "      #  x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "      #  x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = NewsClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \n",
    "epochs = 40\n",
    "losses = []  # epoch\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    if (epoch+1) % 2 == 0:\n",
    "        print(f'Epoch {epoch+1}, loss: {epoch_loss:.4f}')\n",
    "    \n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# \n",
    "plt.figure()\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "96cd21d0-b394-4e7c-913a-8a77cf2d31c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 2, loss: 0.2853\n",
      "Epoch 4, loss: 0.0606\n",
      "Epoch 6, loss: 0.0279\n",
      "Epoch 8, loss: 0.0075\n",
      "Epoch 10, loss: 0.0707\n",
      "Epoch 12, loss: 0.0190\n",
      "Epoch 14, loss: 0.0244\n",
      "Epoch 16, loss: 0.0116\n",
      "Epoch 18, loss: 0.0045\n",
      "Epoch 20, loss: 0.0662\n",
      "Epoch 22, loss: 0.0121\n",
      "Epoch 24, loss: 0.0021\n",
      "Epoch 26, loss: 0.0012\n",
      "Epoch 28, loss: 0.0016\n",
      "Epoch 30, loss: 0.0018\n",
      "Epoch 32, loss: 0.0059\n",
      "Epoch 34, loss: 0.0848\n",
      "Epoch 36, loss: 0.0040\n",
      "Epoch 38, loss: 0.0102\n",
      "Epoch 40, loss: 0.0127\n",
      "Accuracy on test set: 85.20%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# 20 Newsgroups\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')\n",
    "X, y = newsgroups_data.data, newsgroups_data.target\n",
    "\n",
    "# CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=4096)  # \n",
    "X = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        #self.dropout1 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        #self.dropout2 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc3 = nn.Linear(256, 20)  # 2020\n",
    "        self.to(device)  # GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "      #  x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "      #  x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = NewsClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \n",
    "epochs = 40\n",
    "losses = []  # epoch\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    if (epoch+1) % 2 == 0:\n",
    "        print(f'Epoch {epoch+1}, loss: {epoch_loss:.4f}')\n",
    "    \n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# \n",
    "plt.figure()\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "73c27531-5d77-42eb-801b-1829a075085c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 2, loss: 0.2766\n",
      "Epoch 4, loss: 0.0748\n",
      "Epoch 6, loss: 0.0279\n",
      "Epoch 8, loss: 0.0102\n",
      "Epoch 10, loss: 0.0923\n",
      "Epoch 12, loss: 0.0399\n",
      "Epoch 14, loss: 0.0315\n",
      "Epoch 16, loss: 0.0032\n",
      "Epoch 18, loss: 0.0166\n",
      "Epoch 20, loss: 0.0391\n",
      "Accuracy on test set: 85.04%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABOVUlEQVR4nO3deVxU5f4H8M8MMMM+gOzIorjmguSCaGULuWSmZj/RLNE2K+tq1r1mpWjdIrO8ddM0K7XV9ap10zQlrZtiKoKZueEGimwiDMM2MPP8/kBGJ3acmTMzfN6v17xkzjxn+B6OIx+f5znnkQkhBIiIiIjshFzqAoiIiIhMieGGiIiI7ArDDREREdkVhhsiIiKyKww3REREZFcYboiIiMiuMNwQERGRXWG4ISIiIrvCcENERER2heGGqA2aMmUKIiIiWrXv/PnzIZPJTFsQEZEJMdwQWRGZTNasx549e6QuVRJTpkyBu7u71GU02+bNmzFixAj4+vpCoVAgODgY48ePx08//SR1aUR2Tca1pYisx1dffWX0/IsvvsDOnTvx5ZdfGm2/9957ERAQ0OrvU1VVBb1eD6VS2eJ9q6urUV1dDWdn51Z//9aaMmUKNm7cCI1GY/Hv3RJCCDz22GNYvXo1oqOj8dBDDyEwMBCXL1/G5s2bkZqair1792LQoEFSl0pklxylLoCIrnvkkUeMnu/fvx87d+6ss/2vysrK4Orq2uzv4+Tk1Kr6AMDR0RGOjvynozHvvfceVq9ejZkzZ2Lx4sVGw3ivvvoqvvzyS5P8DIUQqKiogIuLy02/F5E94bAUkY2588470bNnT6SmpuKOO+6Aq6srXnnlFQDAt99+i5EjRyI4OBhKpRKRkZF44403oNPpjN7jr3Nuzp8/D5lMhnfffRcrVqxAZGQklEol+vfvj4MHDxrtW9+cG5lMhueeew5btmxBz549oVQq0aNHD2zfvr1O/Xv27EG/fv3g7OyMyMhIfPzxxyafx7Nhwwb07dsXLi4u8PX1xSOPPIJLly4ZtcnJycHUqVPRvn17KJVKBAUFYfTo0Th//ryhzaFDhzBs2DD4+vrCxcUFHTp0wGOPPdbo9y4vL0dSUhK6deuGd999t97jevTRRzFgwAAADc9hWr16NWQymVE9ERERuP/++7Fjxw7069cPLi4u+Pjjj9GzZ0/cdddddd5Dr9cjJCQEDz30kNG2999/Hz169ICzszMCAgIwbdo0XL16tdHjIrIl/O8XkQ26cuUKRowYgQkTJuCRRx4xDFGtXr0a7u7umDVrFtzd3fHTTz9h3rx5UKvVWLRoUZPv+80336CkpATTpk2DTCbDO++8gwcffBBnz55tsrfn119/xaZNm/Dss8/Cw8MD//73vzFu3DhkZmaiXbt2AIC0tDQMHz4cQUFBWLBgAXQ6HV5//XX4+fnd/A/lmtWrV2Pq1Kno378/kpKSkJubiw8++AB79+5FWloavLy8AADjxo3DsWPH8PzzzyMiIgJ5eXnYuXMnMjMzDc+HDh0KPz8/vPzyy/Dy8sL58+exadOmJn8OhYWFmDlzJhwcHEx2XLVOnjyJiRMnYtq0aXjyySfRtWtXxMfHY/78+cjJyUFgYKBRLdnZ2ZgwYYJh27Rp0ww/o7/97W84d+4clixZgrS0NOzdu/emevWIrIYgIqs1ffp08deP6ZAhQwQAsXz58jrty8rK6mybNm2acHV1FRUVFYZtCQkJIjw83PD83LlzAoBo166dKCwsNGz/9ttvBQDx3//+17AtMTGxTk0AhEKhEBkZGYZtR44cEQDEhx9+aNg2atQo4erqKi5dumTYdvr0aeHo6FjnPeuTkJAg3NzcGnxdq9UKf39/0bNnT1FeXm7Y/v333wsAYt68eUIIIa5evSoAiEWLFjX4Xps3bxYAxMGDB5us60YffPCBACA2b97crPb1/TyFEGLVqlUCgDh37pxhW3h4uAAgtm/fbtT25MmTdX7WQgjx7LPPCnd3d8Pfi//9738CgPj666+N2m3fvr3e7US2isNSRDZIqVRi6tSpdbbfOPeipKQEBQUFuP3221FWVoYTJ040+b7x8fHw9vY2PL/99tsBAGfPnm1y37i4OERGRhqe9+7dG56enoZ9dToddu3ahTFjxiA4ONjQrlOnThgxYkST798chw4dQl5eHp599lmjCc8jR45Et27dsHXrVgA1PyeFQoE9e/Y0OBxT28Pz/fffo6qqqtk1qNVqAICHh0crj6JxHTp0wLBhw4y2denSBX369MG6desM23Q6HTZu3IhRo0YZ/l5s2LABKpUK9957LwoKCgyPvn37wt3dHbt37zZLzUSWxnBDZINCQkKgUCjqbD927BjGjh0LlUoFT09P+Pn5GSYjFxcXN/m+YWFhRs9rg05z5mP8dd/a/Wv3zcvLQ3l5OTp16lSnXX3bWuPChQsAgK5du9Z5rVu3bobXlUolFi5ciB9++AEBAQG444478M477yAnJ8fQfsiQIRg3bhwWLFgAX19fjB49GqtWrUJlZWWjNXh6egKoCZfm0KFDh3q3x8fHY+/evYa5RXv27EFeXh7i4+MNbU6fPo3i4mL4+/vDz8/P6KHRaJCXl2eWmoksjeGGyAbVd3VMUVERhgwZgiNHjuD111/Hf//7X+zcuRMLFy4EUDORtCkNzRERzbhjxM3sK4WZM2fi1KlTSEpKgrOzM+bOnYvu3bsjLS0NQM0k6Y0bNyIlJQXPPfccLl26hMceewx9+/Zt9FL0bt26AQCOHj3arDoamkj910ngtRq6Mio+Ph5CCGzYsAEAsH79eqhUKgwfPtzQRq/Xw9/fHzt37qz38frrrzerZiJrx3BDZCf27NmDK1euYPXq1ZgxYwbuv/9+xMXFGQ0zScnf3x/Ozs7IyMio81p921ojPDwcQM2k2786efKk4fVakZGRePHFF/Hjjz/ijz/+gFarxXvvvWfUZuDAgXjzzTdx6NAhfP311zh27BjWrl3bYA233XYbvL29sWbNmgYDyo1qz09RUZHR9tpepubq0KEDBgwYgHXr1qG6uhqbNm3CmDFjjO5lFBkZiStXrmDw4MGIi4ur84iKimrR9ySyVgw3RHaitufkxp4SrVaLjz76SKqSjDg4OCAuLg5btmxBdna2YXtGRgZ++OEHk3yPfv36wd/fH8uXLzcaPvrhhx9w/PhxjBw5EkDNfYEqKiqM9o2MjISHh4dhv6tXr9bpderTpw8ANDo05erqitmzZ+P48eOYPXt2vT1XX331FQ4cOGD4vgDwyy+/GF4vLS3F559/3tzDNoiPj8f+/fuxcuVKFBQUGA1JAcD48eOh0+nwxhtv1Nm3urq6TsAislW8FJzITgwaNAje3t5ISEjA3/72N8hkMnz55ZdWNSw0f/58/Pjjjxg8eDCeeeYZ6HQ6LFmyBD179kR6enqz3qOqqgr//Oc/62z38fHBs88+i4ULF2Lq1KkYMmQIJk6caLgUPCIiAi+88AIA4NSpU7jnnnswfvx43HLLLXB0dMTmzZuRm5truGz6888/x0cffYSxY8ciMjISJSUl+OSTT+Dp6Yn77ruv0Rr//ve/49ixY3jvvfewe/duwx2Kc3JysGXLFhw4cAD79u0DAAwdOhRhYWF4/PHH8fe//x0ODg5YuXIl/Pz8kJmZ2YKfbk14eemll/DSSy/Bx8cHcXFxRq8PGTIE06ZNQ1JSEtLT0zF06FA4OTnh9OnT2LBhAz744AOje+IQ2SwJr9QioiY0dCl4jx496m2/d+9eMXDgQOHi4iKCg4PFP/7xD7Fjxw4BQOzevdvQrqFLweu7NBqASExMNDxv6FLw6dOn19k3PDxcJCQkGG1LTk4W0dHRQqFQiMjISPHpp5+KF198UTg7OzfwU7guISFBAKj3ERkZaWi3bt06ER0dLZRKpfDx8RGTJk0SFy9eNLxeUFAgpk+fLrp16ybc3NyESqUSMTExYv369YY2hw8fFhMnThRhYWFCqVQKf39/cf/994tDhw41WWetjRs3iqFDhwofHx/h6OgogoKCRHx8vNizZ49Ru9TUVBETEyMUCoUICwsTixcvbvBS8JEjRzb6PQcPHiwAiCeeeKLBNitWrBB9+/YVLi4uwsPDQ/Tq1Uv84x//ENnZ2c0+NiJrxrWliEhyY8aMwbFjx3D69GmpSyEiO8A5N0RkUeXl5UbPT58+jW3btuHOO++UpiAisjvsuSEiiwoKCsKUKVPQsWNHXLhwAcuWLUNlZSXS0tLQuXNnqcsjIjvACcVEZFHDhw/HmjVrkJOTA6VSidjYWLz11lsMNkRkMuy5ISIiIrvCOTdERERkVxhuiIiIyK60uTk3er0e2dnZ8PDwaHBNFyIiIrIuQgiUlJQgODgYcnnjfTNtLtxkZ2cjNDRU6jKIiIioFbKystC+fftG27S5cOPh4QGg5ofj6ekpcTVERETUHGq1GqGhoYbf441pc+GmdijK09OT4YaIiMjGNGdKiaQTin/55ReMGjUKwcHBkMlk2LJlS6PtN23ahHvvvRd+fn7w9PREbGwsduzYYZliiYiIyCZIGm5KS0sRFRWFpUuXNqv9L7/8gnvvvRfbtm1Damoq7rrrLowaNQppaWlmrpSIiIhshdXcxE8mk2Hz5s0YM2ZMi/br0aMH4uPjMW/evGa1V6vVUKlUKC4u5rAUERGRjWjJ72+bnnOj1+tRUlICHx+fBttUVlaisrLS8FytVluiNCIiIpKITd/E791334VGo8H48eMbbJOUlASVSmV48DJwIiIi+2az4eabb77BggULsH79evj7+zfYbs6cOSguLjY8srKyLFglERERWZpNDkutXbsWTzzxBDZs2IC4uLhG2yqVSiiVSgtVRkRERFKzuZ6bNWvWYOrUqVizZg1GjhwpdTlERERkZSTtudFoNMjIyDA8P3fuHNLT0+Hj44OwsDDMmTMHly5dwhdffAGgZigqISEBH3zwAWJiYpCTkwMAcHFxgUqlkuQYiIiIyLpI2nNz6NAhREdHIzo6GgAwa9YsREdHGy7rvnz5MjIzMw3tV6xYgerqakyfPh1BQUGGx4wZMySpn4iIiKyP1dznxlJ4nxsiIiLb05Lf3zY354aIiIioMQw3JqLTC+SpK3DhSqnUpRAREbVpDDcmsv/sFQx4KxlPfnFI6lKIiIjaNIYbE/F1r7mXTn5JZRMtiYiIyJwYbkzEz6Mm3Fwtq0KVTi9xNURERG0Xw42JeLk4wVEuAwBc0WglroaIiKjtYrgxEblchnbuCgAcmiIiIpISw40J1Q5N5WsqJK6EiIio7WK4MaHaScUFJRyWIiIikgrDjQn51V4xpeGwFBERkVQYbkzIMCzFOTdERESSYbgxIV/23BAREUmO4caE2HNDREQkPYYbE6oNNwUMN0RERJJhuDEhDksRERFJj+HGhGp7bkoqqlFRpZO4GiIioraJ4caEPJ0doXCs+ZEWsPeGiIhIEgw3JiSTya7f64bzboiIiCTBcGNivrxiioiISFIMNybmd23xzAKuDE5ERCQJhhsT471uiIiIpMVwY2LX15fiyuBERERSYLgxMV8PrgxOREQkJYYbE+PK4ERERNJiuDExzrkhIiKSFsONidUuwcCb+BEREUmD4cbEantuyrQ6lFZWS1wNERFR28NwY2JuSke4KhwAsPeGiIhICgw3ZuDLJRiIiIgkw3BjBpxUTEREJB2GGzPw46RiIiIiyTDcmIGvR836Uuy5ISIisjyGGzPwc3cGwBv5ERERSYHhxgyu99xwCQYiIiJLY7gxAy7BQEREJB2GGzPwMyyeyXBDRERkaQw3ZuB7Q8+NEELiaoiIiNoWhhszqO250Vbroa7gEgxERESWxHBjBs5ODvBwdgTAe90QERFZGsONmfhxCQYiIiJJMNyYiS+XYCAiIpIEw42ZGK6Y4rAUERGRRTHcmAmHpYiIiKTBcGMmXBmciIhIGgw3ZuLrXrMEA4eliIiILIvhxkwMPTcMN0RERBbFcGMmhpXBOSxFRERkUZKGm19++QWjRo1CcHAwZDIZtmzZ0uQ+e/bswa233gqlUolOnTph9erVZq+zNWpXBr+i0UKv5xIMREREliJpuCktLUVUVBSWLl3arPbnzp3DyJEjcddddyE9PR0zZ87EE088gR07dpi50pZr51YzLFWtFygqr5K4GiIiorbDUcpvPmLECIwYMaLZ7ZcvX44OHTrgvffeAwB0794dv/76K/71r39h2LBh5iqzVRSOcni7OuFqWRXySyrh46aQuiQiIqI2wabm3KSkpCAuLs5o27Bhw5CSktLgPpWVlVCr1UYPS6ldHZxXTBEREVmOTYWbnJwcBAQEGG0LCAiAWq1GeXl5vfskJSVBpVIZHqGhoZYoFQDvdUNERCQFmwo3rTFnzhwUFxcbHllZWRb73lyCgYiIyPIknXPTUoGBgcjNzTXalpubC09PT7i4uNS7j1KphFKptER5dfhyCQYiIiKLs6mem9jYWCQnJxtt27lzJ2JjYyWqqHEcliIiIrI8ScONRqNBeno60tPTAdRc6p2eno7MzEwANUNKkydPNrR/+umncfbsWfzjH//AiRMn8NFHH2H9+vV44YUXpCi/SYaeGw5LERERWYyk4ebQoUOIjo5GdHQ0AGDWrFmIjo7GvHnzAACXL182BB0A6NChA7Zu3YqdO3ciKioK7733Hj799FOruwy8FntuiIiILE/SOTd33nknhGj47r313X34zjvvRFpamhmrMh0/XgpORERkcTY158bW1C7BUFiqhY5LMBAREVkEw40ZtXNTQi4D9AK4UsreGyIiIktguDEjB7kMPm6cd0NERGRJDDdm5uteMzRVoNFKXAkREVHbwHBjZrxiioiIyLIYbsyMSzAQERFZFsONmflxCQYiIiKLYrgxMw5LERERWRbDjZlxWIqIiMiyGG7MjCuDExERWRbDjZkZhqXYc0NERGQRDDdmVttzU1RWBW21XuJqiIiI7B/DjZl5uTjBUS4DwCUYiIiILIHhxszkchnn3RAREVkQw40F1K4OziumiIiIzI/hxgJ4Iz8iIiLLYbixgOv3uuHimURERObGcGMBnHNDRERkOQw3FsAlGIiIiCyH4cYCeCM/IiIiy2G4sYDaYakC9twQERGZHcONBXBYioiIyHIYbiygtuempLIaFVU6iashIiKybww3FuDp7AiFY82Pmr03RERE5sVwYwEymez6jfw4qZiIiMisGG4sxNeDk4qJiIgsgeHGQthzQ0REZBkMNxbCK6aIiIgsg+HGQvzcuTI4ERGRJTDcWAh7boiIiCyD4cZCuDI4ERGRZTDcWAhXBiciIrIMhhsL4bAUERGRZTDcWEhtz015lQ6lldUSV0NERGS/GG4sxE3pCFeFAwD23hAREZkTw40FGYameDk4ERGR2TDcWFDt0BSXYCAiIjIfhhsL4hIMRERE5sdwY0G8YoqIiMj8GG4syDAsxZ4bIiIis2G4sSD23BAREZkfw40FXb9aikswEBERmQvDjQX51q4Mzp4bIiIis2G4saAbh6WEEBJXQ0REZJ8YbiyodkKxVqeHuoJLMBAREZkDw40FOTs5wMPZEQAnFRMREZkLw42F8YopIiIi85I83CxduhQRERFwdnZGTEwMDhw40Gj7999/H127doWLiwtCQ0PxwgsvoKKiwkLV3jze64aIiMi8JA0369atw6xZs5CYmIjDhw8jKioKw4YNQ15eXr3tv/nmG7z88stITEzE8ePH8dlnn2HdunV45ZVXLFx567HnhoiIyLwkDTeLFy/Gk08+ialTp+KWW27B8uXL4erqipUrV9bbft++fRg8eDAefvhhREREYOjQoZg4cWKTvT3WhOtLERERmZdk4Uar1SI1NRVxcXHXi5HLERcXh5SUlHr3GTRoEFJTUw1h5uzZs9i2bRvuu+++Br9PZWUl1Gq10UNKtT03vNcNERGReThK9Y0LCgqg0+kQEBBgtD0gIAAnTpyod5+HH34YBQUFuO222yCEQHV1NZ5++ulGh6WSkpKwYMECk9Z+M9hzQ0REZF6STyhuiT179uCtt97CRx99hMOHD2PTpk3YunUr3njjjQb3mTNnDoqLiw2PrKwsC1Zcl6HnhuGGiIjILCTrufH19YWDgwNyc3ONtufm5iIwMLDefebOnYtHH30UTzzxBACgV69eKC0txVNPPYVXX30VcnndrKZUKqFUKk1/AK1Ue7UUJxQTERGZh2Q9NwqFAn379kVycrJhm16vR3JyMmJjY+vdp6ysrE6AcXBwAACbWc7ges+NFnq9bdRMRERkSyTruQGAWbNmISEhAf369cOAAQPw/vvvo7S0FFOnTgUATJ48GSEhIUhKSgIAjBo1CosXL0Z0dDRiYmKQkZGBuXPnYtSoUYaQY+3aXVs8U6cXKCqvgo+bQuKKiIiI7Iuk4SY+Ph75+fmYN28ecnJy0KdPH2zfvt0wyTgzM9Oop+a1116DTCbDa6+9hkuXLsHPzw+jRo3Cm2++KdUhtJiTgxzerk64WlaF/JJKhhsiIiITkwlbGc8xEbVaDZVKheLiYnh6ekpSw9B//YxTuRp89XgMbuvsK0kNREREtqQlv79t6mope8ErpoiIiMyH4UYCvGKKiIjIfBhuJMAb+REREZkPw40EfLkEAxERkdkw3EiAPTdERETmw3AjgdoJxZxzQ0REZHoMNxKonVDMq6WIiIhMj+FGArU9N1dKtajW6SWuhoiIyL4w3EjAx00BuQwQAigs00pdDhERkV1huJGAg1wGHzfOuyEiIjIHhhuJcFIxERGReTDcSOT6EgwcliIiIjIlhhuJ+LrXrAbOnhsiIiLTYriRCIeliIiIzIPhRiJ+vNcNERGRWTDcSIQ9N0RERObBcCMRri9FRERkHgw3EjGsDM5wQ0REZFIMNxKp7bkpKquCtppLMBAREZkKw41EVC5OcHKQAQCulLL3hoiIyFQYbiQil8vQjkswEBERmRzDjYR4xRQREZHpMdxIyI+TiomIiEyO4UZCXIKBiIjI9BhuJMRhKSIiItNjuJGQrztXBiciIjI1hhsJseeGiIjI9BhuJMQlGIiIiEyP4UZChiUY2HNDRERkMgw3EqodliqprEa5VidxNURERPaB4UZCHkpHKB1rTgHvdUNERGQaDDcSkslkhiumOO+GiIjINBhuJMYrpoiIiEyL4UZiXIKBiIjItBhuJGYYlmLPDRERkUkw3EiMw1JERESmxXAjMQ5LERERmRbDjcT8uDI4ERGRSTHcSMwwLMWeGyIiIpNguJGYYWXwEi2EEBJXQ0REZPsYbiRWG27Kq3Qo5RIMREREN43hRmJuSke4KRwAcN4NERGRKTDcWAFfXjFFRERkMq0KN1lZWbh48aLh+YEDBzBz5kysWLHCZIW1JX68kR8REZHJtCrcPPzww9i9ezcAICcnB/feey8OHDiAV199Fa+//rpJC2wLeK8bIiIi02lVuPnjjz8wYMAAAMD69evRs2dP7Nu3D19//TVWr15tyvraBC7BQEREZDqtCjdVVVVQKmt+Ie/atQsPPPAAAKBbt264fPlyi95r6dKliIiIgLOzM2JiYnDgwIFG2xcVFWH69OkICgqCUqlEly5dsG3bttYchtXgEgxERESm06pw06NHDyxfvhz/+9//sHPnTgwfPhwAkJ2djXbt2jX7fdatW4dZs2YhMTERhw8fRlRUFIYNG4a8vLx622u1Wtx77704f/48Nm7ciJMnT+KTTz5BSEhIaw7DanBYioiIyHQcW7PTwoULMXbsWCxatAgJCQmIiooCAHz33XeG4armWLx4MZ588klMnToVALB8+XJs3boVK1euxMsvv1yn/cqVK1FYWIh9+/bByckJABAREdGaQ7AqHJYiIiIynVaFmzvvvBMFBQVQq9Xw9vY2bH/qqafg6urarPfQarVITU3FnDlzDNvkcjni4uKQkpJS7z7fffcdYmNjMX36dHz77bfw8/PDww8/jNmzZ8PBwaE1h2IVOCxFRERkOq0KN+Xl5RBCGILNhQsXsHnzZnTv3h3Dhg1r1nsUFBRAp9MhICDAaHtAQABOnDhR7z5nz57FTz/9hEmTJmHbtm3IyMjAs88+i6qqKiQmJta7T2VlJSorr4cGtVrdrPosyffa4pkFmpolGGQymcQVERER2a5WzbkZPXo0vvjiCwA1E3xjYmLw3nvvYcyYMVi2bJlJC7yRXq+Hv78/VqxYgb59+yI+Ph6vvvoqli9f3uA+SUlJUKlUhkdoaKjZ6mut2mEprU4PdXm1xNUQERHZtlaFm8OHD+P2228HAGzcuBEBAQG4cOECvvjiC/z73/9u1nv4+vrCwcEBubm5Rttzc3MRGBhY7z5BQUHo0qWL0RBU9+7dkZOTA61WW+8+c+bMQXFxseGRlZXVrPosydnJAZ7ONZ1o+ZoKiashIiKyba0KN2VlZfDw8AAA/Pjjj3jwwQchl8sxcOBAXLhwoVnvoVAo0LdvXyQnJxu26fV6JCcnIzY2tt59Bg8ejIyMDOj1esO2U6dOISgoCAqFot59lEolPD09jR7WyNcw76b+kEZERETN06pw06lTJ2zZsgVZWVnYsWMHhg4dCgDIy8trUXiYNWsWPvnkE3z++ec4fvw4nnnmGZSWlhqunpo8ebLRhONnnnkGhYWFmDFjBk6dOoWtW7firbfewvTp01tzGFbFsAQDLwcnIiK6Ka2aUDxv3jw8/PDDeOGFF3D33Xcbelp+/PFHREdHN/t94uPjkZ+fj3nz5iEnJwd9+vTB9u3bDZOMMzMzIZdfz1+hoaHYsWMHXnjhBfTu3RshISGYMWMGZs+e3ZrDsCqGe93wiikiIqKbIhNCiNbsmJOTg8uXLyMqKsoQQA4cOABPT09069bNpEWaklqthkqlQnFxsVUNUc3/7hhW7zuPZ+6MxOzh1vvzIyIikkJLfn+3qucGAAIDAxEYGGhYHbx9+/YtuoEfGeO9boiIiEyjVXNu9Ho9Xn/9dahUKoSHhyM8PBxeXl544403jCb7UvNxCQYiIiLTaFXPzauvvorPPvsMb7/9NgYPHgwA+PXXXzF//nxUVFTgzTffNGmRbYEfl2AgIiIyiVaFm88//xyffvqpYTVwAIYJvs8++yzDTStwWIqIiMg0WjUsVVhYWO+k4W7duqGwsPCmi2qLasPNlVIt9PpWzfEmIiIitDLcREVFYcmSJXW2L1myBL17977potoiH7eamxDq9AJXy3gjPyIiotZq1bDUO++8g5EjR2LXrl2Ge9ykpKQgKysL27ZtM2mBbYWTgxw+bgoUlmqRr6lEu2tzcIiIiKhlWtVzM2TIEJw6dQpjx45FUVERioqK8OCDD+LYsWP48ssvTV1jm2FYHZxLMBAREbVaq+9zExwcXGfi8JEjR/DZZ59hxYoVN11YW+TnocSpXA0XzyQiIroJreq5IfPg5eBEREQ3j+HGivi6197Ij8NSRERErcVwY0V4rxsiIqKb16I5Nw8++GCjrxcVFd1MLW0el2AgIiK6eS0KNyqVqsnXJ0+efFMFtWW+nHNDRER001oUblatWmWuOggcliIiIjIFzrmxIrXhprBMi2odV1cnIiJqDYYbK+LtqoBcBggBFJbyiikiIqLWYLixIg5ymWHZhTwOTREREbUKw42VuX6vG4YbIiKi1mC4sTKcVExERHRzGG6sjGEJBvbcEBERtQrDjZXx9eDK4ERERDeD4cbKsOeGiIjo5jDcWBnDEgycc0NERNQqDDdWhj03REREN4fhxsrwaikiIqKbw3BjZWrDTXF5FSqrdRJXQ0REZHsYbqyMysUJTg4yAMAVDa+YIiIiaimGGysjk8kMdynm0BQREVHLMdxYIS7BQERE1HoMN1aIk4qJiIhaj+HGCvlxWIqIiKjVGG6skGEJBg5LERERtRjDjRXijfyIiIhaj+HGCvl5OAPg4plEREStwXBjhXzda4al2HNDRETUcgw3VohXSxEREbUew40Vqg03mspqlGu5BAMREVFLMNxYIXelI5SONaeGV0wRERG1DMONFZLJZIbemzwOTREREbUIw42Vqg037LkhIiJqGYYbK8XFM4mIiFqH4cZK8YopIiKi1mG4sVJcGZyIiKh1GG6sFHtuiIiIWofhxkpxfSkiIqLWYbixUn5cGZyIiKhVrCLcLF26FBEREXB2dkZMTAwOHDjQrP3Wrl0LmUyGMWPGmLdACfi51yyemV9SCSGExNUQERHZDsnDzbp16zBr1iwkJibi8OHDiIqKwrBhw5CXl9fofufPn8dLL72E22+/3UKVWpbvtZ6biio9SrkEAxERUbNJHm4WL16MJ598ElOnTsUtt9yC5cuXw9XVFStXrmxwH51Oh0mTJmHBggXo2LGjBau1HFeFI9wUDgA4qZiIiKglJA03Wq0WqampiIuLM2yTy+WIi4tDSkpKg/u9/vrr8Pf3x+OPP97k96isrIRarTZ62ApeMUVERNRykoabgoIC6HQ6BAQEGG0PCAhATk5Ovfv8+uuv+Oyzz/DJJ58063skJSVBpVIZHqGhoTddt6VwCQYiIqKWk3xYqiVKSkrw6KOP4pNPPoGvr2+z9pkzZw6Ki4sNj6ysLDNXaTpcgoGIiKjlHKX85r6+vnBwcEBubq7R9tzcXAQGBtZpf+bMGZw/fx6jRo0ybNPr9QAAR0dHnDx5EpGRkUb7KJVKKJVKM1RvfhyWIiIiajlJe24UCgX69u2L5ORkwza9Xo/k5GTExsbWad+tWzccPXoU6enphscDDzyAu+66C+np6TY15NQcXIKBiIio5STtuQGAWbNmISEhAf369cOAAQPw/vvvo7S0FFOnTgUATJ48GSEhIUhKSoKzszN69uxptL+XlxcA1NluD9hzQ0RE1HKSh5v4+Hjk5+dj3rx5yMnJQZ8+fbB9+3bDJOPMzEzI5TY1NchkuAQDERFRy8lEG7v9rVqthkqlQnFxMTw9PaUup1HpWUUYs3QvglXO2DfnHqnLISIikkxLfn+3zS4RG3H9UnAtl2AgIiJqJoYbK+brXrMEg1anh7q8WuJqiIiIbAPDjRVTOjrA07lmWlS+pkLiaoiIiGwDw42Vqx2ayuMVU0RERM3CcGPlbpx3Q0RERE1juLFyXIKBiIioZRhurBxv5EdERNQyDDdWjiuDExERtQzDjZXjsBQREVHLMNxYOQ5LERERtQzDjZXz48rgRERELcJwY+Vqe26ulGqh13MJBiIioqYw3Fg5HzcFZDJApxe4WsZ73RARETWF4cbKOTnI4e1as8ZUPoemiIiImsRwYwP8eMUUERFRszHc2ADe64aIiKj5GG5sgK/7tWEp9twQERE1ieHGBvBeN0RERM3HcGMDuDI4ERFR8zHc2AAuwUBERNR8DDc2gMNSREREzcdwYwN8uQQDERFRszHc2IDanpvCMi2qdHqJqyEiIrJuDDc2wNtVAQe5DEIAhaWcVExERNQYhhsb4CCXwceN97ohIiJqDoYbG2FYgoHzboiIiBrFcGMjDPe6Yc8NERFRoxhubIQve26IiIiaheHGRtT23FwuqpC4EiIiIuvGcGMj+oSqAADbjl5GRZVO4mqIiIisF8ONjYjrHoBglTOulGrx3yPZUpdDRERktRhubISjgxyTB0UAAFbuPQ8hhLQFERERWSmGGxsyoX8oXJwccPyyGvvPFkpdDhERkVViuLEhXq4KPHhrCABg1d5zEldDRERknRhubMzUwREAgJ3Hc5F5pUzaYoiIiKwQw42N6eTvgSFd/CAE8HnKeanLISIisjoMNzaotvdm3cEslFRUSVsMERGRlWG4sUF3dPZDpJ8bNJXV2Jh6UepyiIiIrArDjQ2Sy2WYMrgDAGD1vvPQ63lZOBERUS2GGxs17tYQeDo74sKVMvx0Ik/qcoiIiKwGw42NclU4YmJMGABgJS8LJyIiMmC4sWGTYyPgIJdh35krOJGjlrocIiIiq8BwY8NCvFwwvEcgAGDVr+elLYaIiMhKMNzYuMduiwAAbE6/hCuaSmmLISIisgIMNzbu1jBv9G6vgrZajzUHMqUuh4iISHIMNzZOJpPhsWuXhX+RcgHaar3EFREREUmL4cYO3NcrCP4eSuSVVGLb0ctSl0NERCQpqwg3S5cuRUREBJydnRETE4MDBw402PaTTz7B7bffDm9vb3h7eyMuLq7R9m2BwlGORweGA6i5LFwI3tSPiIjaLsnDzbp16zBr1iwkJibi8OHDiIqKwrBhw5CXV/+N6fbs2YOJEydi9+7dSElJQWhoKIYOHYpLly5ZuHLr8nBMGBSOcvx+sRiHM69KXQ4REZFkZELi/+bHxMSgf//+WLJkCQBAr9cjNDQUzz//PF5++eUm99fpdPD29saSJUswefLkJtur1WqoVCoUFxfD09Pzpuu3Jv/YeATrD13EyF5BWDrpVqnLISIiMpmW/P6WtOdGq9UiNTUVcXFxhm1yuRxxcXFISUlp1nuUlZWhqqoKPj4+9b5eWVkJtVpt9LBXU69NLN5+LAeXisolroaIiEgakoabgoIC6HQ6BAQEGG0PCAhATk5Os95j9uzZCA4ONgpIN0pKSoJKpTI8QkNDb7pua9U9yBODIttBpxf4IuW81OUQERFJQvI5Nzfj7bffxtq1a7F582Y4OzvX22bOnDkoLi42PLKysixcpWXV9t6s+S0TZdpqiashIiKyPEnDja+vLxwcHJCbm2u0PTc3F4GBgY3u++677+Ltt9/Gjz/+iN69ezfYTqlUwtPT0+hhz+7u5o/wdq5QV1Rj0+G2PcmaiIjaJknDjUKhQN++fZGcnGzYptfrkZycjNjY2Ab3e+edd/DGG29g+/bt6NevnyVKtRkOchmmDIoAAKzaew56PS8LJyKitkXyYalZs2bhk08+weeff47jx4/jmWeeQWlpKaZOnQoAmDx5MubMmWNov3DhQsydOxcrV65EREQEcnJykJOTA41GI9UhWJ2H+raHu9IRZ/JL8cvpfKnLISIisijJw018fDzeffddzJs3D3369EF6ejq2b99umGScmZmJy5ev33V32bJl0Gq1eOihhxAUFGR4vPvuu1IdgtXxcHbC+H41E6dX7T0vbTFEREQWJvl9bizNnu9zc6PMK2UY8u5uCAHsmjUEnfzdpS6JiIio1WzmPjdkPmHtXBHXvab3a/W+cxJXQ0REZDkMN3asdrXw/6ReQlGZVuJqiIiILIPhxo4N7OiDboEeKK/SYe1B+76/DxERUS2GGzsmk8nw2G01vTdf7DuPap1e4oqIiIjMj+HGzj0QFYx2bgpkF1dgx7HcpncgIiKycQw3ds7ZyQGTYsIAACv3cmIxERHZP4abNuCRgeFwcpAh9cJVHMkqkrocIiIis2K4aQP8PZ0xqncwgJolGYiIiOwZw00bUbta+Pe/X0auukLiaoiIiMyH4aaN6NVehf4R3qjWC3y1/4LU5RAREZkNw00bUntTv69/y0RFlU7iaoiIiMyD4aYNufeWAIR4uaCwVItv0y9JXQ4REZFZMNy0IY4OciQMCgdQs1p4G1szlYiI2giGmzYmvl8YXJwccCKnBClnrkhdDhERkckx3LQxKlcnPNS3PQDe1I/atooqHbb/cRnfpl9iLyaRnXGUugCyvCmDI/Dl/gtIPpGH8wWliPB1k7okIovQ6wUOni/E5rRL2Hr0MkoqqgEA6VlFmHf/LZDJZBJXSESmwHDTBkX6uePOrn7YczIfq/edx/wHekhdEpFZZeSVYNPhS/g2PRuXisoN2/09lMgrqcSqvecBgAGHyE4w3LRRjw3ugD0n87HhUBZmDe0CT2cnqUsiMqm8kgr898hlbE67iD8uqQ3bPZSOGNErEGOiQzCwQzusP5SFlzcdZcAhsiMMN23U7Z190cnfHRl5Gmw4dBGP39ZB6pKIblqZtho7juVgc1o2fj2dD/21qTSOchnu7OqHsdHtcU93fzg7ORj2mTCgZmHZ2oAjBJA4igGHyJYx3LRRMpkMUwdH4NXNf2D1vnOYMigCDnL+Y062p1qnx94zV7Al7RJ2HMtBmfb6DSqjw7wwNjoE9/cOho+bosH3uDHgrN53HgADDpEtY7hpwx6Mbo93tp9EVmE5dh3PxbAegVKXRNQsQggcy1Zjc9olfHckG/kllYbXwtu5YkyfEIyNDmnRZPkJA8IgkwGz/8OAQ/Zt98k86HQC93T3t9u/3ww3bZiLwgEPx4Rh2Z4zWLX3HMMNWb2LV8vwbXo2tqRdwuk8jWG7t6sT7u8djLG3hiA61KvV/2DH96/pwWHAIXv1+b7zSPzuGABgbHQI/jmmJ9yU9hcF7O+IqEUeHRiOFb+cxf6zhTiWXYwewSqpSyIyUlxehR+OXsbmtEv47VyhYbvCUY57uwdgTHQIhnTxg8LRNLftqg04HKIie/PNb5mGYAMAm9Mu4eilYnw06VZ0CfCQsDLTY7hp44K9XDCiZyC+//0yVu89j0X/FyV1SUQAaoaePtpzBh8kn4a2Wm/YPrCjDx6Mbo/hvQLNdpVffP8wyCDD7E2/M+CQXdiYehGvbjkKAHjqjo6I6x6A59ccRkaeBqOX7MU/x/TEuGs3eLUHMtHGbs2pVquhUqlQXFwMT09PqcuxCqkXrmLcsn1QOMjx5B0d0MnfHZ39PdDRzw2uCuZfsrzKah3mbDqKTYdrFnjt7O+OsbeGYHSfEIR4uVisjvUHszB70+8QAkiIDcf8B3ow4JDN+e5INmauTYNeAFMGRRiCeoGmEi+sS8f/ThcAAOL7hWLB6B5GVxNak5b8/ma4IQgh8OCyfUjLLKrzWoiXCzr5uxsena/96eXa8JUn5lJcVoWsq2XILLz+yLr2yFFX4LZOfnhrbE/4ezpbvDYyncJSLZ7+MhUHzhfCQS7Dggd6YFJMmGShggGHbNkPRy/juTVp0OkFJg4Iw1tjexr9/dXpBZbuzsC/dp2CEEC3QA8snXQrIv3cJay6fgw3jWC4qV+eugLf/34Zp/M0OJOnQUa+BoWl2gbb+7orEOl3Y+jxQCd/dwR4Klv9D7+2Wo/sonLj4FIbZq6UQX3tVvmN8XZ1QtKDvTG8JydH26Iz+Ro8tvogLlwpg4fSEUsn3Yo7uvhJXRYDDtmkXX/m4umvUlGtF3iob3u8M6435A3c8mNvRgFmrE1DgUYLN4UD3h7XG6Oigi1cceMYbhrBcNN8haVaZORpcDqvBBl5GmRcCz7ZxRUN7uOhdERHf3d08nNH54CaPzv5uyPUxxVyGXClVGvU43I9yJTjcnG54aZrDfF1VyLMxwVhPq4I83FF+2t/OjnIMHfLMfx5ueZOtPH9QjFv1C12eRWAvdp3pgBPf5kKdUU12nu7YOWU/lY1yfHGgDM5NhwLGHDIiv18Kh9Pfn4IWp0eo/sEY/H4Pk3eyyxPXYHn16QZJu4/MjAMr428xWqGqRhuGsFwc/NKK6txJl+D07k1PTy1oedCYRl0DaQThaMcjnKZ0Q3W6uPsJL8eXLxdDV+HtXNFe2+XRucAVVbr8K+dp/HxL2cgRM39ThaP74O+4d43dbxkfusPZuGVzUdRrRe4NcwLKyb3g6+7Uuqy6lh/KAuz/8OAQ9ZtX0YBpq4+iMpqPUb0DMSHE6Ph6NC8qwmrdXq8v+s0luzOAAD0DPHERw/3RVg7V3OW3CwMN41guDGfymodLlwpqwk9edeDz9l8DSqvXe0ikwFBns6GHpfaR6iPC0J9XOHn3vphrVr7z17Bi+uP4FJROeQy4Lm7O+P5uzvBqZkfbrIcvV5g4Y4T+PjnswCAUVHBWPRQb6v5n2J9GHDImh04V4iElQdQXqVDXHd/fDSpb6tuk7DnZB5eWJeOq2VV8HB2xKKHoiQf7me4aQTDjeXp9AKXrpajWq9HiLcLlI7m/8VVXF6FxG//wJb0bABAVKgX3o/vgw4tuGMtmVe5VocX1qVj+7EcAMDf7umMF+I620RQuDHgPDowHK+PZsAh6R3OvIpHP/0NpVodhnTxw4rJfW/q39vsonI8vyYNqReuAqhZcPnlEd1Mdk+plmK4aQTDTdvy3ZFsvLb5KNQV1XBxcsC8UbdgQv9Q/iKSWJ66Ak98cQi/XyyGwkGOhQ/1wtho27rHxoZDWfgHAw5ZiaMXi/Hwp/tRUlGNQZHtsHJKf5P0gFbp9Fi04yRW/FLTu9on1AtLHo5Ge2/LD1Mx3DSC4abtyS4qx4vrjyDl7BUAQFz3ALw9rpdVzuloC/7MVuPxzw/icnEFvF2dsGJyP/SP8JG6rFZhwCFr8Ge2GhM/2Y/i8ioMiPDB6sf6m/weZTv/zMWL69OhrqiGysUJi8dH4Z7uASb9Hk1huGkEw03bpNcLfPbrOSzacRJanR6+7gq881Bv3N3Nsh/Oti75eC6eX5OGMq0OkX5uWDmlP8Lb2fZQ4Y0B55GBYXhjdE8GHLKY07kliF+xH4WlWkSHeeHLx2PgbqarRLMKy/DcN4dx5GIxAGDakI54aWhXi81nZLhpBMNN2/Znthoz16XhVG7NoouPDAzDq/fdAheF9U5gtQdCCKzaex7/3Pon9AIYFNkOyyb1hcrVPMsnWNrG1Iv4+8YjDDhkUWfzNYhfsR/5JZXoFaLCV0/EQOVi3s9UZbUOSdtOGJYl6R/hjQ8n3opAlflvnspw0wiGG6qo0mHRjpP47NdzAICOfm54P74Perf3krYwO1Wt02PBf//El/svAAAm9A/FG2N62t3Va38NOK8/0LPBG6YR3awLV0oR//F+5Kgr0C3QA2ufGmjRO8dv/f0yZv/nd2gqq+HjpsD78X3MfsNNhptGMNxQrV9PF+DFDenIVVfCUS7DzLjOeObOTk3e6IqaT11Rhee+ScMvp/IhkwFzRnTDk7d3tNteDQYcsoSLV8sQ//F+XCoqR2d/d6x9aiDaSTCH8HxBKZ79+jD+vKyGTAY8f1cnzIjrYrZ/QxluGsFwQzcqKtPilc1Hse1ozeXI/cK98a/4Pgj1kf6GVbYuq7AMj60+iNN5Grg4OeD9CX0wrIf9L4vBgEPmlFNcgfEfpyCzsAwdfd2wdtpA+HtIt55eRZUOr3//J775LRMAENuxHT6Y2McsNTHcNILhhv5KCIFNhy8h8btj0FRWw13piPkP9MC4W0PstofB3A5nXsVTXxxCgUaLAE8lPkvoj54hKqnLspj/pF7ES9cCzoAIH/Rur0Kkvzs6+roh0t8d7dwU/LtFLZZXUoEJH+/H2YJShPm4Yv20WIvMdWmOLWmX8MrmoyjT6uDrrsS/J/bBoEhfk34PhptGMNxQQ7IKy/DCunQcunbDqvt6BeLNMb3g7Wb5FdBt2X+PZOPFDUegrdbjliBPfDalH4JULlKXZXE3Bpy/Urk4oaOfGyL93BHp5274Orydq93NRSLTuKKpxIQV+3E6T4MQLxesmzZQknvNNCYjT4Nnv07FqVwNfN2V+N8/7jLpxRoMN41guKHG6PQCy38+g3/tPIVqvUCApxLv/l8Ubu8s/crU1k4IgQ9/ysDinacAAHHd/fHBhOg2vXjpqdwSHDhXiLP5pTiTr8GZfA0uFZXXG3gAwEEuQ7iPKzr6uSOyNvz4u6Gjr7vZQrZeL1CqrYamshqlldXQVOqgqah5LoRA9yBPhLdzZU+ThIrKtJj4yW84flmNQE9nrJs20GpvoVCmrUbit8cwsncQ7uzqb9L3ZrhpBMMNNcfRi8WYsS4NZ/NLAQBTBkXgob7t0dHPzeQ3x7IHldU6vPyfo9icdgkA8PhtHfDKfd05ObseFVU6nCuoCTu1oaf2z8YWlvVxU9QMa90QeCJ83SCEgKbyejgpqagNKTVB5frX1dBUVNcEmYrr7UubWMwWALxcndC7vRf6tFchKtQLvdt7wc+DN8G0BHVFFR759Df8frEYvu5KrJs2EJF+7lKXJQmGm0Yw3FBzlWt1eGvbccMlzLVCvFzQyd/d+OFnvv9ZW7vCUi2mfXkIB89fhYNchgUP9MAjA8OlLsvmCCGQo66oE3jO5GmQXVxh9u/vKJfB3dkRbgpHeDg7wk3piCqdHicul0Cr09dpH+LlgqhQFaLaeyEq1Au9QlRtupfOHDSV1Xj0s9+QllkEHzcF1j41EF0CPKQuSzIMN41guKGW2n0iDx//cgancjUoLNU22K6dm+La/6qNg0+wytluuvSrdHpkFZbhXEEpzuaX4mxBKX45lY9LReXwUDrio0du5RCeGZRpqw0/7zN5GkP4ySwsg6ODDO5KR8PDTekId2dHuCtq/nRTOsJd6QB3pRPclA41weXaa4b2SkcoHeX1/j3VVutxIkeNIxeLcSSrCEeyipCRr6kztCaXAZ39PWoCT6gXotp7oWugB+cQtVKZthpTVh7EgfOFULk4Yc2TA3FLcNv+ncVw0wiGG7oZhaVaZORprj+u/c/6UlF5g/u4KhwQ6Xc97NR+ba2TR4UQyFVX4myBBucKSnHu2i/VcwU1v0x1+rr/ZLT3dsGqKf3RuQ3/r7ItKamowtFLxTiSVYzfL9YEnvp6l5SOcvQI9kRUqBf6XAs8nL/TtIoqHR5bfRD7zlyBh9IRXz8Zw5uMguGmUQw3ZA6llTX/s87IL0FGngZn8kqRka/B+YJSVNcTBoCaYYDwdq7Xgo4bPK/9T9rD2QnuzjVDAx7K61839r/r1igur6oJLwUaowBzrqC00bkfzk5ydPCtucKno68bOvi64Z7uAWa/7TtZtzx1xfXenWuBR11RXaedysUJvdur0CfUC7cEeSKsnSvCfFzh4dz2/v7o9QIXr5bjVG4JTuWVICNXU/NnngYVVXq4KRzw5RMxuDXMW+pSrYLNhZulS5di0aJFyMnJQVRUFD788EMMGDCgwfYbNmzA3Llzcf78eXTu3BkLFy7Efffd16zvxXBDllSl0+PClbKawJN/vcenqcmjDXFykNWEH2VtELoefGpDkbvSsSYoOTvCXVnT1ijIXAswBZqGh9gc5DKE+biiw7Xw0sH3WpDxc0OAhzNvSkdN0usFLhSW4UhWEdKvBZ5j2Wpoq+vO3wEAb1cnhPm4ItSnJuyEt7v+dZDKxaYnpzcVYurTzk2BjybdipiO7SxcrfWyqXCzbt06TJ48GcuXL0dMTAzef/99bNiwASdPnoS/f93LyPbt24c77rgDSUlJuP/++/HNN99g4cKFOHz4MHr27Nnk92O4IWug1wtcVldc6+WpGdaqvYJFXVFluLKlpOL6lS7mEOCpvBZe3A29MB383BDq7QqFo/UNmZFt01brcTKnBOnXenYy8jTIKizDlUbmsgE1vZztvV0MYaf2EerjirB2rvC0kl6fG0PM6TwNTteGmUZCjMJRjkg/d3QJcEdnf3d0DvBAlwAPhPm42nSgMwebCjcxMTHo378/lixZAgDQ6/UIDQ3F888/j5dffrlO+/j4eJSWluL77783bBs4cCD69OmD5cuXN/n9GG7IFun1ApobLuEtqai6HnyuhaASw9dVhkCkvvbcXel4Lby4o8O14aQIXze48+oWsgKaympkFZYhs7DM8Gft42Jheb1Xa93I61qvz1+Dj7vSEbWjuDLIrn8tu/7c6Otrr+GG5zXtZYbXattW6fQ4m1/a4hDT2f9akLkWYkK9XeBohXPvrFFLfn9L+i+bVqtFamoq5syZY9gml8sRFxeHlJSUevdJSUnBrFmzjLYNGzYMW7ZsMWepRJKSy2XwdHaymv+hEpmSu9IR3YM80T2o7i8svV4gt6QCF67UDT9ZhWUo0GhRVFaForJi/H6xWILqjSkc5Ojo54YuAR6GENPZ3x1hPq4MMRYkabgpKCiATqdDQECA0faAgACcOHGi3n1ycnLqbZ+Tk1Nv+8rKSlRWVhqeq9Xqm6yaiIgsRS6XIUjlgiCVCwbWM/+ktLIaWVfLkFlP+Kmo0kMIAQFACEBAGC5hr92Ga9tqnt/Q9trXqO81CMhlNfPSulwLL52vhRmGGOtg933SSUlJWLBggdRlEBGRGbgpHdEt0BPdAjnNgK6TNF76+vrCwcEBubm5Rttzc3MRGBhY7z6BgYEtaj9nzhwUFxcbHllZWaYpnoiIiKySpOFGoVCgb9++SE5ONmzT6/VITk5GbGxsvfvExsYatQeAnTt3NtheqVTC09PT6EFERET2S/JhqVmzZiEhIQH9+vXDgAED8P7776O0tBRTp04FAEyePBkhISFISkoCAMyYMQNDhgzBe++9h5EjR2Lt2rU4dOgQVqxYIeVhEBERkZWQPNzEx8cjPz8f8+bNQ05ODvr06YPt27cbJg1nZmZCLr/ewTRo0CB88803eO211/DKK6+gc+fO2LJlS7PucUNERET2T/L73Fga73NDRERke1ry+5vXqxEREZFdYbghIiIiu8JwQ0RERHaF4YaIiIjsCsMNERER2RWGGyIiIrIrDDdERERkVxhuiIiIyK4w3BAREZFdkXz5BUurvSGzWq2WuBIiIiJqrtrf281ZWKHNhZuSkhIAQGhoqMSVEBERUUuVlJRApVI12qbNrS2l1+uRnZ0NDw8PyGQyk763Wq1GaGgosrKy7H7dKh6r/WpLx8tjtV9t6XjbyrEKIVBSUoLg4GCjBbXr0+Z6buRyOdq3b2/W7+Hp6WnXf8FuxGO1X23peHms9qstHW9bONamemxqcUIxERER2RWGGyIiIrIrDDcmpFQqkZiYCKVSKXUpZsdjtV9t6Xh5rParLR1vWzrW5mpzE4qJiIjIvrHnhoiIiOwKww0RERHZFYYbIiIisisMN0RERGRXGG5aaOnSpYiIiICzszNiYmJw4MCBRttv2LAB3bp1g7OzM3r16oVt27ZZqNLWS0pKQv/+/eHh4QF/f3+MGTMGJ0+ebHSf1atXQyaTGT2cnZ0tVPHNmT9/fp3au3Xr1ug+tnheASAiIqLOscpkMkyfPr3e9rZ0Xn/55ReMGjUKwcHBkMlk2LJli9HrQgjMmzcPQUFBcHFxQVxcHE6fPt3k+7b0M28pjR1vVVUVZs+ejV69esHNzQ3BwcGYPHkysrOzG33P1nwWLKGpcztlypQ6dQ8fPrzJ97XGc9vUsdb3+ZXJZFi0aFGD72mt59WcGG5aYN26dZg1axYSExNx+PBhREVFYdiwYcjLy6u3/b59+zBx4kQ8/vjjSEtLw5gxYzBmzBj88ccfFq68ZX7++WdMnz4d+/fvx86dO1FVVYWhQ4eitLS00f08PT1x+fJlw+PChQsWqvjm9ejRw6j2X3/9tcG2tnpeAeDgwYNGx7lz504AwP/93/81uI+tnNfS0lJERUVh6dKl9b7+zjvv4N///jeWL1+O3377DW5ubhg2bBgqKioafM+WfuYtqbHjLSsrw+HDhzF37lwcPnwYmzZtwsmTJ/HAAw80+b4t+SxYSlPnFgCGDx9uVPeaNWsafU9rPbdNHeuNx3j58mWsXLkSMpkM48aNa/R9rfG8mpWgZhswYICYPn264blOpxPBwcEiKSmp3vbjx48XI0eONNoWExMjpk2bZtY6TS0vL08AED///HODbVatWiVUKpXlijKhxMREERUV1ez29nJehRBixowZIjIyUuj1+npft9XzCkBs3rzZ8Fyv14vAwECxaNEiw7aioiKhVCrFmjVrGnyfln7mpfLX463PgQMHBABx4cKFBtu09LMghfqONSEhQYwePbpF72ML57Y553X06NHi7rvvbrSNLZxXU2PPTTNptVqkpqYiLi7OsE0ulyMuLg4pKSn17pOSkmLUHgCGDRvWYHtrVVxcDADw8fFptJ1Go0F4eDhCQ0MxevRoHDt2zBLlmcTp06cRHByMjh07YtKkScjMzGywrb2cV61Wi6+++gqPPfZYo4vI2vJ5rXXu3Dnk5OQYnTeVSoWYmJgGz1trPvPWrLi4GDKZDF5eXo22a8lnwZrs2bMH/v7+6Nq1K5555hlcuXKlwbb2cm5zc3OxdetWPP744022tdXz2loMN81UUFAAnU6HgIAAo+0BAQHIycmpd5+cnJwWtbdGer0eM2fOxODBg9GzZ88G23Xt2hUrV67Et99+i6+++gp6vR6DBg3CxYsXLVht68TExGD16tXYvn07li1bhnPnzuH2229HSUlJve3t4bwCwJYtW1BUVIQpU6Y02MaWz+uNas9NS85baz7z1qqiogKzZ8/GxIkTG11YsaWfBWsxfPhwfPHFF0hOTsbChQvx888/Y8SIEdDpdPW2t5dz+/nnn8PDwwMPPvhgo+1s9bzejDa3Kji1zPTp0/HHH380OT4bGxuL2NhYw/NBgwahe/fu+Pjjj/HGG2+Yu8ybMmLECMPXvXv3RkxMDMLDw7F+/fpm/Y/IVn322WcYMWIEgoODG2xjy+eValRVVWH8+PEQQmDZsmWNtrXVz8KECRMMX/fq1Qu9e/dGZGQk9uzZg3vuuUfCysxr5cqVmDRpUpOT/G31vN4M9tw0k6+vLxwcHJCbm2u0PTc3F4GBgfXuExgY2KL21ua5557D999/j927d6N9+/Yt2tfJyQnR0dHIyMgwU3Xm4+XlhS5dujRYu62fVwC4cOECdu3ahSeeeKJF+9nqea09Ny05b635zFub2mBz4cIF7Ny5s9Fem/o09VmwVh07doSvr2+DddvDuf3f//6HkydPtvgzDNjueW0JhptmUigU6Nu3L5KTkw3b9Ho9kpOTjf5ne6PY2Fij9gCwc+fOBttbCyEEnnvuOWzevBk//fQTOnTo0OL30Ol0OHr0KIKCgsxQoXlpNBqcOXOmwdpt9bzeaNWqVfD398fIkSNbtJ+tntcOHTogMDDQ6Lyp1Wr89ttvDZ631nzmrUltsDl9+jR27dqFdu3atfg9mvosWKuLFy/iypUrDdZt6+cWqOl57du3L6Kiolq8r62e1xaRekazLVm7dq1QKpVi9erV4s8//xRPPfWU8PLyEjk5OUIIIR599FHx8ssvG9rv3btXODo6infffVccP35cJCYmCicnJ3H06FGpDqFZnnnmGaFSqcSePXvE5cuXDY+ysjJDm78e64IFC8SOHTvEmTNnRGpqqpgwYYJwdnYWx44dk+IQWuTFF18Ue/bsEefOnRN79+4VcXFxwtfXV+Tl5Qkh7Oe81tLpdCIsLEzMnj27zmu2fF5LSkpEWlqaSEtLEwDE4sWLRVpamuHqoLffflt4eXmJb7/9Vvz+++9i9OjRokOHDqK8vNzwHnfffbf48MMPDc+b+sxLqbHj1Wq14oEHHhDt27cX6enpRp/jyspKw3v89Xib+ixIpbFjLSkpES+99JJISUkR586dE7t27RK33nqr6Ny5s6ioqDC8h62c26b+HgshRHFxsXB1dRXLli2r9z1s5byaE8NNC3344YciLCxMKBQKMWDAALF//37Da0OGDBEJCQlG7devXy+6dOkiFAqF6NGjh9i6dauFK245APU+Vq1aZWjz12OdOXOm4ecSEBAg7rvvPnH48GHLF98K8fHxIigoSCgUChESEiLi4+NFRkaG4XV7Oa+1duzYIQCIkydP1nnNls/r7t276/17W3s8er1ezJ07VwQEBAilUinuueeeOj+D8PBwkZiYaLStsc+8lBo73nPnzjX4Od69e7fhPf56vE19FqTS2LGWlZWJoUOHCj8/P+Hk5CTCw8PFk08+WSek2Mq5bervsRBCfPzxx8LFxUUUFRXV+x62cl7NSSaEEGbtGiIiIiKyIM65ISIiIrvCcENERER2heGGiIiI7ArDDREREdkVhhsiIiKyKww3REREZFcYboiIiMiuMNwQUZsnk8mwZcsWqcsgIhNhuCEiSU2ZMgUymazOY/jw4VKXRkQ2ylHqAoiIhg8fjlWrVhltUyqVElVDRLaOPTdEJDmlUonAwECjh7e3N4CaIaNly5ZhxIgRcHFxQceOHbFx40aj/Y8ePYq7774bLi4uaNeuHZ566iloNBqjNitXrkSPHj2gVCoRFBSE5557zuj1goICjB07Fq6urujcuTO+++478x40EZkNww0RWb25c+di3LhxOHLkCCZNmoQJEybg+PHjAIDS0lIMGzYM3t7eOHjwIDZs2IBdu3YZhZdly5Zh+vTpeOqpp3D06FF899136NSpk9H3WLBgAcaPH4/ff/8d9913HyZNmoTCwkKLHicRmYjUK3cSUduWkJAgHBwchJubm9HjzTffFELUrFL/9NNPG+0TExMjnnnmGSGEECtWrBDe3t5Co9EYXt+6dauQy+WGlaGDg4PFq6++2mANAMRrr71meK7RaAQA8cMPP5jsOInIcjjnhogkd9ddd2HZsmVG23x8fAxfx8bGGr0WGxuL9PR0AMDx48cRFRUFNzc3w+uDBw+GXq/HyZMnIZPJkJ2djXvuuafRGnr37m342s3NDZ6ensjLy2vtIRGRhBhuiEhybm5udYaJTMXFxaVZ7ZycnIyey2Qy6PV6c5RERGbGOTdEZPX2799f53n37t0BAN27d8eRI0dQWlpqeH3v3r2Qy+Xo2rUrPDw8EBERgeTkZIvWTETSYc8NEUmusrISOTk5RtscHR3h6+sLANiwYQP69euH2267DV9//TUOHDiAzz77DAAwadIkJCYmIiEhAfPnz0d+fj6ef/55PProowgICAAAzJ8/H08//TT8/f0xYsQIlJSUYO/evXj++ecte6BEZBEMN0Qkue3btyMoKMhoW9euXXHixAkANVcyrV27Fs8++yyCgoKwZs0a3HLLLQAAV1dX7NixAzNmzED//v3h6uqKcePGYfHixYb3SkhIQEVFBf71r3/hpZdegq+vLx566CHLHSARWZRMCCGkLoKIqCEymQybN2/GmDFjpC6FiGwE59wQERGRXWG4ISIiIrvCOTdEZNU4ck5ELcWeGyIiIrIrDDdERERkVxhuiIiIyK4w3BAREZFdYbghIiIiu8JwQ0RERHaF4YaIiIjsCsMNERER2RWGGyIiIrIr/w9DoVE8DisxRwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# 20 Newsgroups\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')\n",
    "X, y = newsgroups_data.data, newsgroups_data.target\n",
    "\n",
    "# CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=4096)  # \n",
    "X = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        #self.dropout1 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        #self.dropout2 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc3 = nn.Linear(256, 20)  # 2020\n",
    "        self.to(device)  # GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "      #  x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "      #  x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = NewsClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \n",
    "epochs = 20\n",
    "losses = []  # epoch\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    if (epoch+1) % 2 == 0:\n",
    "        print(f'Epoch {epoch+1}, loss: {epoch_loss:.4f}')\n",
    "    \n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# \n",
    "plt.figure()\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "92a55cf2-fe96-44a8-a662-704ddd77e735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 2, loss: 0.2824\n",
      "Epoch 4, loss: 0.0720\n",
      "Epoch 6, loss: 0.0243\n",
      "Epoch 8, loss: 0.0108\n",
      "Epoch 10, loss: 0.0937\n",
      "Epoch 12, loss: 0.0239\n",
      "Epoch 14, loss: 0.0086\n",
      "Epoch 16, loss: 0.0060\n",
      "Epoch 18, loss: 0.0057\n",
      "Epoch 20, loss: 0.0384\n",
      "Epoch 22, loss: 0.0133\n",
      "Epoch 24, loss: 0.0403\n",
      "Epoch 26, loss: 0.0145\n",
      "Epoch 28, loss: 0.0032\n",
      "Epoch 30, loss: 0.0325\n",
      "Epoch 32, loss: 0.0077\n",
      "Epoch 34, loss: 0.0846\n",
      "Epoch 36, loss: 0.0186\n",
      "Epoch 38, loss: 0.0313\n",
      "Epoch 40, loss: 0.0100\n",
      "Epoch 42, loss: 0.0106\n",
      "Epoch 44, loss: 0.0014\n",
      "Epoch 46, loss: 0.0010\n",
      "Epoch 48, loss: 0.0254\n",
      "Epoch 50, loss: 0.0122\n",
      "Epoch 52, loss: 0.0660\n",
      "Epoch 54, loss: 0.0587\n",
      "Epoch 56, loss: 0.0076\n",
      "Epoch 58, loss: 0.0023\n",
      "Epoch 60, loss: 0.0025\n",
      "Accuracy on test set: 85.60%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# 20 Newsgroups\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')\n",
    "X, y = newsgroups_data.data, newsgroups_data.target\n",
    "\n",
    "# CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=4096)  # \n",
    "X = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        #self.dropout1 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        #self.dropout2 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc3 = nn.Linear(256, 20)  # 2020\n",
    "        self.to(device)  # GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "      #  x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "      #  x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = NewsClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \n",
    "epochs = 60\n",
    "losses = []  # epoch\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}, loss: {epoch_loss:.4f}')\n",
    "    \n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# \n",
    "plt.figure()\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6dc905f4-f740-462e-aa26-1c22b00fda6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 5, loss: 0.2551\n",
      "Epoch 10, loss: 0.0771\n",
      "Epoch 15, loss: 0.0234\n",
      "Epoch 20, loss: 0.0212\n",
      "Accuracy on test set: 85.70%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# 20 Newsgroups\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')\n",
    "X, y = newsgroups_data.data, newsgroups_data.target\n",
    "\n",
    "# CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=4096)  # \n",
    "X = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        #self.dropout1 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        #self.dropout2 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc3 = nn.Linear(256, 20)  # 2020\n",
    "        self.to(device)  # GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "      #  x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "      #  x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = NewsClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# \n",
    "epochs = 20\n",
    "losses = []  # epoch\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}, loss: {epoch_loss:.4f}')\n",
    "    \n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# \n",
    "plt.figure()\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "361ac6f8-ea38-4991-9d24-f92d36060a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 5, loss: 0.2740\n",
      "Epoch 10, loss: 0.0646\n",
      "Epoch 15, loss: 0.0333\n",
      "Epoch 20, loss: 0.0182\n",
      "Epoch 25, loss: 0.0128\n",
      "Epoch 30, loss: 0.0066\n",
      "Epoch 35, loss: 0.0029\n",
      "Epoch 40, loss: 0.0075\n",
      "Accuracy on test set: 85.20%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# 20 Newsgroups\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')\n",
    "X, y = newsgroups_data.data, newsgroups_data.target\n",
    "\n",
    "# CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=4096)  # \n",
    "X = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        #self.dropout1 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        #self.dropout2 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc3 = nn.Linear(256, 20)  # 2020\n",
    "        self.to(device)  # GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "      #  x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "      #  x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = NewsClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# \n",
    "epochs = 40\n",
    "losses = []  # epoch\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}, loss: {epoch_loss:.4f}')\n",
    "    \n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# \n",
    "plt.figure()\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "68732e4b-514d-4897-b6b1-2e1daa1d63eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 5, loss: 0.2703\n",
      "Epoch 10, loss: 0.0777\n",
      "Epoch 15, loss: 0.0232\n",
      "Epoch 20, loss: 0.0245\n",
      "Epoch 25, loss: 0.0128\n",
      "Epoch 30, loss: 0.0107\n",
      "Epoch 35, loss: 0.0037\n",
      "Epoch 40, loss: 0.0021\n",
      "Epoch 45, loss: 0.0028\n",
      "Epoch 50, loss: 0.0064\n",
      "Epoch 55, loss: 0.0023\n",
      "Epoch 60, loss: 0.0014\n",
      "Accuracy on test set: 85.07%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# 20 Newsgroups\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')\n",
    "X, y = newsgroups_data.data, newsgroups_data.target\n",
    "\n",
    "# CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=4096)  # \n",
    "X = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        #self.dropout1 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        #self.dropout2 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc3 = nn.Linear(256, 20)  # 2020\n",
    "        self.to(device)  # GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "      #  x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "      #  x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = NewsClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# \n",
    "epochs = 60\n",
    "losses = []  # epoch\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}, loss: {epoch_loss:.4f}')\n",
    "    \n",
    "\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# \n",
    "plt.figure()\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5b099047-4d4e-47b2-b432-17c3f8190ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 5, loss: 2.1344\n",
      "Epoch 10, loss: 1.2124\n",
      "Epoch 15, loss: 0.8030\n",
      "Epoch 20, loss: 0.5804\n",
      "Accuracy after 20 epochs: 84.27%\n",
      "Epoch 25, loss: 0.4375\n",
      "Epoch 30, loss: 0.3348\n",
      "Epoch 35, loss: 0.2628\n",
      "Epoch 40, loss: 0.2070\n",
      "Accuracy after 40 epochs: 86.84%\n",
      "Epoch 45, loss: 0.1647\n",
      "Epoch 50, loss: 0.1307\n",
      "Epoch 55, loss: 0.1065\n",
      "Epoch 60, loss: 0.0828\n",
      "Accuracy after 60 epochs: 86.84%\n",
      "Final accuracy on test set: 86.84%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# 20 Newsgroups\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')\n",
    "X, y = newsgroups_data.data, newsgroups_data.target\n",
    "\n",
    "# CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=4096)  # \n",
    "X = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        #self.dropout1 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        #self.dropout2 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc3 = nn.Linear(256, 20)  # 2020\n",
    "        self.to(device)  # GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = NewsClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "# \n",
    "epochs = 60\n",
    "losses = []  # epoch\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}, loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    if (epoch+1) % 20 == 0:\n",
    "        accuracy = evaluate_model(model, test_loader)\n",
    "        print(f'Accuracy after {epoch+1} epochs: {accuracy:.2f}%')\n",
    "\n",
    "# \n",
    "final_accuracy = evaluate_model(model, test_loader)\n",
    "print(f'Final accuracy on test set: {final_accuracy:.2f}%')\n",
    "\n",
    "# \n",
    "plt.figure()\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d0e8b6b0-fb4e-4844-b5fd-451e0f250e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 5, loss: 2.9609\n",
      "Epoch 10, loss: 2.9186\n",
      "Epoch 15, loss: 2.8621\n",
      "Epoch 20, loss: 2.7865\n",
      "Accuracy after 20 epochs: 48.54%\n",
      "Epoch 25, loss: 2.6923\n",
      "Epoch 30, loss: 2.5876\n",
      "Epoch 35, loss: 2.4745\n",
      "Epoch 40, loss: 2.3554\n",
      "Accuracy after 40 epochs: 67.35%\n",
      "Epoch 45, loss: 2.2361\n",
      "Epoch 50, loss: 2.1182\n",
      "Epoch 55, loss: 2.0047\n",
      "Epoch 60, loss: 1.8937\n",
      "Accuracy after 60 epochs: 73.21%\n",
      "Final accuracy on test set: 73.21%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# 20 Newsgroups\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')\n",
    "X, y = newsgroups_data.data, newsgroups_data.target\n",
    "\n",
    "# CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=4096)  # \n",
    "X = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# \n",
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        #self.dropout1 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        #self.dropout2 = nn.Dropout(0.5)  # dropout\n",
    "        self.fc3 = nn.Linear(256, 20)  # 2020\n",
    "        self.to(device)  # GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# \n",
    "model = NewsClassifier()\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.000001)\n",
    "\n",
    "# \n",
    "epochs = 60\n",
    "losses = []  # epoch\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}, loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    if (epoch+1) % 20 == 0:\n",
    "        accuracy = evaluate_model(model, test_loader)\n",
    "        print(f'Accuracy after {epoch+1} epochs: {accuracy:.2f}%')\n",
    "\n",
    "# \n",
    "final_accuracy = evaluate_model(model, test_loader)\n",
    "print(f'Final accuracy on test set: {final_accuracy:.2f}%')\n",
    "\n",
    "# \n",
    "plt.figure()\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cca5c81a-2b0e-4803-bb7b-6be8e229f621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.05%\n",
      "Accuracy on test set: 0.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_23432\\181538345.py:36: RuntimeWarning: invalid value encountered in cast\n",
      "  labels[mask] = mode(y_true[mask])[0]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "import numpy as np\n",
    "\n",
    "# 20 Newsgroups\n",
    "newsgroups_data = fetch_20newsgroups(subset='all')\n",
    "X, y = newsgroups_data.data, newsgroups_data.target\n",
    "\n",
    "# CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=4096)  # \n",
    "X = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# K-means\n",
    "kmeans = KMeans(n_clusters=1, random_state=42)\n",
    "\n",
    "# K-means\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "# \n",
    "y_pred_train = kmeans.predict(X_train)\n",
    "y_pred_test = kmeans.predict(X_test)\n",
    "\n",
    "# K-means\n",
    "def map_labels(y_pred, y_true):\n",
    "    labels = np.zeros_like(y_pred)\n",
    "    for i in range(20):\n",
    "        mask = (y_pred == i)\n",
    "        labels[mask] = mode(y_true[mask])[0]\n",
    "    return labels\n",
    "\n",
    "y_pred_train_mapped = map_labels(y_pred_train, y_train)\n",
    "y_pred_test_mapped = map_labels(y_pred_test, y_test)\n",
    "\n",
    "# \n",
    "train_accuracy = accuracy_score(y_train, y_pred_train_mapped)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test_mapped)\n",
    "\n",
    "print(f'Accuracy on training set: {train_accuracy:.2f}%')\n",
    "print(f'Accuracy on test set: {test_accuracy:.2f}%')\n",
    "\n",
    "# K-means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff5b02f8-cbf3-4cb3-8dd2-574daacccbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.87      0.84      0.86       151\n",
      "           comp.graphics       0.70      0.77      0.73       202\n",
      " comp.os.ms-windows.misc       0.79      0.78      0.79       195\n",
      "comp.sys.ibm.pc.hardware       0.65      0.73      0.68       183\n",
      "   comp.sys.mac.hardware       0.87      0.81      0.84       205\n",
      "          comp.windows.x       0.84      0.80      0.82       215\n",
      "            misc.forsale       0.83      0.82      0.83       193\n",
      "               rec.autos       0.86      0.91      0.88       196\n",
      "         rec.motorcycles       0.97      0.91      0.94       168\n",
      "      rec.sport.baseball       0.91      0.93      0.92       211\n",
      "        rec.sport.hockey       0.95      0.92      0.94       198\n",
      "               sci.crypt       0.98      0.92      0.95       201\n",
      "         sci.electronics       0.74      0.74      0.74       202\n",
      "                 sci.med       0.92      0.92      0.92       194\n",
      "               sci.space       0.93      0.94      0.93       189\n",
      "  soc.religion.christian       0.91      0.97      0.94       202\n",
      "      talk.politics.guns       0.93      0.93      0.93       188\n",
      "   talk.politics.mideast       0.97      0.97      0.97       182\n",
      "      talk.politics.misc       0.85      0.87      0.86       159\n",
      "      talk.religion.misc       0.81      0.70      0.75       136\n",
      "\n",
      "                accuracy                           0.86      3770\n",
      "               macro avg       0.86      0.86      0.86      3770\n",
      "            weighted avg       0.86      0.86      0.86      3770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# \n",
    "newsgroups_data = fetch_20newsgroups(subset='all')\n",
    "\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups_data.data, newsgroups_data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "#  TF-IDF \n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=4096)\n",
    "\n",
    "#  SVM \n",
    "svm_classifier = SVC(kernel='linear')\n",
    "\n",
    "# \n",
    "pipeline = make_pipeline(tfidf_vectorizer, svm_classifier)\n",
    "\n",
    "# \n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# \n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# \n",
    "print(classification_report(y_test, y_pred, target_names=newsgroups_data.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d70103-f313-4e50-afb5-78fae65d3684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
